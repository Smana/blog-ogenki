[{"body":"","link":"https://blog.ogenki.io/","section":"","tags":null,"title":""},{"body":"","link":"https://blog.ogenki.io/tags/devxp/","section":"tags","tags":null,"title":"devxp"},{"body":"","link":"https://blog.ogenki.io/tags/gitops/","section":"tags","tags":null,"title":"gitops"},{"body":"üë∑‚Äç‚ôÇÔ∏è Work in progess for this section\nüìÇ Flux repository structure üîê SealedSecrets üõ†Ô∏è Deploy and configure Crossplane ","link":"https://blog.ogenki.io/post/devflux/","section":"post","tags":["gitops","devxp"],"title":"Gitops my infrastructure!"},{"body":"","link":"https://blog.ogenki.io/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://blog.ogenki.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://blog.ogenki.io/tags/infrastructure/","section":"tags","tags":null,"title":"infrastructure"},{"body":"","link":"https://blog.ogenki.io/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"The target of this documentation is to be able to create and manage a GKE cluster using Crossplane. Here are the steps we'll follow in order to get a Kubernetes cluster for development and experimentations use cases.\nüê≥ Create the local k3d cluster for Crossplane's control plane k3d is a lightweight kubernetes cluster that leverages k3s that runs in our local laptop. There are several deployment models for Crossplane, we could for instance deploy the control plane on a management cluster on Kubernetes or a control plane per Kubernetes cluster.\nHere I chose a simple method which is fine for a personal use case: A local Kubernetes instance in which I'll deploy Crossplane.\nLet's install k3d using asdf.\n1asdf plugin-add k3d 2 3asdf install k3d $(asdf latest k3d) 4* Downloading k3d release 5.4.1... 5k3d 5.4.1 installation was successful! Create a single node Kubernetes cluster.\n1k3d cluster create crossplane 2... 3INFO[0043] You can now use it like this: 4kubectl cluster-info 5 6k3d cluster list 7crossplane 1/1 0/0 true Check that the cluster is reachable using the kubectl CLI.\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:40643 3CoreDNS is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy We only need a single node for our Crossplane use case.\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-crossplane-server-0 Ready control-plane,master 26h v1.22.7+k3s1 ‚òÅÔ∏è Generate the Google Cloud service account Warning Store the downloaded crossplane.json credentials file in a safe place.\nCreate a service account\n1GCP_PROJECT=\u0026lt;your_project\u0026gt; 2gcloud iam service-accounts create crossplane --display-name \u0026#34;Crossplane\u0026#34; --project=${GCP_PROJECT} 3Created service account [crossplane]. Assign the proper permissions to the service account.\nCompute Network Admin Kubernetes Engine Admin Service Account User 1SA_EMAIL=$(gcloud iam service-accounts list --filter=\u0026#34;email ~ ^crossplane\u0026#34; --format=\u0026#39;value(email)\u0026#39;) 2 3gcloud projects add-iam-policy-binding \u0026#34;${GCP_PROJECT}\u0026#34; --member=serviceAccount:\u0026#34;${SA_EMAIL}\u0026#34; \\ 4--role=roles/container.admin --role=roles/compute.networkAdmin --role=roles/iam.serviceAccountUser 5Updated IAM policy for project [\u0026lt;project\u0026gt;]. 6bindings: 7- members: 8 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 9 role: roles/compute.networkAdmin 10- members: 11 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 12... 13version: 1 Download the service account key (json format)\n1gcloud iam service-accounts keys create crossplane.json --iam-account ${SA_EMAIL} 2created key [ea2eb9ce2939127xxxxxxxxxx] of type [json] as [crossplane.json] for [crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com] üöß Deploy and configure Crossplane Now that we have a credentials file for Google Cloud, we can deploy the Crossplane operator and configure the provider-gcp provider.\nInfo Most of the following steps are issued from the official documentation\nWe'll first use Helm in order to install the operator\n1helm repo add crossplane-master https://charts.crossplane.io/master/ 2\u0026#34;crossplane-master\u0026#34; has been added to your repositories 3 4helm repo update 5...Successfully got an update from the \u0026#34;crossplane-master\u0026#34; chart repository 6 7helm install crossplane --namespace crossplane-system --create-namespace \\ 8--version 1.18.1 crossplane-stable/crossplane 9 10NAME: crossplane 11LAST DEPLOYED: Mon Jun 6 22:00:02 2022 12NAMESPACE: crossplane-system 13STATUS: deployed 14REVISION: 1 15TEST SUITE: None 16NOTES: 17Release: crossplane 18... Check that the operator is running properly.\n1kubectl get po -n crossplane-system 2NAME READY STATUS RESTARTS AGE 3crossplane-rbac-manager-54d96cd559-222hc 1/1 Running 0 3m37s 4crossplane-688c575476-lgklq 1/1 Running 0 3m37s Info All the files used for the upcoming steps are stored within this blog repository. So you should clone and change the current directory:\n1git clone https://github.com/Smana/smana.github.io.git 2 3cd smana.github.io/content/resources/crossplane_k3d Now we'll configure Crossplane so that it will be able to create and manage GCP resources. This is done by configuring the provider provider-gcp as follows.\nprovider.yaml\n1apiVersion: pkg.crossplane.io/v1 2kind: Provider 3metadata: 4 name: crossplane-provider-gcp 5spec: 6 package: crossplane/provider-gcp:v0.21.0 1kubectl apply -f provider.yaml 2provider.pkg.crossplane.io/crossplane-provider-gcp created 3 4kubectl get providers 5NAME INSTALLED HEALTHY PACKAGE AGE 6crossplane-provider-gcp True True crossplane/provider-gcp:v0.21.0 10s Create the Kubernetes secret that holds the GCP credentials file created above\n1kubectl create secret generic gcp-creds -n crossplane-system --from-file=creds=./crossplane.json 2secret/gcp-creds created Then we need to create a resource named ProviderConfig and reference the newly created secret.\nprovider-config.yaml\n1apiVersion: gcp.crossplane.io/v1beta1 2kind: ProviderConfig 3metadata: 4 name: default 5spec: 6 projectID: ${GCP_PROJECT} 7 credentials: 8 source: Secret 9 secretRef: 10 namespace: crossplane-system 11 name: gcp-creds 12 key: creds 1kubectl apply -f provider-config.yaml 2providerconfig.gcp.crossplane.io/default created Info If the serviceaccount has the proper permissions we can create resources in GCP. In order to learn about all the available resources and parameters we can have a look to the provider's API reference.\nThe first resource we'll create is the network that will host our Kubernetes cluster.\nnetwork.yaml\n1apiVersion: compute.gcp.crossplane.io/v1beta1 2kind: Network 3metadata: 4 name: dev-network 5 labels: 6 service: vpc 7 creation: crossplane 8spec: 9 forProvider: 10 autoCreateSubnetworks: false 11 description: \u0026#34;Network used for experimentations and POCs\u0026#34; 12 routingConfig: 13 routingMode: REGIONAL 1kubectl get network 2NAME READY SYNCED 3dev-network True True You can even get more details by describing this resource. For instance if something fails you would see the message returned by the Cloud provider in the events.\n1kubectl describe network dev-network | grep -A 20 \u0026#39;^Status:\u0026#39; 2Status: 3 At Provider: 4 Creation Timestamp: 2022-06-28T09:45:30.703-07:00 5 Id: 3005424280727359173 6 Self Link: https://www.googleapis.com/compute/v1/projects/${GCP_PROJECT}/global/networks/dev-network 7 Conditions: 8 Last Transition Time: 2022-06-28T16:45:31Z 9 Reason: Available 10 Status: True 11 Type: Ready 12 Last Transition Time: 2022-06-30T16:36:59Z 13 Reason: ReconcileSuccess 14 Status: True 15 Type: Synced üöÄ Create a GKE cluster Everything is ready so that we can create our GKE cluster. Applying the file cluster.yaml will create a cluster and attach a node group to it.\ncluster.yaml\n1--- 2apiVersion: container.gcp.crossplane.io/v1beta2 3kind: Cluster 4metadata: 5 name: dev-cluster 6spec: 7 forProvider: 8 description: \u0026#34;Kubernetes cluster for experimentations and POCs\u0026#34; 9 initialClusterVersion: \u0026#34;1.24\u0026#34; 10 releaseChannel: 11 channel: \u0026#34;RAPID\u0026#34; 12 location: europe-west9-a 13 addonsConfig: 14 gcePersistentDiskCsiDriverConfig: 15 enabled: true 16 networkPolicyConfig: 17 disabled: false 18 networkRef: 19 name: dev-network 20 ipAllocationPolicy: 21 createSubnetwork: true 22 useIpAliases: true 23 defaultMaxPodsConstraint: 24 maxPodsPerNode: 110 25 networkPolicy: 26 enabled: false 27 writeConnectionSecretToRef: 28 namespace: default 29 name: gke-conn 30--- 31apiVersion: container.gcp.crossplane.io/v1beta1 32kind: NodePool 33metadata: 34 name: main-np 35spec: 36 forProvider: 37 initialNodeCount: 1 38 autoscaling: 39 autoprovisioned: false 40 enabled: true 41 maxNodeCount: 4 42 minNodeCount: 1 43 clusterRef: 44 name: dev-cluster 45 config: 46 machineType: n2-standard-2 47 diskSizeGb: 120 48 diskType: pd-standard 49 imageType: cos_containerd 50 preemptible: true 51 labels: 52 environment: dev 53 managed-by: crossplane 54 oauthScopes: 55 - \u0026#34;https://www.googleapis.com/auth/devstorage.read_only\u0026#34; 56 - \u0026#34;https://www.googleapis.com/auth/logging.write\u0026#34; 57 - \u0026#34;https://www.googleapis.com/auth/monitoring\u0026#34; 58 - \u0026#34;https://www.googleapis.com/auth/servicecontrol\u0026#34; 59 - \u0026#34;https://www.googleapis.com/auth/service.management.readonly\u0026#34; 60 - \u0026#34;https://www.googleapis.com/auth/trace.append\u0026#34; 61 metadata: 62 disable-legacy-endpoints: \u0026#34;true\u0026#34; 63 shieldedInstanceConfig: 64 enableIntegrityMonitoring: true 65 enableSecureBoot: true 66 management: 67 autoRepair: true 68 autoUpgrade: true 69 maxPodsConstraint: 70 maxPodsPerNode: 60 71 locations: 72 - \u0026#34;europe-west9-a\u0026#34; 1kubectl apply -f cluster.yaml 2cluster.container.gcp.crossplane.io/dev-cluster created 3nodepool.container.gcp.crossplane.io/main-np created Note that it takes around 10 minutes for the Kubernetes API and the nodes to be available. The STATE will transition from PROVISIONING to RUNNING and when a change is being applied the cluster status is RECONCILING\n1watch \u0026#39;kubectl get cluster,nodepool\u0026#39; 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3cluster.container.gcp.crossplane.io/dev-cluster False True PROVISIONING 34.155.122.6 europe-west9-a 3m15s 4 5NAME READY SYNCED STATE CLUSTER-REF AGE 6nodepool.container.gcp.crossplane.io/main-np False False dev-cluster 3m15s When the column READY switches to True you can download the cluster's credentials.\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RECONCILING 34.42.42.42 europe-west9-a 6m23s 4 5gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project ${GCP_PROJECT} 6Fetching cluster endpoint and auth data. 7kubeconfig entry generated for dev-cluster. For better readability you may want to rename the context id for the newly created cluster\n1kubectl config rename-context gke_${GCP_PROJECT}_europe-west9-a_dev-cluster dev-cluster 2Context \u0026#34;gke_${GCP_PROJECT}_europe-west9-a_dev-cluster\u0026#34; renamed to \u0026#34;dev-cluster\u0026#34;. 3 4kubectl config get-contexts 5CURRENT NAME CLUSTER AUTHINFO NAMESPACE 6* dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster 7 k3d-crossplane k3d-crossplane admin@k3d-crossplane Check that you can call our brand new GKE API\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3gke-dev-cluster-main-np-d0d978f9-5fc0 Ready \u0026lt;none\u0026gt; 10m v1.24.1-gke.1400 That's great üéâ we know have a GKE cluster up and running.\nIn our next article we'll see how to use a GitOps engine to run all the above steps.\n","link":"https://blog.ogenki.io/post/crossplane_k3d/","section":"post","tags":["kubernetes","infrastructure"],"title":"My Kubernetes cluster in GKE with `Crossplane`"},{"body":"I‚Äôm currently on my flight back from the Kubecon EU 2022 and plenty of ideas are currently flooding my brain. Actually this has been a long time I‚Äôm considering starting my own blog post. My main target here is to play and learn about new technologies and share my experimentations and findings. While I‚Äôm writing this I don‚Äôt know what will be the topics in a few months but you‚Äôll find here my next target: ‚û°Ô∏è Creating a Kubernetes cluster on a cloud provider for my future experimentations. I chose to use GKE provision through Crossplane to begin with. More info here.\n","link":"https://blog.ogenki.io/about/","section":"","tags":null,"title":"What is `CDKS` (Cloud and Devops Knowledge Sharing)?"},{"body":"","link":"https://blog.ogenki.io/tags/local/","section":"tags","tags":null,"title":"local"},{"body":"In order to install binaries and to be able to switch from a version to another I like to use asdf.\nLet's install k3d\n1asdf plugin-add k3d Check the versions available\n1asdf list-all k3d| tail -n 3 25.4.0-dev.3 35.4.0 45.4.1 We'll install the latest version\n1asdf install k3d $(asdf latest k3d) 2* Downloading k3d release 5.4.1... 3k3d 5.4.1 installation was successful! Finally we can switch from a version to another. We can set a global version that would be used on all directories.\n1asdf global k3d 5.4.1 or use a local version depending on the current directory\n1cd /tmp 2asdf local k3d 5.4.1 3 4asdf current k3d 5k3d 5.4.1 /tmp/.tool-versions ","link":"https://blog.ogenki.io/post/asdf/asdf/","section":"post","tags":["tooling","local"],"title":"Manage tools versions with `asdf`"},{"body":"","link":"https://blog.ogenki.io/tags/tooling/","section":"tags","tags":null,"title":"tooling"},{"body":"","link":"https://blog.ogenki.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://blog.ogenki.io/categories/devxp/","section":"categories","tags":null,"title":"devxp"},{"body":"","link":"https://blog.ogenki.io/tags/helm/","section":"tags","tags":null,"title":"Helm"},{"body":"Template challenge Here you‚Äôll be able to practice in order to get familiar with some of the possibilities offered by a templating language.\n(pro tip: Don't forget the testing)\nThese examples may seem useless but the purpose of this is just playing with templates.\n1 - \u0026quot;Configuration depends on region\u0026quot; Create a secret that contains a key 'secret' and a value \u0026quot;myEuropeanSecret\u0026quot;. Set an environment variable from this secret only if the value global.region is 'eu-west1' So the first step is to add the values into the values.yaml file.\n1global: 2 region: eu-west-1 2 - \u0026quot;Create only if\u0026quot; Create a job that prints the pod's IP on stdout based on a boolean value printIP.enabled. Use the \u0026quot;busybox\u0026quot; image and the command wget -qO - http://ipinfo.io/ip. The job should be created only if the value is True, before every other resource has been created (pre-install and pre-upgrade hooks)\n3 - \u0026quot;Looping\u0026quot; Given the following values, create a loop whether in the deployment or in the helpers.tpl file in order to add the environment variables.\n1envVars: 2 key1: value1 3 key2: value2 4 key3: value3 4 - \u0026quot;Playing with strings and sprigs\u0026quot; Add to the \u0026quot;common labels\u0026quot;, a new label \u0026quot;codename\u0026quot; with a value composed with the release name, the chart name and the date in the form 20061225. The release name must be at most 3 characters long. The whole string has to be in snakecase.\n(you should get something like codename: rel_web_20210215)\n5 - We want to create a list of etcd hosts in the form of \u0026quot;etcd-0,etcd-1,etcd-2\u0026quot; based on a integer that defines the number of etcd hosts 1etcd: 2 count: 5 This list has to be defined in an environment variable ETCD_HOSTS\nProposition of solutions try_first You should try to find a solution by your own to the above exercises before checking these solutions\n1 The following command generates a secrets in the templates directory\n1kubectl create secret generic --dry-run=client eu-secret --from-literal=secret=\u0026#39;myEuropeanSecret\u0026#39; -o yaml | kubectl neat \u0026gt; templates/secret.yaml Then we'll enclose the environment variable definition with a condition depending on the region in the deployment template.\n1 {{- if eq .Values.global.region \u0026#34;eu-west-1\u0026#34; }} 2 - name: eu-secret 3 valueFrom: 4 secretKeyRef: 5 name: eu-secret 6 key: secret 7 {{- end }} 2 First of all we need to add a new value\n1printIP: 2 enabled: True Then this command will generate a job yaml\n1kubectl create job my-ip --dry-run=client --image=busybox -o yaml -- wget -qO - http://ipinfo.io/ip | kubectl neat \u0026gt; templates/job.yaml If we enclose the whole yaml, it won't be created if the boolean is False. With the hook annotation here, the job will be created before any other resource will be applied. We defined a delete policy \u0026quot;hook-failed\u0026quot; in order to keep the job, otherwise it would have been deleted.\n1{{- if .Values.printIP.enabled -}} 2apiVersion: batch/v1 3kind: Job 4metadata: 5 name: my-ip 6 annotations: 7 \u0026#34;helm.sh/hook\u0026#34;: pre-install,pre-upgrade 8 \u0026#34;helm.sh/hook-weight\u0026#34;: \u0026#34;1\u0026#34; 9 \u0026#34;helm.sh/hook-delete-policy\u0026#34;: hook-failed 10... 11{{- end -}} 3 If we want to keep the deployment easy to read, we would prefer adding the code in the _helpers.tpl\n1{{/* 2Environment variables 3*/}} 4{{- define \u0026#34;web.envVars\u0026#34; -}} 5{{- range $key, $value := .Values.envVars }} 6- name: {{ $key }} 7 value: {{ $value }} 8{{- end }} 9{{- end -}} Then this new variable could be used in the deployment as follows\n1 env: 2 {{- include \u0026#34;web.envVars\u0026#34; . | nindent 12 }} 4 The common labels can be changed in the file templates/_helpers.tpl. Here's a proposal This one is tricky, I needed to dig back into the charts available in the stable github repository.\n1codename: {{ printf \u0026#34;%s %s %s\u0026#34; (.Release.Name | trunc 3) .Chart.Name (now | date \u0026#34;20060102\u0026#34;) | snakecase }} 5 Here's an option to achieve the expected results.\n1 env: 2 - name: ETCD_HOSTS 3 value: \u0026#34;{{ range $index, $e := until (.Values.etcd.count|int) }}{{- if $index }},{{end}}etcd-{{ $index }}{{- end }}\u0026#34; ","link":"https://blog.ogenki.io/post/series/workshop_helm/templating/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Templating exercises"},{"body":"","link":"https://blog.ogenki.io/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://blog.ogenki.io/series/workshop-helm/","section":"series","tags":null,"title":"Workshop Helm"},{"body":"Create a simple webserver chart In order to get familiar with a typical chart we will create a simple webserver chart.\n1$ helm create web 2Creating web The above command will create a chart directory named web\nweb/ charts directory that contains the subcharts Chart.yaml metadatas (author, version, description), dependencies and more templates contains all the templates basically kubernetes resources in the form of templated yaml files. (go template) deployment.yaml helpers.tpl helpers, functions that can be used from the templates. hpa.yaml ingress.yaml NOTES.txt This file is used to print information after a release has been successfully installed. serviceaccount.yaml service.yaml tests contains a job that will run a command to check the application after it has been installed. test-connection.yaml values.yaml Maybe the most important file. We‚Äôll play with the values to define how the kubernetes resources will be rendered. Testing the chart Here‚Äôs a combo if you want to check properly your chart before actually deploying it:\ntemplate + lint + kubeval + test\nGolang errors When you add templating changes, you should run the command helm template --debug \u0026lt;chart_dir\u0026gt;\n1$ helm template --debug web 2install.go:173: [debug] Original chart version: \u0026#34;\u0026#34; 3install.go:190: [debug] CHART PATH: /tmp/web 4 5Error: parse error at (web/templates/_helpers.tpl:73): unexpected EOF 6helm.go:81: [debug] parse error at (web/templates/_helpers.tpl:73): unexpected EOF Read carefully if there are error messages. Always use the option --debug to see the template rendering.\nChart linting The command helm lint \u0026lt;chart_dir\u0026gt; verifies that the chart is well-formed.\n1$ helm lint web/ 2==\u0026gt; Linting web/ 3[ERROR] Chart.yaml: apiVersion \u0026#39;v3\u0026#39; is not valid. The value must be either \u0026#34;v1\u0026#34; or \u0026#34;v2\u0026#34; 4[INFO] Chart.yaml: icon is recommended 5[ERROR] Chart.yaml: chart type is not valid in apiVersion \u0026#39;v3\u0026#39;. It is valid in apiVersion \u0026#39;v2\u0026#39; 6 7Error: 1 chart(s) linted, 1 chart(s) failed Validate Kubernetes resources In order to validate that the rendered kubernetes objects are well-formed we‚Äôll make use of a tool named kubeval.\nThis is even easier by using the Helm plugin.\nInstall the plugin:\n1$ helm plugin install https://github.com/instrumenta/helm-kubeval 2Installing helm-kubeval v0.13.0 ... 3helm-kubeval 0.13.0 is installed. Then check the chart as follows\n1$ helm kubeval web 2The file web/templates/serviceaccount.yaml contains a valid ServiceAccount 3The file web/templates/secret.yaml contains a valid Secret 4... Now you can safely install the chart\n1$ helm upgrade --install web web 2Release \u0026#34;web\u0026#34; has been upgraded. Happy Helming! 3NAME: web 4LAST DEPLOYED: Mon Feb 15 18:22:23 2021 5NAMESPACE: default 6STATUS: deployed 7REVISION: 1 Check that the application works as expected This is a good practice to add tests under the directory template/tests.\nBasically, this is achieved with a job that you can call when the release is already installed (just after)\nIt returns a code 0 if the command succeeds.\nIn the chart we‚Äôve already generated there‚Äôs a job that checks the webserver availability.\nCheck that the release is already installed\n1helm list 2NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION 3web default 1 2021-02-15 15:11:09.036602795 +0100 CET deployed web-0.1.0 1.16.0 1helm test web 2NAME: web 3LAST DEPLOYED: Mon Feb 15 15:11:09 2021 4NAMESPACE: default 5STATUS: deployed 6REVISION: 1 7TEST SUITE: web-test-connection 8Last Started: Mon Feb 15 16:55:17 2021 9Last Completed: Mon Feb 15 16:55:19 2021 10Phase: Succeeded Dependencies Sometimes, the application requires another component to work (caching, database, persistence ‚Ä¶).\nThis dependency system has to be used with caution because this is generally recommended to manage the applications lifecycles independently from each other.\nLet‚Äôs say that our webserver need to store the information related to the sessions in a Redis server.\nWe‚Äôll add a redis server to our web application by declaring the dependency in the file chart.yaml.\n1dependencies: 2 - name: redis 3 version: \u0026#34;12.6.4\u0026#34; 4 repository: https://charts.bitnami.com/bitnami 5 condition: redis.enabled As you may have noticed, this dependency will be pulled only if the condition redis.enabled is True.\nSo we need to change our values.yaml accordingly:\n1redis: 2 enabled: True 3 master: 4 persistence: 5 enabled: False Check all the available values for this chart here.\nWhenever you add a dependency and you‚Äôre using a local chart (on your laptop), you must run the following command to pull it\n1helm dep update web 2Hang tight while we grab the latest from your chart repositories... 3‚Ä¶. 4...Successfully got an update from the \u0026#34;bitnami\u0026#34; chart repository 5Update Complete. ‚éàHappy Helming!‚éà 6Saving 1 charts 7Downloading redis from repo https://charts.bitnami.com/bitnami 8Deleting outdated charts The dependencies are stored in the directory charts.\nAfter testing your changes you can install the release with the command\nhelm upgrade --install \u0026lt;release_name\u0026gt; \u0026lt;chart_dir\u0026gt;\n1helm upgrade --install web web 2Release \u0026#34;web\u0026#34; has been upgraded. Happy Helming! 3NAME: web 4LAST DEPLOYED: Mon Feb 15 18:22:23 2021 5NAMESPACE: default 6STATUS: deployed 7REVISION: 2 You can notice that your webserver has been successfully installed along with a HA Redis cluster\n1kubectl get po 2NAME READY STATUS RESTARTS AGE 3web-74bf5c6c66-fjsmb 1/1 Running 0 3h14m 4web-test-connection 0/1 Completed 0 90m 5web-redis-master-0 1/1 Running 0 3m14s 6web-redis-slave-0 1/1 Running 0 3m14s 7web-redis-slave-1 1/1 Running 0 2m42s Hooks Helm comes with a hook system that allows it to run jobs at given times of the lifecycle.\nThe description is crystal clear in the documentation and you‚Äôll have the opportunity to add one later on during this workshop.\nMastering the Golang template The main challenge when you start using Helm is to learn all the tips and tricks of the Golang template\nThe official Helm documentation is very useful for that.\nYour best friends when you write Helm templates are the Sprig functions, you should definitely add this to your bookmarks.\nFurthermore, even if it has been deprecated, you should clone/fork the original stable chart repository. Indeed it has a wide range of examples.\nNote that most of the time, if you want to keep the kubernetes manifests readable, you would put most of the code in what we call helpers files. There‚Äôs often at least one named _helpers.tpl.\nNote: Even if you can do pretty advanced things with this templating language, you shouldn‚Äôt overuse it in order to keep the kubernetes resources readable and the chart maintainable.\n‚û°Ô∏è Next: Application lifecycle using Helm\n","link":"https://blog.ogenki.io/post/series/workshop_helm/build_chart/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Build your first chart"},{"body":"Apply a change, anything. For example we will add a label stage: dev. Edit the file templates/_helpers.tpl\n1{{- define \u0026#34;web.labels\u0026#34; -}} 2stage: \u0026#34;dev\u0026#34; 3... Deploy a new revision with the same command we ran previously\n1helm upgrade --install web web Now we can have a look to the changes we‚Äôve made so far to the release\n1$ helm history web 2REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 31 Mon Feb 15 15:11:09 2021 superseded web-0.1.0 1.16.0 Install complete 4... 54 Mon Feb 15 21:15:25 2021 superseded web-0.1.0 1.16.0 Upgrade complete 65 Mon Feb 15 21:21:21 2021 deployed web-0.1.0 1.16.0 Upgrade complete We can then check what would be the changes if we rollback to the previous revision\n1helm diff rollback web 4 2default, web, Deployment (apps) has changed: 3 # Source: web/templates/deployment.yaml 4 apiVersion: apps/v1 5 kind: Deployment 6 metadata: 7 name: web 8 labels: 9- stage: \u0026#34;dev\u0026#34; 10 helm.sh/chart: web-0.1.0 11... Now that we‚Äôre sure we can safely rollback to the previous revision\n1helm rollback web 4 2Rollback was a success! Happy Helming! ‚û°Ô∏è Next: Helm templating challenge\n","link":"https://blog.ogenki.io/post/series/workshop_helm/lifecycle/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Lifecycle operations"},{"body":"Helm‚Äôs configuration is stored in the environment variable $HELM_CONFIG_HOME , by default $HOME/.config/helm\nAll the environment variables are described in the documentation there.\nHere are a few tools (with a wide adoption) that will add capabilities to Helm.\nPlugins There are several plugins in order to extend Helm‚Äôs features.\nSome of them are really useful (kubeval, diff, secrets).\nHelmfile Helmfile is a really useful tool that allows you to declare the state of the releases on your cluster.\nIt helps keeping a central view of the releases deployed on a given cluster.\nIt automatically configures repositories, pulls dependencies and it is very helpful to build CI/CD workflows.\nOf course it uses Helm under the hood and a few modules/plugins such as secrets decryption, helm diff\nThese steps are very basic, you should have a look at the documentation for further details.\nInstall the helmdiff plugin (used by helmfile)\n1helm plugin install https://github.com/databus23/helm-diff We‚Äôll make use of some examples provided by CloudPosse\n1git clone git@github.com:cloudposse/helmfiles.git 2cd helmfiles Let‚Äôs say we want to install the kubernetes dashboard and the reloader tool.\n1cat \u0026gt; releases/kubernetes-dashboard/dev.yaml \u0026lt;\u0026lt;EOF 2installed: True 3banner: \u0026#34;Workshop cluster\u0026#34; 4EOF Now we‚Äôll create our main helmfile.yaml that describes all the releases we want to install\n1cat \u0026gt; helmfile.yaml \u0026lt;\u0026lt;EOF 2helmfiles: 3 - path: \u0026#34;releases/kubernetes-dashboard/helmfile.yaml\u0026#34; 4 values: 5 - releases/kubernetes-dashboard/dev.yaml 6 - path: \u0026#34;releases/reloader/helmfile.yaml\u0026#34; 7 values: 8 - installed: True 9EOF Now we can see what changes will be applied.\n1helmfile diff 2Adding repo stable https://charts.helm.sh/stable 3\u0026#34;stable\u0026#34; has been added to your repositories 4 5Comparing release=kubernetes-dashboard, chart=stable/kubernetes-dashboard 6******************** 7 8 Release was not present in Helm. Diff will show entire contents as new. 9 10‚Ä¶ The command helm sync will install the releases\n1helmfile sync 2Adding repo stable https://charts.helm.sh/stable 3\u0026#34;stable\u0026#34; has been added to your repositories 4 5Affected releases are: 6 kubernetes-dashboard (stable/kubernetes-dashboard) UPDATED 7 8Upgrading release=kubernetes-dashboard, chart=stable/kubernetes-dashboard 9Release \u0026#34;kubernetes-dashboard\u0026#34; does not exist. Installing it now. 10NAME: kubernetes-dashboard 11... You can list all the releases managed by the local helmfile.\n1helmfile list 2NAME NAMESPACE ENABLED LABELS 3kubernetes-dashboard kube-system true chart:kubernetes-dashboard,component:monitoring,namespace:kube-system,repo:stable,vendor:kubernetes 4reloader reloader true chart:stakater/reloader,component:reloader,namespace:reloader,repo:stakater,vendor:stakater Delete all the releases\n1helmfile delete 2Listing releases matching ^reloader$ 3reloader reloader 1 2021-02-16 10:10:35.378800455 +0100 CET deployed reloader-v0.0.68 v0.0.68 4 5Deleting reloader 6release \u0026#34;reloader\u0026#34; uninstalled ‚û°Ô∏è Next: Build a Helm chart\n","link":"https://blog.ogenki.io/post/series/workshop_helm/ecosystem/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Ecosystem"},{"body":"Looking for a chart Helm works with what is called a ‚Äúchart‚Äù. A chart is basically a package of yaml resources that support a templating language.\nBefore building our own chart we should always have a look of what is available in the community. There are often charts that fits our needs.\nThese charts can be installed from different sources: a Helm chart repository, a local archive or chart directory.\nFirst of all, let‚Äôs say that we want to install a Wordpress instance on an empty infrastructure.\nWe‚Äôll need to provision a database as well as the Wordpress application.\nLet‚Äôs look for a wordpress chart !\nIf you just installed Helm, your repositories list should be empty\n1helm repo list We‚Äôre going to check what are the Wordpress charts available in the artifacthub.\nYou can either browse from the web page or use the command\n1helm search hub wordpress 2URL CHART VERSION APP VERSION DESCRIPTION 3https://artifacthub.io/packages/helm/bitnami/wo... 10.6.4 5.6.1 Web publishing platform for building blogs and ... 4https://artifacthub.io/packages/helm/groundhog2... 0.2.6 5.6.0-apache A Helm chart for Wordpress on Kubernetes 5https://artifacthub.io/packages/helm/seccurecod... 2.4.0 4.0 Insecure \u0026amp; Outdated Wordpress Instance: Never e... 6https://artifacthub.io/packages/helm/presslabs/... 0.10.5 0.10.5 Presslabs WordPress Operator Helm Chart Using the Hub there are a few things that can help to choose the best option.\nFirst of all the number of stars obviously and whether the artifact comes from a verified publisher or signed by the maintainer.\nWe‚Äôll get the one provided by Bitnami. In the chart page you‚Äôll be guided with the commands to add Bitnami‚Äôs repository.\n1helm repo add bitnami https://charts.bitnami.com/bitnami 2\u0026#34;bitnami\u0026#34; has been added to your repositories 3 4helm repo update From now on we can install all the charts published by Bitnami:\n1helm search repo bitnami 2NAME CHART VERSION APP VERSION DESCRIPTION 3bitnami/bitnami-common 0.0.9 0.0.9 DEPRECATED Chart with custom templates used in ... 4bitnami/airflow 8.0.3 2.0.1 Apache Airflow is a platform to programmaticall... 5bitnami/apache 8.2.3 2.4.46 Chart for Apache HTTP Server 6bitnami/aspnet-core 1.2.3 3.1.9 ASP.NET Core is an open-source framework create... 7bitnami/cassandra 7.3.2 3.11.10 Apache Cassandra is a free and open-source dist... Inspect the chart OK let‚Äôs get back to what we want to achieve: Installing a Wordpress instance.\nNow that we identified the chart, we‚Äôre going to check what it actually does. You should always check what will be installed.\nyou can download the chart on your laptop and have a look to its content 1helm pull --untar bitnami/wordpress 2 3tree -L 2 wordpress/ 4wordpress/ 5‚îú‚îÄ‚îÄ Chart.lock 6‚îú‚îÄ‚îÄ charts 7‚îÇ ‚îú‚îÄ‚îÄ common 8‚îÇ ‚îî‚îÄ‚îÄ mariadb 9‚îú‚îÄ‚îÄ Chart.yaml 10‚îú‚îÄ‚îÄ ci 11‚îÇ ‚îú‚îÄ‚îÄ ct-values.yaml 12‚îÇ ‚îú‚îÄ‚îÄ ingress-wildcard-values.yaml 13‚îÇ ‚îú‚îÄ‚îÄ values-hpa-pdb.yaml 14‚îÇ ‚îî‚îÄ‚îÄ values-metrics-and-ingress.yaml 15‚îú‚îÄ‚îÄ README.md 16‚îú‚îÄ‚îÄ templates 17‚îÇ ‚îú‚îÄ‚îÄ configmap.yaml 18‚Ä¶ 19‚îÇ ‚îú‚îÄ‚îÄ tests 20‚îÇ ‚îî‚îÄ‚îÄ tls-secrets.yaml 21‚îú‚îÄ‚îÄ values.schema.json 22‚îî‚îÄ‚îÄ values.yaml read carefully the readme check what are the dependencies pulled by this chart 1helm show chart bitnami/wordpress 2annotations: 3 category: CMS 4apiVersion: v2 5appVersion: 5.6.1 6dependencies: 7- condition: mariadb.enabled 8 name: mariadb 9 repository: https://charts.bitnami.com/bitnami 10 version: 9.x.x 11- name: common 12 repository: https://charts.bitnami.com/bitnami 13 tags: 14 - bitnami-common 15 version: 1.x.x 16... Note: that the wordpress chart defines the mariadb chart as dependency\nLook at the available values 1helm show values bitnami/wordpress Our first release Our next step will be to set our desired values. Indeed you mentioned that Helm uses a templating language to render the manifests. This will help us to configure our instance according to our environment.\nno persistency at all, this is just a workshop 2 replicas for the wordpress instance a database named ‚Äúfoodb‚Äù an owner ‚Äúfoobar‚Äù for this database passwords All the charts have a file named values.yaml that contains the default values.\nThese values can be overridden at the command line with --set or we can put them in a yaml file that we‚Äôll use with the -f parameter.\nFor this exercise we‚Äôll create a file named ‚Äúoverride-values.yaml‚Äù and we‚Äôll use the command line for sensitive information.\n1wordpressUsername: foobar 2wordpressPassword: \u0026#34;\u0026#34; 3wordpressBlogName: Foo\u0026#39;s Blog! 4replicaCount: 2 5persistence: 6 enabled: false 7service: 8 type: ClusterIP 9mariadb: 10 auth: 11 rootPassword: \u0026#34;\u0026#34; 12 database: foodb 13 username: foobar 14 password: \u0026#34;\u0026#34; 15 primary: 16 persistence: 17 enabled: false Note: In order to define the values of a subchart you must put the chart name as the first key. here mariadb.values of the mariadb chart.\nHere we go!\nFirst of all we‚Äôll run it in dry-run mode in order to check the yaml rendering (be careful, the passwords are printed in plain text)\n1helm install foo-blog bitnami/wordpress \\ 2-f override-values.yaml \\ 3--set mariadb.auth.rootPassword=r00tP4ss \\ 4--set mariadb.auth.password=us3rP4ss \\ 5--set wordpressPassword=azerty123 \\ 6--dry-run Another word you need to know is Release.\n‚ÄúA Release is an instance of a chart running in a Kubernetes cluster‚Äù. Our release name here is foo-blog\nIf the output looks OK we can install our wordpress, just remove the --dry-run parameter\n1helm install foo-blog bitnami/wordpress -f override-values.yaml --set mariadb.auth.rootPassword=\u0026#34;r00tP4ss\u0026#34; --set mariadb.auth.password=\u0026#34;us3rP4ss\u0026#34; --set wordpressPassword=\u0026#34;azerty123\u0026#34; 2NAME: foo-blog 3LAST DEPLOYED: Fri Feb 12 16:33:21 2021 4NAMESPACE: default 5STATUS: deployed 6REVISION: 1 7NOTES: 8** Please be patient while the chart is being deployed ** 9 10Your WordPress site can be accessed through the following DNS name from within your cluster: 11 12 foo-blog-wordpress.default.svc.cluster.local (port 80) 13 14To access your WordPress site from outside the cluster follow the steps below: 15 161. Get the WordPress URL by running these commands: 17 18 NOTE: It may take a few minutes for the LoadBalancer IP to be available. 19 Watch the status with: \u0026#39;kubectl get svc --namespace default -w foo-blog-wordpress\u0026#39; 20 21 export SERVICE_IP=$(kubectl get svc --namespace default foo-blog-wordpress --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\u0026#34;) 22 echo \u0026#34;WordPress URL: http://$SERVICE_IP/\u0026#34; 23 echo \u0026#34;WordPress Admin URL: http://$SERVICE_IP/admin\u0026#34; 24 252. Open a browser and access WordPress using the obtained URL. 26 273. Login with the following credentials below to see your blog: 28 29 echo Username: foobar 30 echo Password: $(kubectl get secret --namespace default foo-blog-wordpress -o jsonpath=\u0026#34;{.data.wordpress-password}\u0026#34; | base64 --decode) When the release has been successfully installed you‚Äôll get the above ‚ÄúNOTES‚Äù that are very useful to get access to your application. You just have to copy/paste.\nBut first of all we‚Äôre going to check that the pods are actually running\n1kubectl get deploy,sts 2NAME READY UP-TO-DATE AVAILABLE AGE 3deployment.apps/foo-blog-wordpress 2/2 2 2 55m 4 5NAME READY AGEkubectl get deploy,sts 6statefulset.apps/foo-blog-mariadb 1/1 55m We didn‚Äôt define an ingress for the purpose of the workshop, therefore we‚Äôll use a port-forward\n1kubectl port-forward svc/foo-blog-wordpress 9090:80 Then open a browser using the URL http://localhost:9090/admin, you‚Äôll be prompted to fill in the credentials you defined above. (wordpressPassword)\nWe‚Äôll check the database credentials too as follows\n1MARIADB=$(kubectl get po -l app.kubernetes.io/name=mariadb -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 1kubectl exec -ti ${MARIADB} -- bash -c \u0026#39;mysql -u foobar -pus3rP4ss\u0026#39; 2Welcome to the MariaDB monitor. Commands end with ; or \\g. 3Your MariaDB connection id is 372 4Server version: 10.5.8-MariaDB Source distribution 5 6Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. 7 8Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. 9 10MariaDB [(none)]\u0026gt; SHOW GRANTS; 11+-------------------------------------------------------------------------------------------------------+ 12| Grants for foobar@% | 13+-------------------------------------------------------------------------------------------------------+ 14| GRANT USAGE ON *.* TO `foobar`@`%` IDENTIFIED BY PASSWORD \u0026#39;*CD5BE357349BDA710A444B0BD741E8EB12B8BC2C\u0026#39; | 15| GRANT ALL PRIVILEGES ON `foodb`.* TO `foobar`@`%` | 16+-------------------------------------------------------------------------------------------------------+ 172 rows in set (0.000 sec) Delete the wordpress release\n1helm uninstall foo-blog Deploy a complete monitoring stack with a single command! The purpose of this step is to show that, even if the stack is composed of dozens of manifest, Helm makes things easy.\n1helm repo add prometheus-community https://prometheus-community.github.io/helm-charts 2\u0026#34;prometheus-community\u0026#34; has been added to your repositories 3 4helm repo update 1helm install kube-prometheus prometheus-community/kube-prometheus-stack --create-namespace --namespace monitoring 2NAME: kube-prometheus 3LAST DEPLOYED: Fri Feb 12 18:03:05 2021 4NAMESPACE: monitoring 5STATUS: deployed 6REVISION: 1 7NOTES: 8kube-prometheus-stack has been installed. Check its status by running: 9 kubectl --namespace monitoring get pods -l \u0026#34;release=kube-prometheus\u0026#34; Check that all the pods are running and run a port-forward\n1kubectl port-forward -n monitoring svc/kube-prometheus-grafana 9090:80 Then open a browser using the URL http://localhost:9090/admin\ndefault credentials: admin / prom-operator\nYou should browse a few minutes over all the dashboards available. There is pretty useful info.\nYou can then have a look to the resources that have been applied with a single command line as follows\n1helm get manifest -n monitoring kube-prometheus Well for a production ready prometheus we would have played a bit with the values but you get the point.\nDelete the kube-prometheus stack\n1helm uninstall -n monitoring kube-prometheus ‚û°Ô∏è Next: Helm ecosystem\n","link":"https://blog.ogenki.io/post/series/workshop_helm/third_party/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Third party charts"},{"body":"Requirements docker k3d \u0026gt;5.x.x helm \u0026gt;3.x.x helmfile In order to have an easily provisioned temporary playground we‚Äôll make use of k3d which is a lightweight local Kubernetes instance.\nAfter installing the binary you should enable the completion (bash or zsh) as follows (do the same for both helm and k3d).\n1source \u0026lt;(k3d completion bash) Then create the sandbox cluster named ‚Äúhelm-workshop‚Äù\n1k3d cluster create helm-workshop 2INFO[0000] Created network \u0026#39;k3d-helm-workshop\u0026#39; 3INFO[0000] Created volume \u0026#39;k3d-helm-workshop-images\u0026#39; 4INFO[0001] Creating node \u0026#39;k3d-helm-workshop-server-0\u0026#39; 5INFO[0006] Creating LoadBalancer \u0026#39;k3d-helm-workshop-serverlb\u0026#39; 6INFO[0007] (Optional) Trying to get IP of the docker host and inject it into the cluster as \u0026#39;host.k3d.internal\u0026#39; for easy access 7INFO[0010] Successfully added host record to /etc/hosts in 2/2 nodes and to the CoreDNS ConfigMap 8INFO[0010] Cluster \u0026#39;helm-workshop\u0026#39; created successfully! 9INFO[0010] You can now use it like this: 10kubectl cluster-info Note that your current configuration should be automatically switched to the newly created cluster.\n1$ kubectl config current-context 2k3d-helm-workshop Playing with third party charts Environment and ecosystem Build your first chart Application lifecycle Templating challenge Other considerations Hosting and versioning Most of the time we would want to share the charts in order to be used on different systems or to pull the dependencies.\nThere are multiple options for that, here are the ones that are generally used.\nChartmuseum is the official solution. This is a pretty simple webserver that exposes a Rest API. Harbor. Its main purpose is to store images (containers), but it offers many other features such as vulnerability scanning, images signing and integrates chartmuseum. Artifactory can be used to stored Helm charts too An OCI store (container registry). Pushing the charts into a central location requires to manage the versions of the charts. Any changes should trigger a version bump in the Chart.yaml file.\nSecrets management One sensitive topic that we didn‚Äôt talk about is how to handle secrets.\nThis is not directly related to Helm but this is a general issue on Kubernetes.\nThere are many options, some of them work great with Helm, some others require managing secrets apart from Helm releases.\nIn the ArgoCD documentation they tried to reference all the options available.\nCleanup Pretty simple we‚Äôll drop the whole k3d cluster\n1k3d cluster delete helm-workshop ","link":"https://blog.ogenki.io/post/series/workshop_helm/intro/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop"},{"body":"","link":"https://blog.ogenki.io/categories/containers/","section":"categories","tags":null,"title":"containers"},{"body":"RBAC is the method used by Kubernetes to authorize access to API resources.\n‚ÑπÔ∏è When it makes sense you can use the default roles that are available in all Kubernetes installation instead of having to maintain custom ones.\nFor this lab what we want to achieve is to give permissions the following permissions to an application myapp:\nread the configmaps in the namespace foo List the pods in all the namespaces It is a good practice to configure your pod to make use of a serviceaccount. A serviceaccount are used to identify applications and give them permissions if necessary.\nCreate a service account\n1kubectl create serviceaccount myapp 2serviceaccount/myapp created When a service account is created, a token is automatically generated and stored in a secret.\n1kubectl describe sa myapp | grep -i token 2Mountable secrets: myapp-token-bz2zq 3Tokens: myapp-token-bz2zq 4 5kubectl get secret myapp-token-bz2zq --template={{.data.token}} | base64 -d 6eyJhb...EYxhjI_ckZ74A Using a tool to decode the JWT token you should see the following content\n1kubectl get secret myapp-token-bz2zq --template={{.data.token}} | base64 -d | jwt decode - 2 3Token header 4------------ 5{ 6 \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, 7 \u0026#34;kid\u0026#34;: \u0026#34;IdsXYO6E93xozgJg-LY2oETTPEHBJjydTU4vF2wy-wg\u0026#34; 8} 9 10Token claims 11------------ 12{ 13 \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, 14 \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;foo\u0026#34;, 15 \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;myapp-token-bz2zq\u0026#34;, 16 \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;myapp\u0026#34;, 17 \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;eb606bdc-b713-4b7c-8da8-c4f71075995e\u0026#34;, 18 \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:foo:myapp\u0026#34; 19} We're going to create a deployment that will be configured to used this serviceaccount. In the yaml you'll notice that we defined the serviceAccountName.\n1kubectl apply -f content/resources/kubernetes_workshop/rbac/deployment.yaml 2deployment.apps/myapp created As we didn't assigned any permissions to this serviceaccount, our application won't be able to call any of the API endpoints\n1POD_NAME=$(kubectl get po -l app=myapp -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 2 3kubectl exec ${POD_NAME} -- kubectl auth can-i -n foo --list 4Resources Non-Resource URLs Resource Names Verbs 5selfsubjectaccessreviews.authorization.k8s.io [] [] [create] 6selfsubjectrulesreviews.authorization.k8s.io [] [] [create] 7 [/.well-known/openid-configuration] [] [get] 8 [/api/*] [] [get] 9 [/api] [] [get] 10 [/apis/*] [] [get] 11 [/apis] [] [get] 12 [/healthz] [] [get] 13 [/healthz] [] [get] 14 [/livez] [] [get] 15 [/livez] [] [get] 16 [/openapi/*] [] [get] 17 [/openapi] [] [get] 18 [/openid/v1/jwks] [] [get] 19 [/readyz] [] [get] 20 [/readyz] [] [get] 21 [/version/] [] [get] 22 [/version/] [] [get] 23 [/version] [] [get] 24 [/version] [] [get] In order to allow it to read configmaps in the namespace foo, we're going to create 2 resources:\nA role which will describe the permissions and which is bounded to a namespace A rolebinding to assign this role to our application (serviceaccount) Create the role\n1kubectl apply -f content/resources/kubernetes_workshop/rbac/role.yaml 2role.rbac.authorization.k8s.io/read-configmaps created And assign it to the serviceaccount we've created previously\n1kubectl create rolebinding -n foo myapp-configmap --serviceaccount=foo:myapp --role=read-configmaps 2rolebinding.rbac.authorization.k8s.io/myapp-configmap created Note that in the above command the serviceaccount must be specified with the namespace as a prefix and separated by a semicolon.\nYou don't have to restart the pod to get the permissions enabled.\n1kubectl exec ${POD_NAME} -- kubectl auth can-i get configmaps -n foo 2yes 3 4kubectl exec ${POD_NAME} -- kubectl get cm 5NAME DATA AGE 6kube-root-ca.crt 1 3d20h 7helloworld 2 2d2h This is possible thanks to the token mounted within the container\n1kubectl exec -ti ${POD_NAME} -- bash -c \u0026#39;curl -skH \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; https://kubernetes.default/api/v1/namespaces/foo/configmaps\u0026#39; 2{ 3 \u0026#34;kind\u0026#34;: \u0026#34;ConfigMapList\u0026#34;, 4 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 5 \u0026#34;metadata\u0026#34;: { 6 \u0026#34;resourceVersion\u0026#34;: \u0026#34;76334\u0026#34; 7 }, 8 \u0026#34;items\u0026#34;: [ 9 { 10 \u0026#34;metadata\u0026#34;: { 11 \u0026#34;name\u0026#34;: \u0026#34;kube-root-ca.crt\u0026#34;, 12 \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, 13 \u0026#34;uid\u0026#34;: \u0026#34;c352e4cd-3b88-4400-80a0-cbba318794e4\u0026#34;, 14... Finally we want to list the pods in all the namespaces of our cluster. we need:\nA clusterrole which will describe the permissions that are cluster wide. A clusterrolebinding to assign this clusterrole to our application (serviceaccount) 1$ kubectl apply -f content/resources/kubernetes_workshop/rbac/clusterrole.yaml 2clusterrole.rbac.authorization.k8s.io/list-pods created 1kubectl create clusterrolebinding -n foo myapp-pods --serviceaccount=foo:myapp --clusterrole=list-pods 2clusterrolebinding.rbac.authorization.k8s.io/myapp-pods created Now lets have a look to the permissions our applications has in the namespace foo\n1kubectl exec ${POD_NAME} -- kubectl auth can-i -n foo --list 2Resources Non-Resource URLs Resource Names Verbs 3selfsubjectaccessreviews.authorization.k8s.io [] [] [create] 4selfsubjectrulesreviews.authorization.k8s.io [] [] [create] 5pods [] [] [get list] 6configmaps [] [] [get watch list] 7 [/.well-known/openid-configuration] [] [get] 8 [/api/*] [] [get] 9 [/api] [] [get] 10 [/apis/*] [] [get] 11... ","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/rbac/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Manage permissions in Kubernetes"},{"body":"","link":"https://blog.ogenki.io/series/workshop-kubernetes/","section":"series","tags":null,"title":"Workshop Kubernetes"},{"body":"Events The first source of information when something goes wrong is the event stream. Note that you may want to sort them by creation time\n1kubectl get events -n foo --sort-by=.metadata.creationTimestamp 2... 320m Normal Created pod/web-85575f4476-5pbqv Created container nginx 420m Normal Started pod/web-85575f4476-5pbqv Started container nginx 520m Normal SuccessfulDelete replicaset/web-987f6cf9 Deleted pod: web-987f6cf9-mzsxd 620m Normal ScalingReplicaSet deployment/web Scaled down replica set web-987f6cf9 to 0 Logs Having a look to a pod's logs is just the matter of running\n1kubectl logs -f --tail=7 -c mysql wordpress-mysql-6c597b98bd-4mbbd 22021-06-24 08:27:38 1 [Note] - \u0026#39;::\u0026#39; resolves to \u0026#39;::\u0026#39;; 32021-06-24 08:27:38 1 [Note] Server socket created on IP: \u0026#39;::\u0026#39;. 42021-06-24 08:27:38 1 [Warning] Insecure configuration for --pid-file: Location \u0026#39;/var/run/mysqld\u0026#39; in the path is accessible to all OS users. Consider choosing a different directory. 52021-06-24 08:27:38 1 [Warning] \u0026#39;proxies_priv\u0026#39; entry \u0026#39;@ root@wordpress-mysql-6c597b98bd-4mbbd\u0026#39; ignored in --skip-name-resolve mode. 62021-06-24 08:27:38 1 [Note] Event Scheduler: Loaded 0 events 72021-06-24 08:27:38 1 [Note] mysqld: ready for connections. 8Version: \u0026#39;5.6.51\u0026#39; socket: \u0026#39;/var/run/mysqld/mysqld.sock\u0026#39; port: 3306 MySQL Community Server (GPL) Alternatively you can use a tool made to display logs from multiple pods: stern. A better way to explore logs is to send them to a central location using a tool such as Loki or the well know EFK stack.\nHealth checks Kubernetes self healing system is mostly based on health checks. There are different types of health checks (please have a look to the official documentation).\nWe'll add a new plugin to kubectl which is really useful to export a resource while cleaning useless metadatas: neat\n1kubectl krew install neat 2Updated the local copy of plugin index. 3Installing plugin: neat 4Installed plugin: neat 5... Let's create a new deployment using the image nginx\n1kubectl create deploy web --image=nginx --dry-run=client -o yaml | kubectl neat \u0026gt; /tmp/web.yaml Edit its content and add an HTTP health check on port 80. The endpoint must return a code ranging between 200 and 400 and it has to be a relevant test that shows the actual availability of the service.\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 labels: 5 app: web 6 name: web 7spec: 8 replicas: 1 9 selector: 10 matchLabels: 11 app: web 12 template: 13 metadata: 14 creationTimestamp: null 15 labels: 16 app: web 17 spec: 18 containers: 19 - image: nginx 20 name: nginx 21 livenessProbe: 22 httpGet: 23 path: / 24 port: 80 25 initialDelaySeconds: 3 26 periodSeconds: 3 1kubectl apply -f /tmp/web.yaml 2deployment.apps/web created 3 4kubectl describe deploy web | grep Liveness: 5 Liveness: http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #failure=3 The pod should be up without any error\n1kubectl get po -l app=web 2NAME READY STATUS RESTARTS AGE 3web-85575f4476-6qvd5 1/1 Running 0 92s We're going to simulate a service being unavailable, just change the path being checked. Here we'll use another method to modify a resource by creating a patch and applying it.\nCreate a yaml /tmp/patch.yaml file\n1cat \u0026gt; /tmp/patch.yaml \u0026lt;\u0026lt;EOF 2spec: 3 template: 4 spec: 5 containers: 6 - name: nginx 7 livenessProbe: 8 httpGet: 9 path: /foobar 10EOF And we're going to apply our change as follows\n1kubectl patch deployment web --patch \u0026#34;$(cat /tmp/patch.yaml)\u0026#34; --record 2deployment.apps/web patched 3 4kubectl describe deployment web | grep Liveness: 5 Liveness: http-get http://:80/foobar delay=3s timeout=1s period=3s #success=1 #failure=3 Now our pod should start to fail, the number of restarts increases\n1kubectl get po -l app=web 2web-987f6cf9-n4rnb 1/1 Running 4 83s Until the pod enter in a CrashLoopBackOff, meaning that it constantly restarts.\n1kubectl get po -l app=web 2NAME READY STATUS RESTARTS AGE 3web-987f6cf9-n4rnb 0/1 CrashLoopBackOff 5 3m23s Describing the pod will give you a hint on the reason it restarts\n1kubectl describe po web-987f6cf9-n4rnb | tail -n 5 2Normal Created 4m7s (x3 over 4m30s) kubelet Created container nginx 3Normal Started 4m7s (x3 over 4m30s) kubelet Started container nginx 4Warning Unhealthy 3m56s (x9 over 4m26s) kubelet Liveness probe failed: HTTP probe failed with statuscode: 404 5Normal Killing 3m56s (x3 over 4m20s) kubelet Container nginx failed liveness probe, will be restarted 6Normal Pulling 3m56s (x4 over 4m35s) kubelet Pulling image \u0026#34;nginx\u0026#34; Rollback the latest change in order to return to a working state. Note that we used the option --record when we applied the patch. That helps saving changes history.\n1kubectl rollout history deployment web 2deployment.apps/web 3REVISION CHANGE-CAUSE 41 \u0026lt;none\u0026gt; 52 kubectl patch deployment web --patch=spec: 6 template: 7 spec: 8 containers: 9 - name: nginx 10 livenessProbe: 11 httpGet: 12 path: /foobar --record=true 13 14kubectl rollout undo deployment web 15deployment.apps/web rolled back Cleanup 1kubectl delete deploy web 2deployment.apps \u0026#34;web\u0026#34; deleted learnk8s documentation There is a great documentation that contains all the steps that help debugging a deployment: https://learnk8s.io/troubleshooting-deployments\n‚û°Ô∏è Next: RBAC\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/troubleshoot/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Troubleshooting"},{"body":"Resources allocation in Kubernetes Resources allocation in Kubernetes is made using requests and limits in the container's definition.\nrequests: What the container is guaranteed to get. These values are used when the scheduler takes a decision on where (what node) to place a given pod. limits: Are values that cannot be exceeded ‚ÑπÔ∏è You can use explain to have a look to the documentation of resources.\n1kubectl explain --recursive pod.spec.containers.resources.limits 2KIND: Pod 3VERSION: v1 4 5FIELD: limits \u0026lt;map[string]string\u0026gt; 6 7DESCRIPTION: 8 Limits describes the maximum amount of compute resources allowed. More 9... The wordpress we've created in the previous lab doesn't have resources definition. There are different ways to edit its current state (kubectl edit, apply, patch ...)\n1kubectl edit deploy wordpress replace resources: {} with this block\n1... 2 resources: 3 requests: 4 cpu: 100m 5 memory: 100Mi 6 limits: 7 cpu: 1000m 8 memory: 200Mi 9... The pods resources usage can be displayed using (this might take a few seconds)\n1kubectl top pods 2NAME CPU(cores) MEMORY(bytes) 3wordpress-694866c6b7-mqxdd 1m 171Mi 4wordpress-mysql-6c597b98bd-4mbbd 1m 531Mi Configure the autoscaling base on cpu usage. When a pod reaches 50% of its allocated cpu a new pod is created.\n1kubectl autoscale deployment wordpress --cpu-percent=50 --min=1 --max=5 2horizontalpodautoscaler.autoscaling/wordpress autoscaled It takes up to 15 seconds (default configuration) to get the first values\n1kubectl get hpa 2NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 3wordpress Deployment/wordpress \u0026lt;unknown\u0026gt;/50% 1 5 0 10s 4 5kubectl get hpa 6NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 7wordpress Deployment/wordpress 1%/50% 1 5 1 20s Now we'll run an HTTP bench using wrk. Open a new shell and run\n1kubectl run -ti --rm bench --image=jess/wrk -- /bin/sh -c \u0026#39;wrk -t12 -c100 -d180s http://wordpress\u0026#39; During the benchmark above (3 minutes duration) let's have a look to the hpa\n1watch kubectl get hpa 2Every 2.0s: kubectl get hpa 3hostname: Tue Jun 22 11:13:08 2021 4 5NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 6wordpress Deployment/wordpress 1%/50% 1 5 1 8m28s After a few seconds we'll see that the upscaling will be done automatically. Here the number of replicas will reach the maximum we defined (5 pods).\n1Every 2.0s: kubectl get hpa 2hostname: Tue Jun 22 11:14:13 2021 3 4NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 5wordpress Deployment/wordpress 998%/50% 1 5 5 9m33s That was a pretty simple configuration, basing the autoscaling on CPU usage for a webserver makes sense. You can also base the autoscaling on any other metrics that are reported by your application.\n‚û°Ô∏è Next: Troubleshooting\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/autoscaling/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Resources allocation and autoscaling"},{"body":"‚ÑπÔ∏è This section is, for a most part, based on the official Kubernetes doc.\nBy the end of this lab we'll create the following components. You may want to come back to this schema from time to time in order to get the whole picture.\nA database with a persistent volume Check that your cluster is up and running and that your context is still configured with the namespace foo\n1kubectl config get-contexts 2CURRENT NAME CLUSTER AUTHINFO NAMESPACE 3* k3d-workshop k3d-workshop admin@k3d-workshop foo Create a persistent volume claim There are several options when it comes to persistent workloads on Kubernetes. For this workshop we'll use our local disks thanks to the local path provisionner.\nCreate a persistentVolumeClaim, it will stay pending until a pod consumes it\n1kubectl apply -f content/resources/kubernetes_workshop/mysql/pvc.yaml 2persistentvolumeclaim/local-path-mysql created 3 4kubectl get pvc 5NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 6local-path-mysql Pending local-path 16s Create the MySQL secret In Kubernetes sensitive data are stored in Secrets. Here we'll create a secret that stores the MySQL root password\n1kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD 2secret/mysql-pass created Note that a secret is stored in an base64 encoded format and can be easily decoded. (There are best practices to enforce safe access to the secrets that we're not going to cover there)\n1kubectl get secrets mysql-pass -o yaml 2apiVersion: v1 3data: 4 password: WU9VUl9QQVNTV09SRA== 5kind: Secret 6metadata: 7 creationTimestamp: \u0026#34;2021-06-20T09:11:59Z\u0026#34; 8 name: mysql-pass 9 namespace: foo 10 resourceVersion: \u0026#34;2809\u0026#34; 11 uid: c96c58d6-8472-4d68-8554-5dcfb69d834c 12type: Opaque 13 14echo -n \u0026#34;WU9VUl9QQVNTV09SRA==\u0026#34; | base64 -d 15YOUR_PASSWORD Run the MySQL deployment We will now create a MySQL deployment. It will be composed of a single replica as we're accessing to a local volume and it is configured to make use of the secret we've created previously.\n1kubectl apply -f content/resources/kubernetes_workshop/mysql/deployment.yaml 2deployment.apps/wordpress-mysql created 3 4kubectl get po -w 5NAME READY STATUS RESTARTS AGE 6wordpress-mysql-6c597b98bd-vcm62 0/1 ContainerCreating 0 9s 7wordpress-mysql-6c597b98bd-vcm62 1/1 Running 0 13s 8^C Service discovery in Kubernetes In order to be able to call our MySQL deployment we may want to expose it using a service.\n1kubectl apply -f content/resources/kubernetes_workshop/mysql/svc.yaml 2service/wordpress-mysql created 3 4kubectl get svc 5NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 6wordpress-mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 6s Kubernetes's service discovery is based on an internal DNS system. For instance a service A service is accessible using the following nomenclature: \u0026lt;service_name\u0026gt;.\u0026lt;Namespace\u0026gt;.svc.\u0026lt;Cluster_domain_name\u0026gt;\nLet's try to access to the database server using a mysql client pod and create a database named foobar\n1kubectl run -ti --rm mysql-client --restart=Never --image=mysql:5.7 -- /bin/bash 2If you don\u0026#39;t see a command prompt, try pressing enter. 3root@mysql-client:/# apt -qq update \u0026amp;\u0026amp; apt install -yq netcat 4... 5Setting up netcat (1.10-41.1) ... 6 7 8root@mysql-client:/# nc -vz wordpress-mysql.foo.svc.cluster.local 3306 9DNS fwd/rev mismatch: wordpress-mysql.foo.svc.cluster.local != 10-42-1-8.wordpress-mysql.foo.svc.cluster.local 10wordpress-mysql.foo.svc.cluster.local [10.42.1.8] 3306 (?) open 11 12root@mysql-client:/# mysql -u root -h wordpress-mysql -p 13Enter password: 14... 15 16mysql\u0026gt; show databases; 17+--------------------+ 18| Database | 19+--------------------+ 20| information_schema | 21| mysql | 22| performance_schema | 23+--------------------+ 243 rows in set (0.01 sec) 25 26mysql\u0026gt; create database foobar; 27Query OK, 1 row affected (0.00 sec) 28 29mysql\u0026gt; exit 30Bye Note: You can either use the service name wordpress-mysql, or if your source pod is in another namespace use wordpress-mysql.foo\nCheck how the data is persisted with the local-path-provisioner We may want to check how the data is stored. Now that we have a MySQL instance running and consuming the pvc, a persistent volume has been provision\n1kubectl get pvc 2NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 3local-path-mysql Bound pvc-4bb3c033-2261-4d5c-ba61-41e364769599 500Mi RWO local-path 14m Having a closer look we notice that the volume is actually a directory within a worker node.\n1kubectl describe pv pvc-4bb3c033-2261-4d5c-ba61-41e364769599 2Name: pvc-4bb3c033-2261-4d5c-ba61-41e364769599 3Labels: \u0026lt;none\u0026gt; 4Annotations: pv.kubernetes.io/provisioned-by: rancher.io/local-path 5Finalizers: [kubernetes.io/pv-protection] 6StorageClass: local-path 7Status: Bound 8Claim: foo/local-path-mysql 9Reclaim Policy: Delete 10Access Modes: RWO 11VolumeMode: Filesystem 12Capacity: 500Mi 13Node Affinity: 14 Required Terms: 15 Term 0: kubernetes.io/hostname in [k3d-workshop-agent-0] 16Message: 17Source: 18 Type: HostPath (bare host directory volume) 19 Path: /var/lib/rancher/k3s/storage/pvc-4bb3c033-2261-4d5c-ba61-41e364769599_foo_local-path-mysql 20 HostPathType: DirectoryOrCreate 21Events: \u0026lt;none\u0026gt; 1docker exec k3d-workshop-agent-0 ls /var/lib/rancher/k3s/storage/pvc-4bb3c033-2261-4d5c-ba61-41e364769599_foo_local-path-mysql 2auto.cnf 3foobar 4ib_logfile0 5ib_logfile1 6ibdata1 7mysql 8performance_schema That means that even if you restart your laptop you should retrieve the data (here the database foobar we've created previously)\n1k3d cluster stop workshop 2INFO[0000] Stopping cluster \u0026#39;workshop\u0026#39; 3 4k3d cluster list 5NAME SERVERS AGENTS LOADBALANCER 6workshop 0/1 0/1 true 7 8k3d cluster start workshop 9INFO[0000] Starting cluster \u0026#39;workshop\u0026#39; 10INFO[0000] Starting servers... 11INFO[0000] Starting Node \u0026#39;k3d-workshop-server-0\u0026#39; 12INFO[0006] Starting agents... 13INFO[0006] Starting Node \u0026#39;k3d-workshop-agent-0\u0026#39; 14INFO[0013] Starting helpers... 15INFO[0013] Starting Node \u0026#39;k3d-workshop-serverlb\u0026#39; 16 17kubectl run -ti --rm mysql-client --restart=Never --image=mysql:5.7 -- mysql -u root -h wordpress-mysql --password=\u0026#34;YOUR_PASSWORD\u0026#34; 18If you don\u0026#39;t see a command prompt, try pressing enter. 19 20mysql\u0026gt; show databases; 21+--------------------+ 22| Database | 23+--------------------+ 24| information_schema | 25| foobar | 26| mysql | 27| performance_schema | 28+--------------------+ 294 rows in set (0.00 sec) The Wordpress deployment Now we will deploy the wordpress instance with a persistent volume.\nSo first of all create a pvc as follows\n1kubectl apply -f content/resources/kubernetes_workshop/wordpress/pvc.yaml 2persistentvolumeclaim/wp-pv-claim created Then create the deployment. Note that it is configured with our mysql database as backend.\n1kubectl apply -f content/resources/kubernetes_workshop/wordpress/deployment.yaml 2deployment.apps/wordpress created 3 4$ kubectl get deploy 5NAME READY UP-TO-DATE AVAILABLE AGE 6wordpress-mysql 1/1 1 1 11h 7wordpress 1/1 1 1 4s Most of the time, when we want to expose an HTTP service to the outside world (outside of the cluster), we would create an ingress\n1kubectl apply -f content/resources/kubernetes_workshop/wordpress/svc.yaml 2service/wordpress created 3 4kubectl apply -f content/resources/kubernetes_workshop/wordpress/ingress.yaml 5ingress.networking.k8s.io/wordpress created With k3d the ingress endpoint has been defined when we've created the cluster. With the parameter -p \u0026quot;8081:80@loadbalancer\u0026quot; Our wordpress should therefore be accessible through http://localhost:8081\nConfigure your pods A ConfigMap is a kubernetes resource that stores non-sensitive data. Its content can be consumed as config files, environment variables or command args.\nLet's consider that we need a configfile to be mounted in our wordpress deployment as well as an environment variable made available.\nCreate a dumb \u0026quot;hello world\u0026quot; config file\n1echo \u0026#34;Hello World!\u0026#34; \u0026gt; /tmp/helloworld.conf Then we'll create a configmap that contains a file and environment variable we want to make use of. Note This following command doesn't actually apply the resource on our Kubernetes cluster. It just generate a local yaml file using --dry-run and -o yaml.\n1kubectl create configmap helloworld --from-file=/tmp/helloworld.conf --from-literal=HELLO=WORLD -o yaml --dry-run=client \u0026gt; /tmp/cm.yaml Check the configmap\n1apiVersion: v1 2data: 3 HELLO: WORLD 4 helloworld.conf: | 5 Hello World! 6kind: ConfigMap 7metadata: 8 creationTimestamp: null 9 name: helloworld And apply it\n1$ kubectl apply -f /tmp/cm.yaml 2configmap/helloworld created Now we're gonna make use of it by changing the wordpress deployment. For this kind of change it is recommended to use an IDE with a Kubernetes plugin that will highlight errors.\nEdit the file located here: content/resources/kubernetes_workshop/wordpress/deployment.yaml\n1... 2 env: 3 - name: WORDPRESS_DB_HOST 4 value: wordpress-mysql 5 - name: WORDPRESS_DB_PASSWORD 6 valueFrom: 7 secretKeyRef: 8 name: mysql-pass 9 key: password 10 - name: HELLO 11 valueFrom: 12 configMapKeyRef: 13 name: helloworld 14 key: HELLO 15 volumeMounts: 16 - name: wordpress-persistent-storage 17 mountPath: /var/www/html 18 - name: helloworld-config 19 mountPath: /config 20 volumes: 21 - name: wordpress-persistent-storage 22 persistentVolumeClaim: 23 claimName: wp-pv-claim 24 - name: helloworld-config 25 configMap: 26 name: helloworld 27 items: 28 - key: helloworld.conf 29 path: helloworld.conf Applying this change will trigger a rolling-update\n1$ kubectl apply -f content/resources/kubernetes_workshop/wordpress/deployment.yaml 2deployment.apps/wordpress configured 3 4$ kubectl get po 5NAME READY STATUS RESTARTS AGE 6wordpress-mysql-6c597b98bd-4mbbd 1/1 Running 2 41h 7wordpress-594f88c9c4-n9qqr 1/1 Running 0 5s And the configuration will be available in the newly created pod\n1$ kubectl exec -ti wordpress-594f88c9c4-n9qqr -- env | grep HELLO 2HELLO=WORLD 3 4$ kubectl exec -ti wordpress-594f88c9c4-n9qqr -- cat /config/helloworld.conf 5Hello World! ‚ö†Ô∏è Do not delete anything, we'll make use of these resources in the next section.\n‚û°Ô∏è Next: Resources in Kubernetes\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/application_stack/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Complete application stack"},{"body":"Prepare your local Kubernetes environment Goal: Having a running local Kubernetes environment\nEnsure that you fulfilled the requirements\nUsing k3d to create a cluster In order to have an easily provisioned temporary playground we‚Äôll make use of k3d which is a lightweight local Kubernetes instance. (Note that they are alternatives to run a local Kubernetes cluster such as: kubeadm, microk8s, minikube)\nAfter installing the binary you should enable the completion (bash or zsh) as follows (do the same for both kubectl and k3d).\n1source \u0026lt;(k3d completion bash) Then create the sandbox cluster named \u0026quot;workshop\u0026quot; with an additional worker\n1k3d cluster create workshop -p \u0026#34;8081:80@loadbalancer\u0026#34; --agents 1 2INFO[0000] Prep: Network 3INFO[0000] Created network \u0026#39;k3d-workshop\u0026#39; (ce74508d3fe09d8622f1ae83effd412d754dfdb441aa9d550723805f9b528c6b) 4INFO[0000] Created volume \u0026#39;k3d-workshop-images\u0026#39; 5INFO[0001] Creating node \u0026#39;k3d-workshop-server-0\u0026#39; 6INFO[0001] Creating node \u0026#39;k3d-workshop-agent-0\u0026#39; 7INFO[0001] Creating LoadBalancer \u0026#39;k3d-workshop-serverlb\u0026#39; 8INFO[0001] Starting cluster \u0026#39;workshop\u0026#39; 9INFO[0001] Starting servers... 10INFO[0001] Starting Node \u0026#39;k3d-workshop-server-0\u0026#39; 11INFO[0006] Starting agents... 12INFO[0006] Starting Node \u0026#39;k3d-workshop-agent-0\u0026#39; 13INFO[0018] Starting helpers... 14INFO[0018] Starting Node \u0026#39;k3d-workshop-serverlb\u0026#39; 15INFO[0019] (Optional) Trying to get IP of the docker host and inject it into the cluster as \u0026#39;host.k3d.internal\u0026#39; for easy access 16INFO[0023] Successfully added host record to /etc/hosts in 3/3 nodes and to the CoreDNS ConfigMap 17INFO[0023] Cluster \u0026#39;workshop\u0026#39; created successfully! 18INFO[0023] --kubeconfig-update-default=false --\u0026gt; sets --kubeconfig-switch-context=false 19INFO[0023] You can now use it like this: 20kubectl config use-context k3d-workshop 21kubectl cluster-info As k3d is made to be used on top of docker you can see the status of the running containers. You should have 3 containers, one for the loadbalancing, one for the control-plane and an agent (worker).\n1docker ps 2CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 34b5847b265dd rancher/k3d-proxy:v4.4.6 \u0026#34;/bin/sh -c nginx-pr‚Ä¶\u0026#34; About a minute ago Up About a minute 80/tcp, 0.0.0.0:43903-\u0026gt;6443/tcp k3d-workshop-serverlb 4523a025087b3 rancher/k3s:v1.21.1-k3s1 \u0026#34;/bin/entrypoint.sh ‚Ä¶\u0026#34; About a minute ago Up About a minute k3d-workshop-agent-0 5791b8a69bc1f rancher/k3s:v1.21.1-k3s1 \u0026#34;/bin/entrypoint.sh ‚Ä¶\u0026#34; About a minute ago Up About a minute k3d-workshop-server-0 With kubectl you'll see 2 running pods: a control-plane and a worker\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-workshop-agent-0 Ready \u0026lt;none\u0026gt; 2m34s v1.21.1+k3s1 4k3d-workshop-server-0 Ready control-plane,master 2m44s v1.21.1+k3s1 You can also have a look to the default cluster's components that are all located in the namespace kube-system\n1kubectl get pods -n kube-system 2NAME READY STATUS RESTARTS AGE 3helm-install-traefik-crd-h5j7m 0/1 Completed 0 16h 4helm-install-traefik-8mzhk 0/1 Completed 0 16h 5svclb-traefik-gh4rk 2/2 Running 2 16h 6traefik-97b44b794-lcmh4 1/1 Running 1 16h 7coredns-7448499f4d-h7xvn 1/1 Running 2 16h 8local-path-provisioner-5ff76fc89d-qvpf7 1/1 Running 1 16h 9svclb-traefik-cbvmp 2/2 Running 2 16h 10metrics-server-86cbb8457f-5v9ls 1/1 Running 1 16h The CLI configuration The main interface to the Kubernetes API is kubectl. This CLI is configured with what we call a kubeconfig you can have a look at its content wether by having a look at its default location is ~/.kube/config or running the command\n1kubectl config view 2apiVersion: v1 3clusters: 4- cluster: 5 certificate-authority-data: DATA+OMITTED 6 server: https://0.0.... and you can check if the CLI is properly configured by running\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:43903 3CoreDNS is running at https://0.0.0.0:43903/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:43903/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy Kubectl plugins It is really easy to extend the capabilities of he kubectl CLI. Here is a basic \u0026quot;hello-world\u0026quot; example:\nWrite a dumb script, just ensure its name is prefixed with kubectl- and put it in your PATH\n1cat \u0026gt; kubectl-helloworld\u0026lt;\u0026lt;EOF 2#!/bin/bash 3echo \u0026#34;Hello world!\u0026#34; 4EOF 5 6chmod u+x kubectl-helloworld \u0026amp;\u0026amp; sudo mv kubectl-helloworld /usr/local/bin Then it can be used as an argument of kubectl\n1kubectl helloworld 2Hello world! Delete our test\n1sudo rm /usr/local/bin/kubectl-helloworld You can find more information on how to create a kubectl plugin here\nIn order to benefit from the plugins written by the community there's a tool named krew\nUpdate the local index\n1kubectl krew update 2Adding \u0026#34;default\u0026#34; plugin index from https://github.com/kubernetes-sigs/krew-index.git. 3Updated the local copy of plugin index. Browse the available plugins\n1kubectl krew search 2NAME DESCRIPTION INSTALLED 3access-matrix Show an RBAC access matrix for server resources no 4advise-psp Suggests PodSecurityPolicies for cluster. no 5allctx Run commands on contexts in your kubeconfig no 6apparmor-manager Manage AppArmor profiles for cluster. no 7... For the current workshop we'll make use of ctx ns\nctx: Switch between contexts in your kubeconfig (Really helpful when you have multiple clusters to manage) ns: Switch between Kubernetes namespaces (Avoid to specify the namespace for each kubectl commands when working on a given namespace) 1kubectl krew install ctx ns 2Updated the local copy of plugin index. 3Installing plugin: ctx 4... Then you'll be able to switch between contexts (clusters) and namespaces.\n1kubectl ns 2Context \u0026#34;k3d-workshop\u0026#34; modified. 3Active namespace is \u0026#34;kube-system\u0026#34;. ‚û°Ô∏è Next: Run an application on Kubernetes\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/local/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Local environment"},{"body":"Namespaces Namespaces allow to logically distribute your applications, generally based on teams, projects or applications stacks. Resources names are unique within a namespace. That means that you could have a service named webserver on 2 different namespaces.\nThey can be used to isolate applications using network policies, or to define quotas.\nFor this training we'll work on a namespace named foo\n1kubectl create ns foo 2namespace/foo created And we'll make use of the plugin ns installed in the previous section to set the default namespace as follows\n1kubectl ns 2Context \u0026#34;k3d-workshop\u0026#34; modified. 3Active namespace is \u0026#34;foo\u0026#34;. Check that your kubectl is properly configured, here you can see the cluter and the namespace:\n1kubectl config get-contexts 2CURRENT NAME CLUSTER AUTHINFO NAMESPACE 3* k3d-workshop k3d-workshop admin@k3d-workshop foo Create your first pod Creating resources in Kubernetes is often done by applying a yaml/json definition through the API.\nFirst of all we need to clone this repository and change the current path to its root\n1git clone https://github.com/Smana/workshop_kubernetes_2021.git 2 3cd workshop_kubernetes_2021.git Start by creating a pretty simple pod:\n1kubectl apply -f content/resources/kubernetes_workshop/pod.yaml --namespace foo 2pod/web created 3 4kubectl get po 5NAME READY STATUS RESTARTS AGE 6web 1/1 Running 0 98s We can get detailed information about the pod as follows\n1kubectl describe po web 2Name: web 3Namespace: foo 4Priority: 0 5Node: k3d-workshop-agent-0/172.20.0.3 6Start Time: Fri, 18 Jun 2021 17:05:46 +0200 7Labels: run=web 8Annotations: \u0026lt;none\u0026gt; 9Status: Running 10IP: 10.42.1.6 11... Or even get a specific attribute, here is an example to get the pod's IP\n1kubectl get po web --template={{.status.podIP}} 210.42.1.6 This is worth noting that a pod isn't controlled by a replicaset-controller. That means that when it is deleted, it is not restarted automatically.\n1kubectl delete po web 2pod \u0026#34;web\u0026#34; deleted 3 4kubectl get po 5No resources found in foo namespace. A pod with 2 containers Now create a new pod using the manifest content/resources/kubernetes_workshop/pod2containers.yaml. Look at its content, we will be using a shared temporary directory and we'll mount its content on both containers. That way we can share data between 2 containers of a given pod.\n1kubectl apply -f content/resources/kubernetes_workshop/pod2containers.yaml 2pod/web created 3 4kubectl get pod 5NAME READY STATUS RESTARTS AGE 6web 2/2 Running 0 36s We can check that the logs are accessible on the 2 containers\n1kubectl logs web -c logger --tail=6 -f 2Mon Jun 28 21:06:20 2021 3Mon Jun 28 21:06:21 2021 4Mon Jun 28 21:06:22 2021 5Mon Jun 28 21:06:23 2021 6Mon Jun 28 21:06:24 2021 7 8kubectl exec web -c web -- tail -n 5 /log/out.log 9Mon Jun 28 21:07:19 2021 10Mon Jun 28 21:07:20 2021 11Mon Jun 28 21:07:21 2021 12Mon Jun 28 21:07:22 2021 13Mon Jun 28 21:07:23 2021 Delete the pod\n1kubectl delete po web 2pod \u0026#34;web\u0026#34; deleted Create a simple webserver deployment A deployment is a resource that describes the desired state of an application. Kubernetes will ensure that its current status is aligned with the desired one.\nCreating a simple deployment can be done using kubectl\n1kubectl create deployment podinfo --image stefanprodan/podinfo 2deployment.apps/podinfo created After a few seconds the deployment will be up to date, meaning that the a pod is up and running.\n1kubectl get deploy 2NAME READY UP-TO-DATE AVAILABLE AGE 3podinfo 1/1 1 1 14s Replicas and scaling A deployment creates a replicaset under the hood in order to ensure that the number of replicas (pods) matches the desired one.\n1kubectl get replicasets 2NAME DESIRED CURRENT READY AGE 3podinfo-7fbb45ccfc 1 1 1 36s Creating a deployment without specifying the number of replicas will create a single replica. We can scale it on demand using\n1kubectl scale deploy podinfo --replicas 6 2deployment.apps/podinfo scaled 3 4kubectl rollout status deployment podinfo 5Waiting for deployment \u0026#34;podinfo\u0026#34; rollout to finish: 4 of 6 updated replicas are available... 6Waiting for deployment \u0026#34;podinfo\u0026#34; rollout to finish: 5 of 6 updated replicas are available... 7deployment \u0026#34;podinfo\u0026#34; successfully rolled out The default Kubernetes scheduler will try to spread evenly the pods according to the available resources on worker nodes.\n1kubectl get po -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3podinfo-7fbb45ccfc-dwxtx 1/1 Running 0 114s 10.42.1.8 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4podinfo-7fbb45ccfc-p2djv 1/1 Running 0 34s 10.42.1.11 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5podinfo-7fbb45ccfc-4fk9z 1/1 Running 0 34s 10.42.1.9 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6podinfo-7fbb45ccfc-gqwz6 1/1 Running 0 34s 10.42.1.10 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7podinfo-7fbb45ccfc-4qgvs 1/1 Running 0 34s 10.42.0.8 k3d-workshop-server-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8podinfo-7fbb45ccfc-r6dn5 1/1 Running 0 34s 10.42.0.9 k3d-workshop-server-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The deployment controller will ensure to start new pods if the number of replicas doesn't match its configuration.\n1kubectl delete po $(kubectl get po -l app=podinfo -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 2pod \u0026#34;podinfo-7fbb45ccfc-r6dn5\u0026#34; deleted 3 4kubectl describe rs podinfo-7fbb45ccfc 5Name: podinfo-7fbb45ccfc 6Namespace: foo 7Selector: app=podinfo,pod-template-hash=7fbb45ccfc 8Labels: app=podinfo 9 pod-template-hash=7fbb45ccfc 10Annotations: deployment.kubernetes.io/desired-replicas: 6 11 deployment.kubernetes.io/max-replicas: 8 12 deployment.kubernetes.io/revision: 5 13 deployment.kubernetes.io/revision-history: 1,3 14Controlled By: Deployment/podinfo 15Replicas: 6 current / 6 desired 16Pods Status: 6 Running / 0 Waiting / 0 Succeeded / 0 Failed 17... 18Events: 19 Type Reason Age From Message 20 ---- ------ ---- ---- ------- 21... 22 Normal SuccessfulCreate 16h (x3 over 16h) replicaset-controller (combined from similar events): Created pod: podinfo-7fbb45ccfc-pkt4r 23 Normal SuccessfulCreate 96s replicaset-controller Created pod: podinfo-7fbb45ccfc-bkm8n 24 25kubectl get deploy 26NAME READY UP-TO-DATE AVAILABLE AGE 27podinfo 6/6 6 6 18m Rolling update Using a deployment allows to manage the application lifecycle. Changing its configuration will trigger a rolling update.\nFirst of all we'll change the image tag of our deployment\n1kubectl set image deployment podinfo podinfo=stefanprodan/podinfo:5.2.1 2deployment.apps/podinfo image updated During a rolling update a new replicaset is created in order to update the application in place without any downtime. New pods (with the current deployment state) will be created in the new replicaset while they will be deleted progressively from the previous replicaset.\n1kubectl get rs -o wide 2NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR 3podinfo-7fbb45ccfc 0 0 0 21m podinfo stefanprodan/podinfo app=podinfo,pod-template-hash=7fbb45ccfc 4podinfo-564b4ddd7c 6 6 6 30s podinfo stefanprodan/podinfo:5.2.1 app=podinfo,pod-template-hash=564b4ddd7c Keeping the old replicaset makes very easy to rollback\n1kubectl rollout undo deployment podinfo 2deployment.apps/podinfo rolled back 3 4kubectl get rs -o wide 5NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR 6podinfo-7fbb45ccfc 6 6 6 22m podinfo stefanprodan/podinfo app=podinfo,pod-template-hash=7fbb45ccfc 7podinfo-564b4ddd7c 0 0 0 77s podinfo stefanprodan/podinfo:5.2.1 app=podinfo,pod-template-hash=564b4ddd7c Expose a deployment Now that we have a running web application we may want to access it. There are several ways to expose an app, here we'll use the easiest way: Create a service and run a port-forward.\nThe following command will create a service which will be in charge of forwarding calls through the tcp port 9898\n1kubectl expose deploy podinfo --port 9898 2service/podinfo exposed We can get more information on the service as follows\n1kubectl get svc -o yaml podinfo 2apiVersion: v1 3kind: Service 4metadata: 5 labels: 6 app: podinfo 7 name: podinfo 8 namespace: foo 9spec: 10 clusterIP: 10.43.47.17 11 clusterIPs: 12 - 10.43.47.17 13 ipFamilies: 14 - IPv4 15 ipFamilyPolicy: SingleStack 16 ports: 17 - port: 9898 18 selector: 19 app: podinfo A service uses the selector above to identify on which pod to forward the traffic and usually creates the endpoints accordingly.\n1kubectl get po -l app=podinfo 2NAME READY STATUS RESTARTS AGE 3podinfo-7fbb45ccfc-bkm8n 1/1 Running 1 146m 4podinfo-7fbb45ccfc-sbqht 1/1 Running 2 18h 5... 6 7kubectl get endpoints 8NAME ENDPOINTS AGE 9podinfo 10.42.0.16:9898,10.42.0.17:9898,10.42.1.18:9898 + 3 more... 92s The service we've created has an IP that's only accessible from within the cluster. Using the port-forward command we're able to forward the traffic from our local machine to the application (through the API server). Note that you can target either a deployment, a service or a single pod\n1 2kubectl port-forward svc/podinfo 9898 \u0026amp; 3Forwarding from 127.0.0.1:9898 -\u0026gt; 9898 4Forwarding from [::1]:9898 -\u0026gt; 9898 5 6curl http://localhost:9898 7Handling connection for 9898 8{ 9 \u0026#34;hostname\u0026#34;: \u0026#34;podinfo-7fbb45ccfc-sbqht\u0026#34;, 10 \u0026#34;version\u0026#34;: \u0026#34;6.0.0\u0026#34;, 11 \u0026#34;revision\u0026#34;: \u0026#34;\u0026#34;, 12 \u0026#34;color\u0026#34;: \u0026#34;#34577c\u0026#34;, 13 \u0026#34;logo\u0026#34;: \u0026#34;https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif\u0026#34;, 14 \u0026#34;message\u0026#34;: \u0026#34;greetings from podinfo v6.0.0\u0026#34;, 15 \u0026#34;goos\u0026#34;: \u0026#34;linux\u0026#34;, 16 \u0026#34;goarch\u0026#34;: \u0026#34;amd64\u0026#34;, 17 \u0026#34;runtime\u0026#34;: \u0026#34;go1.16.5\u0026#34;, 18 \u0026#34;num_goroutine\u0026#34;: \u0026#34;6\u0026#34;, 19 \u0026#34;num_cpu\u0026#34;: \u0026#34;16\u0026#34; 20} Cleanup In this section we created 2 resources: a deployment and a service.\n1fg 2kubectl port-forward svc/podinfo 9898 3^C 4 5kubectl delete svc,deploy podinfo 6service \u0026#34;podinfo\u0026#34; deleted 7deployment.apps \u0026#34;podinfo\u0026#34; deleted ‚û°Ô∏è Next: Deploy a Wordpress\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/run_app/","section":"post","tags":["Kubernetes"],"title":"Run an application on Kubernetes"},{"body":"This repository aims to quickly learn the basics of Kubernetes.\n‚ö†Ô∏è None of the examples given here are made for production.\nRequirements docker k3d \u0026gt;5.x.x kubectl krew (optional)fzf Agenda Prepare your local Kubernetes environment Run an application on Kubernetes Deploy a Wordpress Resources and autoscaling Troubleshooting RBAC Cleanup Pretty simple we‚Äôll drop the whole k3d cluster\n1k3d cluster delete workshop 2INFO[0000] Deleting cluster \u0026#39;workshop\u0026#39; 3... 4INFO[0008] Successfully deleted cluster workshop! ‚û°Ô∏è You may want to continue with the Helm workshop\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/intro/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop"},{"body":"","link":"https://blog.ogenki.io/archives/","section":"","tags":null,"title":""},{"body":"","link":"https://blog.ogenki.io/tags/index/","section":"tags","tags":null,"title":"index"}]