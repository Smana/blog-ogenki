[{"body":"","link":"https://blog.ogenki.io/","section":"","tags":null,"title":""},{"body":"When deploying an application on Kubernetes, the next step usually involves making it accessible to users. We commonly use Ingress controllers, such as Nginx, Haproxy, Traefik, or those from Cloud providers, to direct incoming traffic to the application, manage load balancing, TLS termination, and more.\nThen we have to choose from the plethora of available options ü§Ø. Cilium is, relatively recently, one of them and aims to handle all these networking aspects.\nCilium is an Open-Source networking and security solution based on eBPF whose adoption is growing rapidly. It's probably the network plugin that provides the most features. We won't cover all of them, but one such feature involves managing incoming traffic using the Gateway API (GAPI).\nüéØ Our target Understand exactly what the Gateway API is and how it represents an evolution from the Ingress API. Demonstrations of real-world scenarios deployed the GitOps way. Current limitations and upcoming developments. Tip All the steps carried out in this article come from this git repository.\nI encourage you to explore it, as it goes far beyond the context of this article:\nInstallation of an EKS cluster with Cilium configured with the kube-proxy replacement enbled and a dedicated Daemonset for Envoy. Proposal of a Flux structure with dependency management and a DRY code I find effective. Crossplane and IRSA composition which simplifies the management of IAM permissions for platform components. Automated domain names and certificates management with External-DNS and Let's Encrypt. The idea being to have everything set up in just a few minutes, with a single command line ü§©.\n‚ò∏ Introduction to Gateway API As mentioned previously, there are many Ingress Controllers options, and each has its own specificities and particular features, sometimes making their use complex. Furthermore, the traditionnal Ingress API in Kubernetes has very limited parameters. Some solutions have even created their own CRDs (Kubernetes Custom Resources) while others use annotations to overcome these limitations.\nHere comes the Gateway API! This is actually a standard that allows declaring advanced networking features without requiring specific extensions to the underlying controller. Moreover, since all controllers use the same API, it is possible to switch from one solution to another without changing the configuration (The Kubenetes manifests which describe how the incoming traffic should be routed).\nAmong the concepts that we will explore, GAPI brings a granular authorization model which defines explicit roles with distinct permissions. (More information on the GAPI security model here).\nThis is worth noting that this project is driven by the sig-network-kubernetes working group, and there's a slack channel where you can reach out to them if needed.\nLet's see how GAPI is used in practice with Cilium üöÄ!\n‚òëÔ∏è Prerequisites For the remainder of this article, we assume an EKS cluster has been deployed. If you're not using the method suggested in the demo repo as the basis for this article, there are a few points to check for GAPI to be usable.\n‚ÑπÔ∏è The installation method described here is based on Helm, all the values can be viewed here.\nInstall the CRDs available in the Gateway API repository. Note If Cilium is set up with GAPI support (see below) and the CRDs are missing, it won't start. In the demo repo, the GAPI CRDs are installed once during the cluster creation so that Cilium can start, and then they are managed by Flux.\nReplace kube-proxy with the network forwarding features provided by Cilium and eBPF.\n1kubeProxyReplacement: true Enable Gateway API support. 1gatewayAPI: 2 enabled: true Check the installation For that you need to install the command line tool cilium. I personnaly use asdf:\n1asdf plugin-add cilium-cli 2asdf install cilium-cli 0.15.7 3asdf global cilium 0.15.7 The following command allows to ensure that all the components are up and running:\n1cilium status --wait 2 /¬Ø¬Ø\\ 3/¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK 4\\__/¬Ø¬Ø\\__/ Operator: OK 5/¬Ø¬Ø\\__/¬Ø¬Ø\\ Envoy DaemonSet: OK 6\\__/¬Ø¬Ø\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled 8 9Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 10DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 11DaemonSet cilium-envoy Desired: 2, Ready: 2/2, Available: 2/2 12Containers: cilium Running: 2 13 cilium-operator Running: 2 14 cilium-envoy Running: 2 15Cluster Pods: 33/33 managed by Cilium 16Helm chart version: 1.14.2 17Image versions cilium quay.io/cilium/cilium:v1.14.2@sha256:6263f3a3d5d63b267b538298dbeb5ae87da3efacf09a2c620446c873ba807d35: 2 18 cilium-operator quay.io/cilium/operator-aws:v1.14.2@sha256:8d514a9eaa06b7a704d1ccead8c7e663334975e6584a815efe2b8c15244493f1: 2 19 cilium-envoy quay.io/cilium/cilium-envoy:v1.25.9-e198a2824d309024cb91fb6a984445e73033291d@sha256:52541e1726041b050c5d475b3c527ca4b8da487a0bbb0309f72247e8127af0ec: 2 Finally you can check that the Gateway API support is enabled by running\n1cilium config view | grep -w \u0026#34;enable-gateway-api\u0026#34; 2enable-gateway-api true 3enable-gateway-api-secrets-sync true You could also run end to end tests as follows\n1cilium connectivity test ‚ö†Ô∏è However this command (connectivity test) currently throws errors with Envoy as a DaemonSet enabled. (Github Issue).\nInfo as DaemonSet\nBy default, the Cilium agent also runs Envoy within the same pod and delegates to it level 7 network operations. Since version the version v1.14, it is possible to deploy Envoy separately, which brings several benefits:\nIf one modifies/restarts a component (whether it's Cilium or Envoy), it doesn't affect the other. Better allocate resources to each component to optimize performance. Limits the attack surface in case of compromise of one of the pods. Envoy logs and Cilium agent logs are not mixed. You can use the following command to check that this feature is indeed active:\n1cilium status 2 /¬Ø¬Ø\\ 3 /¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK 4 \\__/¬Ø¬Ø\\__/ Operator: OK 5 /¬Ø¬Ø\\__/¬Ø¬Ø\\ Envoy DaemonSet: OK 6 \\__/¬Ø¬Ø\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled More info.\nüö™ The Entry Point: GatewayClass and Gateway Once the conditions are met, we have access to several elements. We can make use of the custom resources defined by the Gateway API CRDs. Moreover, right after installing Cilium, a GatewayClass is immediately available.\n1kubectl get gatewayclasses.gateway.networking.k8s.io 2NAME CONTROLLER ACCEPTED AGE 3cilium io.cilium/gateway-controller True 7m59s On a Kubernetes cluster, you could configure multiple GatewayClass, thus having the ability to use different implementations. For instance, we can use Linkerd by referencing the GatewayClass in the Gateway configuration.\nThe Gateway is the resource that allows triggering the creation of load balancing components in the Cloud provider.\nHere's a simple example: apps/base/echo/gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo-gateway 5 namespace: echo 6spec: 7 gatewayClassName: cilium 8 listeners: 9 - protocol: HTTP 10 port: 80 11 name: echo-1-echo-server 12 allowedRoutes: 13 namespaces: 14 from: Same On AWS (EKS), when configuring a Gateway, Cilium creates a Service of type LoadBalancer. Then another controller (The AWS Load Balancer Controller) will handle the creation of the Cloud load balancer (NLB)\n1kubectl get svc -n echo cilium-gateway-echo 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3cilium-gateway-echo LoadBalancer 172.20.19.82 k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com 80:30395/TCP 2m58s This is worth noting that the load balancer address is also linked to the Gateway.\n1kubectl get gateway -n echo echo 2NAME CLASS ADDRESS PROGRAMMED AGE 3echo cilium k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com True 16m Note workaround\nAs of now, it is not possible to configure the annotations of services generated by the Gateways (Github Issue). A workaround has been proposed to modify the service generated by the Gateway as soon as it is created.\nKyverno is a tool that ensures configuration compliance with best practices and security requirements. We are using it here solely for its ability to easily describe a mutation rule.\nsecurity/mycluster-0/echo-gw-clusterpolicy.yaml\n1spec: 2 rules: 3 - name: mutate-svc-annotations 4 match: 5 any: 6 - resources: 7 kinds: 8 - Service 9 namespaces: 10 - echo 11 name: cilium-gateway-echo 12 mutate: 13 patchStrategicMerge: 14 metadata: 15 annotations: 16 external-dns.alpha.kubernetes.io/hostname: echo.${domain_name} 17 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; 18 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 19 spec: 20 loadBalancerClass: service.k8s.aws/nlb The service cilium-gateway-echo will therefore have the AWS controller's annotations added, as well as an annotation allowing for automatic DNS entry configuration.\n‚Ü™Ô∏è Routing rules: HTTPRoute A basic rule To summarize the above diagram in a few words: An HTTPRoute allows configuring the routing to the service by referencing the Gateway and defining the desired routing parameters.\napps/base/echo/httproute.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 rules: 11 - matches: 12 - path: 13 type: PathPrefix 14 value: / 15 backendRefs: 16 - name: echo-1-echo-server 17 port: 80 The example used above is very simple: all requests are forwarded to the echo-1-echo-server service.\nparentRefs indicates which Gateway to use and then the routing rules are defined under the rules section.\nThe routing rules could also be based on the path.\n1... 2spec: 3 hostnames: 4 - foo.bar.com 5 rules: 6 - matches: 7 - path: 8 type: PathPrefix 9 value: /login Or based on an HTTP Header\n1... 2spec: 3 rules: 4 - matches: 5 headers: 6 - name: \u0026#34;version\u0026#34; 7 value: \u0026#34;2\u0026#34; 8... Let's check if the service is reachable.:\n1curl -s http://echo.cloud.ogenki.io | jq -rc \u0026#39;.environment.HOSTNAME\u0026#39; 2echo-1-echo-server-fd88497d-w6sgn As you can see, the service is exposed in HTTP without a certificate. Let's try to fix that üòâ\nConfigure a TLS certificate There are several methods to configure TLS with GAPI. Here, we will use the most common case: HTTPS protocol and TLS termination at the Gateway.\nLet's assume we want to configure the domain name echo.cloud.ogenki.io used earlier. The configuration is mainly done by configuring the Gateway.\napps/base/echo/tls-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo 5 namespace: echo 6 annotations: 7 cert-manager.io/cluster-issuer: letsencrypt-prod 8spec: 9 gatewayClassName: cilium 10 listeners: 11 - name: http 12 hostname: \u0026#34;echo.${domain_name}\u0026#34; 13 port: 443 14 protocol: HTTPS 15 allowedRoutes: 16 namespaces: 17 from: Same 18 tls: 19 mode: Terminate 20 certificateRefs: 21 - name: echo-tls The essential point here is the reference to a secret containing the certificate named echo-tls. This certificate can be created manually, but for this article, I chose to automate this with Let's Encrypt and cert-manager.\nInfo cert-manager\nWith cert-manager, it's pretty straightforward to automate the creation and update of certificates exposed by the Gateway. For this, you need to allow the controller to access route53 in order to solve a DNS01 challenge (A mechanism that ensures that clients can only request certificates for domains they own).\nA ClusterIssuer resource describes the required configuration to generate certificates with cert-manager.\nNext, we just need to add an annotation cert-manager.io/cluster-issuer and set the Kubernetes secret where the certificate will be stored.\nMore information\n‚ÑπÔ∏è In the demo repo, permissions are assigned using Crossplane, which takes care of configuring these IAM perms in AWS.\nFor routing to work correctly, you also need to attach the HTTPRoute to the right Gateway and specify the domain name.\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 hostnames: 11 - \u0026#34;echo.${domain_name}\u0026#34; 12... After a few seconds the certificate will be created.\n1kubectl get cert -n echo 2NAME READY SECRET AGE 3echo-tls True echo-tls 43m Finally, we can check that the certificate indeed comes from Let's Encrypt as follows:\n1curl https://echo.cloud.ogenki.io -v 2\u0026gt;\u0026amp;1 | grep -A 6 \u0026#39;Server certificate\u0026#39; 2* Server certificate: 3* subject: CN=echo.cloud.ogenki.io 4* start date: Sep 15 14:43:00 2023 GMT 5* expire date: Dec 14 14:42:59 2023 GMT 6* subjectAltName: host \u0026#34;echo.cloud.ogenki.io\u0026#34; matched cert\u0026#39;s \u0026#34;echo.cloud.ogenki.io\u0026#34; 7* issuer: C=US; O=Let\u0026#39;s Encrypt; CN=R3 8* SSL certificate verify ok. Info GAPI also allows you to configure end-to-end TLS, all the way to the container. This is done by configuring the Gateway in Passthrough mode and using a TLSRoute resource. The certificate must also be carried by the pod that performs the TLS termination.\nSharing a Gateway accross multiple namespaces With GAPI, it is possible to route traffic across Namespaces. This is made possible thanks to distinct resources for each function: A Gateway that allows configuring the infrastructure, and the *Routes. These routes can be attached to a Gateway located in another namespace. It is thus possible for different teams/projects to share the same infrastructure components.\nHowever, this requires to specify which route is allowed to reference the Gateway. Here we assume that we have a Gateway dedicated to internal tools called platform. By using the allowedRoutes parameter, we explicitly specify which namespaces are allowed to be attached to this Gateway.\ninfrastructure/base/gapi/platform-gateway.yaml\n1... 2 allowedRoutes: 3 namespaces: 4 from: Selector 5 selector: 6 matchExpressions: 7 - key: kubernetes.io/metadata.name 8 operator: In 9 values: 10 - observability 11 - flux-system 12 tls: 13 mode: Terminate 14 certificateRefs: 15 - name: platform-tls The HTTPRoutes configured in the namespaces observability and flux-system are attached to this unique Gateway.\n1... 2spec: 3 parentRefs: 4 - name: platform 5 namespace: infrastructure And therefore, use the same load balancer from the Cloud provider.\n1NLB_DOMAIN=$(kubectl get svc -n infrastructure cilium-gateway-platform -o jsonpath={.status.loadBalancer.ingress[0].hostname}) 2 3dig +short ${NLB_DOMAIN} 413.36.89.108 5 6dig +short grafana-mycluster-0.cloud.ogenki.io 713.36.89.108 8 9dig +short gitops-mycluster-0.cloud.ogenki.io 1013.36.89.108 Note üîí These internal tools shouldn't be exposed on the Internet, but you know: this is just a demo üôè. For instance, we could use an internal Gateway (private IP) by playing with the annotations and make use of a private connection system (VPN, tunnels...).\nTraffic splitting One feature that is commonly brought by Service Meshes is the ability to test an application on a portion of the traffic when a new version is available (A/B testing or Canary deployment). GAPI makes this quite simple by using weights.\nHere's an example that forwards 5% of the traffic to the service echo-2-echo-server:\napps/base/echo/httproute-split.yaml\n1... 2 hostnames: 3 - \u0026#34;split-echo.${domain_name}\u0026#34; 4 rules: 5 - matches: 6 - path: 7 type: PathPrefix 8 value: / 9 backendRefs: 10 - name: echo-1-echo-server 11 port: 80 12 weight: 95 13 - name: echo-2-echo-server 14 port: 80 15 weight: 5 Let's check that the distribution happens as expected:\nscripts/check-split.sh\n1./scripts/check-split.sh https://split-echo.cloud.ogenki.io 2Number of requests for echo-1: 95 3Number of requests for echo-2: 5 Headers modifications It is also possible to change HTTP Headers: to add, modify, or delete them. These modifications can be applied to either request or response headers through the use of filters in the HTTPRoute manifest.\nFor instance, we will add a Header to the request.\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7... 8 rules: 9 - matches: 10 - path: 11 type: PathPrefix 12 value: /req-header-add 13 filters: 14 - type: RequestHeaderModifier 15 requestHeaderModifier: 16 add: 17 - name: foo 18 value: bar 19 backendRefs: 20 - name: echo-1-echo-server 21 port: 80 22... This command allows to that the header is indeed added:\n1curl -s https://echo.cloud.ogenki.io/req-header-add | jq \u0026#39;.request.headers\u0026#39; 2{ 3 \u0026#34;host\u0026#34;: \u0026#34;echo.cloud.ogenki.io\u0026#34;, 4 \u0026#34;user-agent\u0026#34;: \u0026#34;curl/8.2.1\u0026#34;, 5 \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;x-forwarded-for\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 7 \u0026#34;x-forwarded-proto\u0026#34;: \u0026#34;https\u0026#34;, 8 \u0026#34;x-envoy-external-address\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 9 \u0026#34;x-request-id\u0026#34;: \u0026#34;320ba4d2-3bd6-4c2f-8a97-74296a9f3f26\u0026#34;, 10 \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34; 11} üîí Assign the proper permissions GAPI offers a clear permission-sharing model between the traffic routing infrastructure (managed by cluster administrators) and the applications (managed by developers).\nThe availability of multiple custom resources allows to use Kubernete's RBAC configuration to assign permissions in a declarative way. I've added a few examples which have no effect in my demo cluster but might give you an idea.\nThe configuration below grants members of the developers group the ability to manage HTTPRoutes within the echo namespace, while only providing them read access to the Gateways.\n1--- 2apiVersion: rbac.authorization.k8s.io/v1 3kind: Role 4metadata: 5 namespace: echo 6 name: gapi-developer 7rules: 8 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 9 resources: [\u0026#34;httproutes\u0026#34;] 10 verbs: [\u0026#34;*\u0026#34;] 11 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 12 resources: [\u0026#34;gateways\u0026#34;] 13 verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] 14--- 15apiVersion: rbac.authorization.k8s.io/v1 16kind: RoleBinding 17metadata: 18 name: gapi-developer 19 namespace: echo 20subjects: 21 - kind: Group 22 name: \u0026#34;developers\u0026#34; 23 apiGroup: rbac.authorization.k8s.io 24roleRef: 25 kind: Role 26 name: gapi-developer 27 apiGroup: rbac.authorization.k8s.io ü§î A somewhat unclear scope at first glance One could be confused with what's commonly referred to as an API Gateway. A section of the FAQ has been created to clarify its difference with the Gateway API. Although GAPI offers features typically found in an API Gateway, it primarily serves as a specific implementation for Kubernetes. However, the choice of this name can indeed cause confusion.\nMoreover please note that this article focuses solely on inbound traffic, termed north-south, traditionally managed by Ingress Controllers. This traffic is actually GAPI's initial scope. A recent initiative named GAMMA aims to also handle east-west routing, which will standardize certain features commonly provided by Service Meshes solutions in the future. (See this article for more details).\nüí≠ Final thoughts To be honest, I've known about the Gateway API for some time. Although I've read a few articles, I hadn't truly dived deep. I'd think, \u0026quot;Why bother? My Ingress Controller works, and there's a learning curve with this.\u0026quot;\nGAPI is on the rise and nearing its GA release. Several projects have embraced it, and this API for managing traffic within Kubernetes will quickly become the standard.\nI must say, configuring GAPI felt intuitive and explicit ‚ù§Ô∏è. Its security model strikes a balance, empowering developers without compromising security. And the seamless infrastructure management? You can switch between implementations without touching the *Routes.\nWould I swap my Ingress Controller for Cilium today? Not yet, but it's on the horizon.\nIt's worth highlighting Cilium's broad range of capabilities: With Kubernetes surrounded by a plethora of tools, Cilium stands out, promising features like metrics, tracing, service-mesh, security, and, yes, Ingress Controller with GAPI.\nHowever, there are a few challenges to note:\nTCP and UDP support GRPC support The need to use a mutation rule to configure cloud components (Github Issue). Many of the features discussed in this blog are still in the experimental stage. For instance, the extended functions, which have been supported since the most recent release at the time of my writing (v1.14.2). I attempted to set up a straightforward HTTP\u0026gt;HTTPS redirect but ran into this issue. Consequently, I expect some modifications to the API in the near future. While I've only scratched the surface of what Cilium's GAPI can offer (honestly, this post is already quite long üòú), I am hopeful that we can consider its use in production soon. If you haven't thought about this transition yet, now's the time üòâ! But considering the points mentioned earlier, I would advise waiting a bit longer.\nüîñ References https://gateway-api.sigs.k8s.io/ https://docs.cilium.io/en/latest/network/servicemesh/gateway-api/gateway-api/#gs-gateway-api https://isovalent.com/blog/post/cilium-gateway-api/ https://isovalent.com/blog/post/tutorial-getting-started-with-the-cilium-gateway-api/ Isovalent's labs are great to start playing with Gateway API and you'll get new badges to add to your collection üòÑ ","link":"https://blog.ogenki.io/post/cilium-gateway-api/","section":"post","tags":["kubernetes","infrastructure","network"],"title":"`Gateway API`: Can I replace my Ingress Controller with `Cilium`?"},{"body":"","link":"https://blog.ogenki.io/tags/infrastructure/","section":"tags","tags":null,"title":"infrastructure"},{"body":"","link":"https://blog.ogenki.io/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"","link":"https://blog.ogenki.io/tags/network/","section":"tags","tags":null,"title":"network"},{"body":"","link":"https://blog.ogenki.io/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://blog.ogenki.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"Terraform is probably the most used \u0026quot;Infrastructure As Code\u0026quot; tool for building, modifying, and versioning Cloud infrastructure changes. It is an Open Source project developed by Hashicorp that uses the HCL language to declare the desired state of Cloud resources. The state of the created resources is stored in a file called terraform state.\nTerraform can be considered a \u0026quot;semi-declarative\u0026quot; tool as there is no built-in automatic reconciliation feature. There are several solutions to address this issue, but generally speaking, a modification will be applied using terraform apply. The code is actually written using the HCL configuration files (declarative), but the execution is done imperatively. As a result, there can be a drift between the declared and actual state (for example, a colleague who would have changed something directly into the console üòâ).\n‚ùì‚ùì So, how can I ensure that what is committed using Git is really applied. How to be notified if there is a change compared to the desired state and how to automatically apply what is in my code (GitOps)?\nThis is the promise of tf-controller, an Open Source Kubernetes operator from Weaveworks, tightly related to Flux (a GitOps engine from the same company). Flux is one of the solutions I really appreciate, that's why I invite you to have a look on my previous article\nInfo All the steps described in this article come from this Git repo\nüéØ Our target By following the steps in this article, we aim to achieve the following things:\nDeploy a Control plane EKS cluster. Long story short, it will host the Terraform controller that will be in charge of managing all the desired infrastructure components. Use Flux as the GitOps engine for all Kubernetes resources. Regarding the Terraform controller, we will see:\nHow to define dependencies between modules Creation of several AWS resources: Route53 Zone, ACM Certificate, network, EKS cluster. The different reconciliation options (automatic, requiring confirmation) How to backup and restore a Terraform state. üõ†Ô∏è Install the tf-controller ‚ò∏ The control plane In order to be able to use the Kubernetes controller tf-controller, we first need a Kubernetes cluster üòÜ. So we are going to create a control plane cluster using the terraform command line and EKS best practices.\nWarning It is crucial that this cluster is resilient, secure, and supervised as it will be responsible for managing all the AWS resources created subsequently.\nWithout going into detail, the control plane cluster was created using this code. That said, it is important to note that all application deployment operations are done using Flux.\nInfo By following the instructions in the README, an EKS cluster will be created but not only! Indeed, it is required to give permissions to the Terraform controller so it will able to apply infrastructure changes. Furthermore, Flux must be installed and configured to apply the configuration defined here.\nWe end up with several components installed and configured:\nThe almost unavoidable addons: aws-loadbalancer-controller and external-dns IRSA roles for these same components are installed using tf-controller Prometheus / Grafana monitoring stack. external-secrets to be able to retrieve sensitive data from AWS secretsmanager. To demonstrate all this after a few minutes the web interface for Flux is accessible via the URL gitops-\u0026lt;cluster_name\u0026gt;.\u0026lt;domain_name\u0026gt;\nStill you should check that Flux is working properly\n1aws eks update-kubeconfig --name controlplane-0 --alias controlplane-0 2Updated context controlplane-0 in /home/smana/.kube/config 1flux check 2... 3‚úî all checks passed 4 5flux get kustomizations 6NAME REVISION SUSPENDED READY MESSAGE 7flux-config main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 8flux-system main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 9infrastructure main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 10security main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 11tf-controller main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 12... üì¶ The Helm chart and Flux Now that the control plane cluster is available we can add the Terraform controller and this is just the matter of using the Helm chart as follows.\nWe must declare the its Source first:\nsource.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: tf-controller 5spec: 6 interval: 30m 7 url: https://weaveworks.github.io/tf-controller Then we need to define the HelmRelease:\nrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: tf-controller 5spec: 6 releaseName: tf-controller 7 chart: 8 spec: 9 chart: tf-controller 10 sourceRef: 11 kind: HelmRepository 12 name: tf-controller 13 namespace: flux-system 14 version: \u0026#34;0.12.0\u0026#34; 15 interval: 10m0s 16 install: 17 remediation: 18 retries: 3 19 values: 20 resources: 21 limits: 22 memory: 1Gi 23 requests: 24 cpu: 200m 25 memory: 500Mi 26 runner: 27 serviceAccount: 28 annotations: 29 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/tfcontroller_${cluster_name}\u0026#34; When this change is actually written into Git, the HelmRelease will be deployed and the tf-controller started:\n1kubectl get hr -n flux-system 2NAME AGE READY STATUS 3tf-controller 67m True Release reconciliation succeeded 4 5kubectl get po -n flux-system -l app.kubernetes.io/instance=tf-controller 6NAME READY STATUS RESTARTS AGE 7tf-controller-7ffdc69b54-c2brg 1/1 Running 0 2m6s In this demo, there are already a several AWS resources declared. Therefore, after a few minutes, the cluster takes care of creating these: Info Although the majority of operations are performed declaratively or via the CLIs kubectl and flux, another tool allows to manage Terraform resources: tfctl\nüöÄ Apply a change One of the Terraform's best practices is to use modules. A module is a set of logically linked Terraform resources bundled into a single reusable unit. They allow to abstract complexity, take inputs, perform specific actions, and produce outputs.\nYou can create your own modules and make them available as Sources or use the many modules shared and maintained by communities. You just need to specify a few variables in order to fit to your context.\nWith tf-controller, the first step is therefore to define the Source of the module. Here we are going to configure the AWS base networking components (vpc, subnets...) using the terraform-aws-vpc module.\n1apiVersion: source.toolkit.fluxcd.io/v1 2kind: GitRepository 3metadata: 4 name: terraform-aws-vpc 5 namespace: flux-system 6spec: 7 interval: 30s 8 ref: 9 tag: v5.0.0 10 url: https://github.com/terraform-aws-modules/terraform-aws-vpc Then we can make use of this Source within a Terraform resource:\nvpc/dev.yaml\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6 interval: 8m 7 path: . 8 destroyResourcesOnDeletion: true # You wouldn\u0026#39;t do that on a prod env ;) 9 storeReadablePlan: human 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-vpc 13 namespace: flux-system 14 vars: 15 - name: name 16 value: vpc-dev 17 - name: cidr 18 value: \u0026#34;10.42.0.0/16\u0026#34; 19 - name: azs 20 value: 21 - \u0026#34;eu-west-3a\u0026#34; 22 - \u0026#34;eu-west-3b\u0026#34; 23 - \u0026#34;eu-west-3c\u0026#34; 24 - name: private_subnets 25 value: 26 - \u0026#34;10.42.0.0/19\u0026#34; 27 - \u0026#34;10.42.32.0/19\u0026#34; 28 - \u0026#34;10.42.64.0/19\u0026#34; 29 - name: public_subnets 30 value: 31 - \u0026#34;10.42.96.0/24\u0026#34; 32 - \u0026#34;10.42.97.0/24\u0026#34; 33 - \u0026#34;10.42.98.0/24\u0026#34; 34 - name: enable_nat_gateway 35 value: true 36 - name: single_nat_gateway 37 value: true 38 - name: private_subnet_tags 39 value: 40 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 41 \u0026#34;karpenter.sh/discovery\u0026#34;: dev 42 - name: public_subnet_tags 43 value: 44 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 45 writeOutputsToSecret: 46 name: vpc-dev In summary: the terraform code from the terraform-aws-vpc source is applied using the variables defined within vars.\nThere are then several parameters that influence the tf-controller behavior. The main parameters that control how modifications are applied are .spec.approvePlan and .spec.autoApprove\nüö® Drift detection Setting spec.approvePlan to disable only notifies that the current state of resources has drifted from the Terraform code. This allows you to choose when and how to apply the changes.\nNote This is worth noting that there is a missing section on notifications: Drift, pending plans, reconciliation problems. I'm trying to identify possible methods (preferably with Prometheus) and update this article as soon as possible.\nüîß Manual execution The given example above (vpc-dev) does not contain the .spec.approvePlan parameter and therefore inherits the default value which is false. In other words, the actual execution of changes (apply) is not done automatically.\nA plan is executed and will be waiting for validation:\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system vpc-dev Unknown Plan generated: set approvePlan: \u0026#34;plan-v5.0.0-26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. true 2 minutes I also advise to configure the storeReadablePlan parameter to human. This allows you to easily visualize the pending modifications using tfctl:\n1tfctl show plan vpc-dev 2 3Terraform used the selected providers to generate the following execution 4plan. Resource actions are indicated with the following symbols: 5 + create 6 7Terraform will perform the following actions: 8 9 # aws_default_network_acl.this[0] will be created 10 + resource \u0026#34;aws_default_network_acl\u0026#34; \u0026#34;this\u0026#34; { 11 + arn = (known after apply) 12 + default_network_acl_id = (known after apply) 13 + id = (known after apply) 14 + owner_id = (known after apply) 15 + tags = { 16 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 17 } 18 + tags_all = { 19 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 20 } 21 + vpc_id = (known after apply) 22 23 + egress { 24 + action = \u0026#34;allow\u0026#34; 25 + from_port = 0 26 + ipv6_cidr_block = \u0026#34;::/0\u0026#34; 27 + protocol = \u0026#34;-1\u0026#34; 28 + rule_no = 101 29 + to_port = 0 30 } 31 + egress { 32... 33Plan generated: set approvePlan: \u0026#34;plan-v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. 34To set the field, you can also run: 35 36 tfctl approve vpc-dev -f filename.yaml After reviewing the above modifications, you just need to add the identifier of the plan to validate and push the change to git as follows:\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6... 7 approvePlan: plan-v5.0.0-26c38a66f1 8... After a few seconds, a runner will be launched and will apply the changes:\n1kubectl logs -f -n flux-system vpc-dev-tf-runner 22023/07/01 15:33:36 Starting the runner... version sha 3... 4aws_vpc.this[0]: Creating... 5aws_vpc.this[0]: Still creating... [10s elapsed] 6... 7aws_route_table_association.private[1]: Creation complete after 0s [id=rtbassoc-01b7347a7e9960a13] 8aws_nat_gateway.this[0]: Still creating... [10s elapsed] As soon as the apply is finished the status of the Terraform resource becomes \u0026quot;READY\u0026quot;\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m ü§ñ Automatic reconciliation We can also enable automatic reconciliation. To do this, set the .spec.autoApprove parameter to true.\nAll IRSA resources are configured in this way:\nexternal-secrets.yaml\n1piVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: irsa-external-secrets 5spec: 6 approvePlan: auto 7 destroyResourcesOnDeletion: true 8 interval: 8m 9 path: ./modules/iam-role-for-service-accounts-eks 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-iam 13 namespace: flux-system 14 vars: 15 - name: role_name 16 value: ${cluster_name}-external-secrets 17 - name: attach_external_secrets_policy 18 value: true 19 - name: oidc_providers 20 value: 21 main: 22 provider_arn: ${oidc_provider_arn} 23 namespace_service_accounts: [\u0026#34;security:external-secrets\u0026#34;] So if I make any change on the AWS console for example, it will be quickly overwritten by the one managed by tf-controller.\nInfo The deletion policy of components created by a Terraform resource is controlled by the setting destroyResourcesOnDeletion. By default anything created is not destroyed by the controller. If you want to destroy the resources when the Terraform object is deleted you must set this parameter to true.\nHere we want to be able to delete IRSA roles because they're tightly linked to a given EKS cluster\nüîÑ Inputs/Outputs and modules dependencies When using Terraform, we often need to share data from one module to another. This is done using the outputs that are defined within modules. So we need a way to store them somewhere and import them into another module.\nLet's take again the given example above (vpc-dev). We can see at the bottom of the YAML file, the following block:\n1... 2 writeOutputsToSecret: 3 name: vpc-dev When this resource is applied, we will get a message confirming that the outputs are available (\u0026quot;Outputs written\u0026quot;):\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m Indeed this module exports many information (126).\n1kubectl get secrets -n flux-system vpc-dev 2NAME TYPE DATA AGE 3vpc-dev Opaque 126 15s 4 5kubectl get secret -n flux-system vpc-dev --template=\u0026#39;{{.data.vpc_id}}\u0026#39; | base64 -d 6vpc-0c06a6d153b8cc4db Some of these are then used to create a dev EKS cluster. Note that you don't have to read them all, you can cherry pick a few chosen outputs from the secret:\nvpc/dev.yaml\n1... 2 varsFrom: 3 - kind: Secret 4 name: vpc-dev 5 varsKeys: 6 - vpc_id 7 - private_subnets 8... üíæ Backup and restore a tfstate For my demos, I don't want to recreate the zone and the certificate each time the control plane is destroyed (The DNS propagation and certificate validation take time). Here is an example of the steps to take so that I can restore the state of these resources when I use this demo.\nNote This is a manual procedure to demonstrate the behavior of tf-controller with respect to state files. By default, these tfstates are stored in secrets, but we would prefer to configure a GCS or S3 backend.\nThe initial creation of the demo environment allowed me to save the state files (tfstate) as follows.\n1WORKSPACE=\u0026#34;default\u0026#34; 2STACK=\u0026#34;route53-cloud-hostedzone\u0026#34; 3BACKUPDIR=\u0026#34;${HOME}/tf-controller-backup\u0026#34; 4 5mkdir -p ${BACKUPDIR} 6 7kubectl get secrets -n flux-system tfstate-${WORKSPACE}-${STACK} -o jsonpath=\u0026#39;{.data.tfstate}\u0026#39; | \\ 8base64 -d \u0026gt; ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate.gz When the cluster is created again, tf-controller tries to create the zone because the state file is empty.\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system route53-cloud-hostedzone Unknown Plan generated: set approvePlan: \u0026#34;plan-main@sha1:345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. true 16 minutes 5 6tfctl show plan route53-cloud-hostedzone 7 8Terraform used the selected providers to generate the following execution 9plan. Resource actions are indicated with the following symbols: 10 + create 11 12Terraform will perform the following actions: 13 14 # aws_route53_zone.this will be created 15 + resource \u0026#34;aws_route53_zone\u0026#34; \u0026#34;this\u0026#34; { 16 + arn = (known after apply) 17 + comment = \u0026#34;Experimentations for blog.ogenki.io\u0026#34; 18 + force_destroy = false 19 + id = (known after apply) 20 + name = \u0026#34;cloud.ogenki.io\u0026#34; 21 + name_servers = (known after apply) 22 + primary_name_server = (known after apply) 23 + tags = { 24 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 25 } 26 + tags_all = { 27 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 28 } 29 + zone_id = (known after apply) 30 } 31 32Plan: 1 to add, 0 to change, 0 to destroy. 33 34Changes to Outputs: 35 + domain_name = \u0026#34;cloud.ogenki.io\u0026#34; 36 + nameservers = (known after apply) 37 + zone_arn = (known after apply) 38 + zone_id = (known after apply) 39 40Plan generated: set approvePlan: \u0026#34;plan-main@345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. 41To set the field, you can also run: 42 43 tfctl approve route53-cloud-hostedzone -f filename.yaml So we need to restore the terraform state as it was when the cloud resources where initially created.\n1cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 2apiVersion: v1 3kind: Secret 4metadata: 5 name: tfstate-${WORKSPACE}-${STACK} 6 namespace: flux-system 7 annotations: 8 encoding: gzip 9type: Opaque 10data: 11 tfstate: $(cat ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate.gz | base64 -w 0) 12EOF You will also need to trigger a plan manually\n1tfctl replan route53-cloud-hostedzone 2Ôò´ Replan requested for flux-system/route53-cloud-hostedzone 3Error: timed out waiting for the condition We can then check that the state file has been updated\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3flux-system route53-cloud-hostedzone True Outputs written: main@sha1:d0934f979d832feb870a8741ec01a927e9ee6644 false 19 minutes üîç Focus on Key Features of Flux Well, I lied a bit about the agenda üòù. Indeed I want to highlight two features that I hadn't explored until now and that are very useful!\nVariable Substitution When Flux is initialized, some cluster-specific Kustomization files are applied. It is possible to specify variable substitutions within these files, so that they can be used in all resources deployed by this Kustomization. This helps to minimize code duplication.\nI recently discovered the efficiency of this feature. Here is how I use it:\nThe Terraform code that creates an EKS cluster also generates a ConfigMap that contains cluster-specific variables such as the cluster name, as well as all the parameters that vary between clusters.\nflux.tf\n1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;flux_clusters_vars\u0026#34; { 2 metadata { 3 name = \u0026#34;eks-${var.cluster_name}-vars\u0026#34; 4 namespace = \u0026#34;flux-system\u0026#34; 5 } 6 7 data = { 8 cluster_name = var.cluster_name 9 oidc_provider_arn = module.eks.oidc_provider_arn 10 aws_account_id = data.aws_caller_identity.this.account_id 11 region = var.region 12 environment = var.env 13 vpc_id = module.vpc.vpc_id 14 } 15 depends_on = [flux_bootstrap_git.this] 16} As mentioned previously, variable substitutions are defined in the Kustomization files. Let's take a concrete example. Below, we define the Kustomization that deploys all the resources controlled by the tf-controller. Here, we declare the eks-controlplane-0-vars ConfigMap that was generated during the EKS cluster creation.\ninfrastructure.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1 2kind: Kustomization 3metadata: 4 name: tf-custom-resources 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 path: ./infrastructure/controlplane-0/terraform/custom-resources 10 postBuild: 11 substitute: 12 domain_name: \u0026#34;cloud.ogenki.io\u0026#34; 13 substituteFrom: 14 - kind: ConfigMap 15 name: eks-controlplane-0-vars 16 - kind: Secret 17 name: eks-controlplane-0-vars 18 optional: true 19 sourceRef: 20 kind: GitRepository 21 name: flux-system 22 dependsOn: 23 - name: tf-controller Finally, below is an example of a Kubernetes resource that makes use of it. This single manifest can be used by all clusters!.\nexternal-dns/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-dns 5spec: 6... 7 values: 8 global: 9 imageRegistry: public.ecr.aws 10 fullnameOverride: external-dns 11 aws: 12 region: ${region} 13 zoneType: \u0026#34;public\u0026#34; 14 batchChangeSize: 1000 15 domainFilters: [\u0026#34;${domain_name}\u0026#34;] 16 logFormat: json 17 txtOwnerId: \u0026#34;${cluster_name}\u0026#34; 18 serviceAccount: 19 annotations: 20 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/${cluster_name}-external-dns\u0026#34; This will reduce significantly the number of overlays that were used to patch with cluster-specific parameters.\nWeb UI (Weave GitOps) In my previous article on Flux, I mentioned one of its downsides (when compared to its main competitor, ArgoCD): the lack of a Web interface. While I am a command line guy, this is sometimes useful to have a consolidated view and the ability to perform some operations with just a few clicks üñ±\nThis is now possible with Weave GitOps! Of course, it is not comparable to ArgoCD's UI, but the essentials are there: pausing reconciliation, visualizing manifests, dependencies, events...\nThere is also the VSCode plugin as an alternative.\nüí≠ Final thoughts One might say \u0026quot;yet another infrastructure management tool from Kubernetes\u0026quot;. Well this is true but despite a few minor issues faced along the way, which I shared on the project's Git repo, I really enjoyed the experience. tf-controller provides a concrete answer to a common question: how to manage our infrastructure like we manage our code?\nI really like the GitOps approach applied to infrastructure, and I had actually written an article on Crossplane. tf-controller tackles the problem from a different angle: using Terraform directly. This means that we can leverage our existing knowledge and code. There's no need to learn a new way of declaring our resources. This is an important criterion to consider because migrating to a new tool when you already have an existing infrastructure represents a significant effort. However, I would also add that tf-controller is only targeted at Flux users, which restricts its target audience.\nCurrently, I'm using a combination of Terraform, Terragrunt and RunAtlantis. tf-controller could become a serious alternative: We've talked about the value of Kustomize combined with variable substitions to avoid code deduplication. The project's roadmap also aims to display plans in pull-requests. Another frequent need is to pass sensitive information to modules. Using a Terraform resource, we can inject variables from Kubernetes secrets. This makes it possible to use common secrets management tools, such as external-secrets, sealed-secrets ...\nSo, I encourage you to try tf-controller yourself, and perhaps even contribute to it üôÇ\nNote The demo I used create quite a few resources, some of which are quite critical (like the network). So, keep in mind that this is just for the demo! I suggest taking a gradual approach if you plan to implement it: start by using drift detection, then create simple resources. I also took some shortcuts in terms of security that should be avoided, such as giving admin rights to the controller. ","link":"https://blog.ogenki.io/post/terraform-controller/","section":"post","tags":["infrastructure"],"title":"Applying GitOps Principles to Infrastructure: An overview of `tf-controller`"},{"body":"Kubernetes is now the de facto platform for orchestrating stateless applications. Containers that don't store data can be destroyed and easily recreated elsewhere. On the other hand, running persistent applications in an ephemeral environment can be quite challenging. There is an increasing number of mature cloud-native database solutions (like CockroachDB, TiDB, K8ssandra, Strimzi...) and there are a lot of things to consider when evaluating them:\nHow mature is the operator? What do the CRDs look like, which options, settings, and status do they expose? Which Kubernetes storage APIs does it leverage? (PV/PVC, CSI, snapshots...) Can it differentiate HDD and SSD, local/remote storage? What happens when something goes wrong: how resilient is the system? Backup and recovery: how easy is it to perform and schedule backups? What replication and scaling options are available? What about connection and concurrency limits, connection pooling, bouncers? Observability: what metrics are exposed and how? I was looking for a solution to host a PostgreSQL database. This database is a requirement for a ticket reservation software named Alf.io that's being used for an upcoming event: The Kubernetes Community Days France. (By the way you're welcome to submit a talk üëê, the CFP closes soon).\nI was specifically looking for a cloud-agnostic solution, with emphasis on ease of use. I was already familiar with several Kubernetes operators, and I ended up evaluating a fairly new kid on the block: CloudNativePG.\nCloudNativePG is the Kubernetes operator that covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication.\nIt has been created by the company EnterpriseDB, who submitted it to the CNCF in order to join the Sandbox projects.\nüéØ Our target I'm going to give you an introduction to the main CloudNativePG features. The plan is to:\ncreate a PostgreSQL database on a GKE cluster, add a standby instance, run a few resiliency tests. We will also see how it behaves in terms of performances and what are the observability tools available. Finally we'll have a look to the backup/restore methods.\nInfo In this article, we will create and update everything manually; but in production, we probably should use a GitOps engine, for instance Flux (which has been covered in a previous article).\nIf you want to see a complete end-to-end example, you can look at the KCD France infrastructure repository.\nAll the manifests shown in this article can be found in this repository.\n‚òëÔ∏è Requirements üì• Tooling gcloud SDK: we're going to deploy on Google Cloud (specifically, on GKE) and we will need to create a few resources in our GCP project; so we'll need the Google Cloud SDK and CLI. If needed, you can install and configure it using this documentation.\nkubectl plugin: to facilitate cluster management, CloudNativePG comes with a handy kubectl plugin that gives insights of your PostgreSQL instance and allows to perform some operations. It can be installed using krew as follows:\n1kubectl krew install cnpg ‚òÅÔ∏è Create the Google cloud requirements Before creating our PostgreSQL instance, we need to configure a few things:\nWe need a Kubernetes cluster. This article assumes that you have already taken care of provisioning a GKE cluster. We'll create a bucket to store the backups and WAL files. We'll grant permissions to our pods so that they can write to that bucket. Create the bucket using gcloud CLI\n1gcloud storage buckets create --location=eu --default-storage-class=coldline gs://cnpg-ogenki 2Creating gs://cnpg-ogenki/... 3 4gcloud storage buckets describe gs://cnpg-ogenki 5[...] 6name: cnpg-ogenki 7owner: 8 entity: project-owners-xxxx0008 9projectNumber: \u0026#39;xxx00008\u0026#39; 10rpo: DEFAULT 11selfLink: https://www.googleapis.com/storage/v1/b/cnpg-ogenki 12storageClass: STANDARD 13timeCreated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; 14updated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; Now, we're going to give the permissions to the pods (PostgreSQL server) to write/read from the bucket using Workload Identity.\nNote The GKE cluster should be configured with Workload Identity enabled. Check your cluster configuration, you should get something with this command:\n1gcloud container clusters describe \u0026lt;cluster_name\u0026gt; --format json --zone \u0026lt;zone\u0026gt; | jq .workloadIdentityConfig 2{ 3 \u0026#34;workloadPool\u0026#34;: \u0026#34;{{ gcp_project }}.svc.id.goog\u0026#34; 4} Create a Google Cloud service account\n1gcloud iam service-accounts create cloudnative-pg --project={{ gcp_project }} 2Created service account [cloudnative-pg]. Assign the storage.admin permission to the serviceaccount\n1gcloud projects add-iam-policy-binding {{ gcp_project }} \\ 2--member \u0026#34;serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com\u0026#34; \\ 3--role \u0026#34;roles/storage.admin\u0026#34; 4[...] 5- members: 6 - serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 7 role: roles/storage.admin 8etag: BwXrGA_VRd4= 9version: 1 Allow the Kubernetes service account to impersonate the IAM service account. ‚ÑπÔ∏è ensure you use the proper format serviceAccount:{{ gcp_project }}.svc.id.goog[{{ kubernetes_namespace }}/{{ kubernetes_serviceaccount }}]\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 7 role: roles/iam.workloadIdentityUser 8etag: BwXrGBjt5kQ= 9version: 1 We're ready to create the Kubernetes resources üí™\nüîë Create the users secrets We need to create the users credentials that will be used during the bootstrap process (more info later on): The superuser and the newly created database owner.\n1kubectl create secret generic cnpg-mydb-superuser --from-literal=username=postgres --from-literal=password=foobar --namespace demo 2secret/cnpg-mydb-superuser created 1kubectl create secret generic cnpg-mydb-user --from-literal=username=smana --from-literal=password=barbaz --namespace demo 2secret/cnpg-mydb-user created üõ†Ô∏è Deploy the CloudNativePG operator using Helm CloudNativePG is basically a Kubernetes operator which comes with some CRDs. We'll use the Helm chart as follows\n1helm repo add cnpg https://cloudnative-pg.github.io/charts 2 3helm upgrade --install cnpg --namespace cnpg-system \\ 4--create-namespace charts/cloudnative-pg 5 6kubectl get po -n cnpg-system 7NAME READY STATUS RESTARTS AGE 8cnpg-74488f5849-8lhjr 1/1 Running 0 6h17m Here are the Custom Resource Definitions installed along with the operator.\n1kubectl get crds | grep cnpg.io 2backups.postgresql.cnpg.io 2022-10-08T16:15:14Z 3clusters.postgresql.cnpg.io 2022-10-08T16:15:14Z 4poolers.postgresql.cnpg.io 2022-10-08T16:15:14Z 5scheduledbackups.postgresql.cnpg.io 2022-10-08T16:15:14Z For a full list of the available parameters for these CRDs please refer to the API reference.\nüöÄ Create a PostgreSQL server Now we can create our first instance using a custom resource Cluster. The following definition is pretty simple: we want to start a PostgreSQL server, automatically create a database named mydb and configure the credentials based on the secrets created previously.\n1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki 5 namespace: demo 6spec: 7 description: \u0026#34;PostgreSQL Demo Ogenki\u0026#34; 8 imageName: ghcr.io/cloudnative-pg/postgresql:14.5 9 instances: 1 10 11 bootstrap: 12 initdb: 13 database: mydb 14 owner: smana 15 secret: 16 name: cnpg-mydb-user 17 18 serviceAccountTemplate: 19 metadata: 20 annotations: 21 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 22 23 superuserSecret: 24 name: cnpg-mydb-superuser 25 26 storage: 27 storageClass: standard 28 size: 10Gi 29 30 backup: 31 barmanObjectStore: 32 destinationPath: \u0026#34;gs://cnpg-ogenki\u0026#34; 33 googleCredentials: 34 gkeEnvironment: true 35 retentionPolicy: \u0026#34;30d\u0026#34; 36 37 resources: 38 requests: 39 memory: \u0026#34;1Gi\u0026#34; 40 cpu: \u0026#34;500m\u0026#34; 41 limits: 42 memory: \u0026#34;1Gi\u0026#34; Create the namespace where our PostgreSQL instance will be deployed\n1kubectl create ns demo 2namespace/demo created Change the above cluster manifest to fit your needs and apply it.\n1kubectl apply -f cluster.yaml 2cluster.postgresql.cnpg.io/ogenki created You'll notice that the cluster will be in initializing phase. Let's use the cnpg plugin for the first time, it will become our best friend to display a neat view of the cluster's status.\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Primary server is initializing 4Name: ogenki 5Namespace: demo 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: (switching to ogenki-1) 8Status: Setting up primary Creating primary instance ogenki-1 9Instances: 1 10Ready instances: 0 11 12Certificates Status 13Certificate Name Expiration Date Days Left Until Expiration 14---------------- --------------- -------------------------- 15ogenki-ca 2023-01-13 20:02:40 +0000 UTC 90.00 16ogenki-replication 2023-01-13 20:02:40 +0000 UTC 90.00 17ogenki-server 2023-01-13 20:02:40 +0000 UTC 90.00 18 19Continuous Backup status 20First Point of Recoverability: Not Available 21No Primary instance found 22Streaming Replication status 23Not configured 24 25Instances status 26Name Database Size Current LSN Replication role Status QoS Manager Version Node 27---- ------------- ----------- ---------------- ------ --- --------------- ---- The first thing that runs under the hood is the bootstrap process. In our example we create a brand new database named mydb with an owner smana and credentials are retrieved from the secret we created previously.\n1[...] 2 bootstrap: 3 initdb: 4 database: mydb 5 owner: smana 6 secret: 7 name: cnpg-mydb-user 8[...] 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 0/1 Running 0 55s 4ogenki-1-initdb-q75cz 0/1 Completed 0 2m32s After a few seconds the cluster becomes ready üëè\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7154833472216277012 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 1 10Ready instances: 1 11 12[...] 13 14Instances status 15Name Database Size Current LSN Replication role Status QoS Manager Version Node 16---- ------------- ----------- ---------------- ------ --- --------------- ---- 17ogenki-1 33 MB 0/17079F8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xczh Info There are many ways of bootstrapping your cluster. For instance, restoring a backup into a brand new instance, running SQL scripts... more info here.\nü©π Standby instance and resiliency Info In traditional PostgreSQL architectures we usually find an additional component to handle high availability (e.g. Patroni). A specific aspect of the CloudNativePG operator is that it leverages built-in Kubernetes features and relies on a component named Postgres instance manager.\nAdd a standby instance by setting the number of replicas to 2.\n1kubectl edit cluster -n demo ogenki 2cluster.postgresql.cnpg.io/ogenki edited 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3[...] 4spec: 5 instances: 2 6[...] The operator immediately notices the change, adds a standby instance, and starts the replication process.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Creating a new replica Creating replica ogenki-2-join 9Instances: 2 10Ready instances: 1 11Current Write LSN: 0/1707A30 (Timeline: 1 - WAL File: 000000010000000000000001) 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 0 3m16s 4ogenki-2-join-xxrwx 0/1 Pending 0 82s After a while (depending on the amount of data to replicate), the standby instance will be up and running and we can see the replication statistics.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 2 10Ready instances: 2 11Current Write LSN: 0/3000060 (Timeline: 1 - WAL File: 000000010000000000000003) 12 13[...] 14 15Streaming Replication status 16Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 17---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 18ogenki-2 0/3000060 0/3000060 0/3000060 0/3000060 00:00:00 00:00:00 00:00:00 streaming async 0 19 20Instances status 21Name Database Size Current LSN Replication role Status QoS Manager Version Node 22---- ------------- ----------- ---------------- ------ --- --------------- ---- 23ogenki-1 33 MB 0/3000060 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 24ogenki-2 33 MB 0/3000060 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc Let's promote the standby instance to primary (perform a Switchover).\nThe cnpg plugin allows to do this imperatively by running this command\n1kubectl cnpg promote ogenki ogenki-2 -n demo 2Node ogenki-2 in cluster ogenki will be promoted In my case the switchover was really fast. We can check that the instance ogenki-2 is now the primary and that the replication is done the other way around.\n1kubectl cnpg status -n demo ogenki 2[...] 3Status: Switchover in progress Switching over to ogenki-2 4Instances: 2 5Ready instances: 1 6[...] 7Streaming Replication status 8Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 9---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 10ogenki-1 0/4004CA0 0/4004CA0 0/4004CA0 0/4004CA0 00:00:00 00:00:00 00:00:00 streaming async 0 11 12Instances status 13Name Database Size Current LSN Replication role Status QoS Manager Version Node 14---- ------------- ----------- ---------------- ------ --- --------------- ---- 15ogenki-2 33 MB 0/4004CA0 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc 16ogenki-1 33 MB 0/4004CA0 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 Now let's simulate a Failover by deleting the primary pod\n1kubectl delete po -n demo --grace-period 0 --force ogenki-2 2Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. 3pod \u0026#34;ogenki-2\u0026#34; force deleted 1Cluster Summary 2Name: ogenki 3Namespace: demo 4System ID: 7155095145869606932 5PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 6Primary instance: ogenki-1 7Status: Failing over Failing over from ogenki-2 to ogenki-1 8Instances: 2 9Ready instances: 1 10Current Write LSN: 0/4005D98 (Timeline: 3 - WAL File: 000000030000000000000004) 11 12[...] 13Instances status 14Name Database Size Current LSN Replication role Status QoS Manager Version Node 15---- ------------- ----------- ---------------- ------ --- --------------- ---- 16ogenki-1 33 MB 0/40078D8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 17ogenki-2 - - - pod not available Burstable - gke-kcdfrance-main-np-0e87115b-xszc After a few seconds, the cluster becomes healthy again\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 13m 2 2 Cluster in healthy state ogenki-1 So far so good, we've been able to test the high availability and the experience is pretty smooth üòé.\nüëÅÔ∏è Monitoring We're going to use Prometheus Stack. We won't cover its installation in this article. If you want to see how to install it \u0026quot;the GitOps way\u0026quot; you can check this example.\nTo scrape our instance's metrics, we need to create a PodMonitor.\n1apiVersion: monitoring.coreos.com/v1 2kind: PodMonitor 3metadata: 4 labels: 5 prometheus-instance: main 6 name: cnpg-ogenki 7 namespace: demo 8spec: 9 namespaceSelector: 10 matchNames: 11 - demo 12 podMetricsEndpoints: 13 - port: metrics 14 selector: 15 matchLabels: 16 postgresql: ogenki We can then add the Grafana dashboard available here.\nFinally, you may want to configure alerts and you can create a PrometheusRule using these rules.\nüî• Performances and benchmark Info Update: It is now possible to use the cnpg plugin. The following method is deprecated, I'll update it asap.\nThis is worth running a performance test in order to know the limits of your current server and keep a baseline for further improvements.\nNote When it comes to performance there are many improvement areas we can work on. It mostly depends on the target we want to achieve. Indeed we don't want to waste time and money for performance we'll likely never need.\nHere are the main things to look at:\nPostgreSQL configuration tuning Compute resources (cpu and memory) Disk type IOPS, local storage (local-volume-provisioner), Dedicated disks for WAL and PG_DATA Connection pooling PGBouncer. The CloudNativePG comes with a CRD Pooler to handle that. Database optimization, analyzing the query plans using explain, use the extension pg_stat_statement ... First of all we'll add labels to the nodes in order to run the pgbench command on different machines than the ones hosting the database.\n1PG_NODE=$(kubectl get po -n demo -l postgresql=ogenki,role=primary -o jsonpath={.items[0].spec.nodeName}) 2kubectl label node ${PG_NODE} workload=postgresql 3node/gke-kcdfrance-main-np-0e87115b-vlzm labeled 4 5 6# Choose any other node different than the ${PG_NODE} 7kubectl label node gke-kcdfrance-main-np-0e87115b-p5d7 workload=pgbench 8node/gke-kcdfrance-main-np-0e87115b-p5d7 labeled And we'll deploy the Helm chart as follows\n1git clone git@github.com:EnterpriseDB/cnp-bench.git 2cd cnp-bench 3 4cat \u0026gt; pgbench-benchmark/myvalues.yaml \u0026lt;\u0026lt;EOF 5cnp: 6 existingCluster: true 7 existingHost: ogenki-rw 8 existingCredentials: cnpg-mydb-superuser 9 existingDatabase: mydb 10 11pgbench: 12 # Node where to run pgbench 13 nodeSelector: 14 workload: pgbench 15 initialize: true 16 scaleFactor: 1 17 time: 600 18 clients: 10 19 jobs: 1 20 skipVacuum: false 21 reportLatencies: false 22EOF 23 24helm upgrade --install -n demo pgbench -f pgbench-benchmark/myvalues.yaml pgbench-benchmark/ Info There are different services depending on wether you want to read and write or read only.\n1kubectl get ep -n demo 2NAME ENDPOINTS AGE 3ogenki-any 10.64.1.136:5432,10.64.1.3:5432 15d 4ogenki-r 10.64.1.136:5432,10.64.1.3:5432 15d 5ogenki-ro 10.64.1.136:5432 15d 6ogenki-rw 10.64.1.3:5432 15d 1kubectl logs -n demo job/pgbench-pgbench-benchmark -f 2Defaulted container \u0026#34;pgbench\u0026#34; out of: pgbench, wait-for-cnp (init), pgbench-init (init) 3pgbench (14.1, server 14.5 (Debian 14.5-2.pgdg110+2)) 4starting vacuum...end. 5transaction type: \u0026lt;builtin: TPC-B (sort of)\u0026gt; 6scaling factor: 1 7query mode: simple 8number of clients: 10 9number of threads: 1 10duration: 600 s 11number of transactions actually processed: 545187 12latency average = 11.004 ms 13initial connection time = 111.585 ms 14tps = 908.782896 (without initial connection time) üíΩ Backup and Restore Note Writing backups and WAL files to the GCP bucket is possible because we gave the permissions using an annotation in the pod's serviceaccount\n1 serviceAccountTemplate: 2 metadata: 3 annotations: 4 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com We can first trigger an on-demand backup using the custom resource Backup\n1apiVersion: postgresql.cnpg.io/v1 2kind: Backup 3metadata: 4 name: ogenki-now 5 namespace: demo 6spec: 7 cluster: 8 name: ogenki 1kubectl apply -f backup.yaml 2backup.postgresql.cnpg.io/ogenki-now created 3 4kubectl get backup -n demo 5NAME AGE CLUSTER PHASE ERROR 6ogenki-now 36s ogenki completed If you take a look at the Google Cloud Storage content you'll see an new directory that stores the base backups\n1gcloud storage ls gs://cnpg-ogenki/ogenki/base 2gs://cnpg-ogenki/ogenki/base/20221023T130327/ But most of the time we would want to have a scheduled backup. So let's configure a daily schedule.\n1apiVersion: postgresql.cnpg.io/v1 2kind: ScheduledBackup 3metadata: 4 name: ogenki-daily 5 namespace: demo 6spec: 7 backupOwnerReference: self 8 cluster: 9 name: ogenki 10 schedule: 0 0 0 * * * Recoveries can only be done on new instances. Here we'll use the backup we've created previously to bootstrap a new instance with it.\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore] 7 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 8 role: roles/iam.workloadIdentityUser 9etag: BwXrs755FPA= 10version: 1 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki-restore 5 namespace: demo 6spec: 7 instances: 1 8 9 serviceAccountTemplate: 10 metadata: 11 annotations: 12 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 13 14 storage: 15 storageClass: standard 16 size: 10Gi 17 18 resources: 19 requests: 20 memory: \u0026#34;1Gi\u0026#34; 21 cpu: \u0026#34;500m\u0026#34; 22 limits: 23 memory: \u0026#34;1Gi\u0026#34; 24 25 superuserSecret: 26 name: cnpg-mydb-superuser 27 28 bootstrap: 29 recovery: 30 backup: 31 name: ogenki-now We can notice a first pod that performs the full recovery from the backup.\n1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 1 (18h ago) 18h 4ogenki-2 1/1 Running 0 18h 5ogenki-restore-1 0/1 Init:0/1 0 0s 6ogenki-restore-1-full-recovery-5p4ct 0/1 Completed 0 51s Then the new cluster becomes ready.\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 18h 2 2 Cluster in healthy state ogenki-1 4ogenki-restore 80s 1 1 Cluster in healthy state ogenki-restore-1 üßπ Cleanup Delete the cluster\n1kubectl delete cluster -n demo ogenki ogenki-restore 2cluster.postgresql.cnpg.io \u0026#34;ogenki\u0026#34; deleted 3cluster.postgresql.cnpg.io \u0026#34;ogenki-restore\u0026#34; deleted Cleanup the IAM serviceaccount\n1gcloud iam service-accounts delete cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 2You are about to delete service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 3 4Do you want to continue (Y/n)? y 5 6deleted service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com] üí≠ final thoughts I just discovered CloudNativePG and I only scratched the surface but one thing for sure is that managing PostgreSQL is really made easy. However choosing a database solution is a tough decision. Depending on the use case, the company constraints, the criticity of the application and the ops teams skills, there are plenty of options: Cloud managed databases, traditional bare metal installations, building the architecture with an Infrastructure As Code tool...\nWe may also consider using Crossplane and composition to give an opinionated way of declaring managed databases in cloud providers but that requires more configuration.\nCloudNativePG shines by its simplicity: it is easy to run and easy to understand. Furthermore the documentation is excellent (one of the best I ever seen!), especially for such a young open source project (Hopefuly this will help in the CNCF Sandbox acceptance process ü§û).\nIf you want to learn more about it, there was a presentation on about it at KubeCon NA 2022.\n","link":"https://blog.ogenki.io/post/cnpg/","section":"post","tags":["data"],"title":"`CloudNativePG`: An easy way to run PostgreSQL on Kubernetes"},{"body":"","link":"https://blog.ogenki.io/tags/data/","section":"tags","tags":null,"title":"data"},{"body":"In a previous article, we've seen how to use Crossplane so that we can manage cloud resources the same way as our applications. ‚ù§Ô∏è Declarative approach! There were several steps and command lines in order to get everything working and reach our target to provision a dev Kubernetes cluster.\nHere we'll achieve exactly the same thing but we'll do that in the GitOps way. According to the OpenGitOps working group there are 4 GitOps principles:\nThe desired state of our system must be expressed declaratively. This state must be stored in a versioning system. Changes are pulled and applied automatically in the target platform whenever the desired state changes. If, for any reason, the current state is modified, it will be automatically reconciled with the desired state. There are several GitOps engine options. The most famous ones are ArgoCD and Flux. We won't compare them here. I chose Flux because I like its composable architecture with different controllers, each one handling a core Flux feature (GitOps toolkit).\nLearn more about GitOps toolkit components here.\nüéØ Our target Here we want to declare our desired infrastructure components only by adding git changes. By the end of this article you'll get a GKE cluster provisioned using a local Crossplane instance. We'll discover Flux basics and how to use it in order to build a complete GitOps CD workflow.\n‚òëÔ∏è Requirements üì• Install required tools First of all we need to install a few tools using asdf\nCreate a local file .tool-versions\n1cd ~/sources/devflux/ 2 3cat \u0026gt; .tool-versions \u0026lt;\u0026lt;EOF 4flux2 0.31.3 5kubectl 1.24.3 6kubeseal 0.18.1 7kustomize 4.5.5 8EOF 1for PLUGIN in $(cat .tool-versions | awk \u0026#39;{print $1}\u0026#39;); do asdf plugin-add $PLUGIN; done 2 3asdf install 4Downloading ... 100.0% 5Copying Binary 6... Check that all the required tools are actually installed.\n1asdf current 2flux2 0.31.3 /home/smana/sources/devflux/.tool-versions 3kubectl 1.24.3 /home/smana/sources/devflux/.tool-versions 4kubeseal 0.18.1 /home/smana/sources/devflux/.tool-versions 5kustomize 4.5.5 /home/smana/sources/devflux/.tool-versions üîë Create a Github personal access token In this article the git repository is hosted in Github. In order to be able to use the flux bootstrap a personnal access token is required.\nPlease follow this procedure.\nWarning Store the Github token in a safe place for later use\nüßë‚Äçüíª Clone the devflux repository All the files used for the upcoming steps can be retrieved from this repository. You should clone it, that will be easier to copy them into your own repository.\n1git clone https://github.com/Smana/devflux.git üöÄ Bootstrap flux in the Crossplane cluster As we will often be using the flux CLI you may want to configure the bash|zsh completion\n1source \u0026lt;(flux completion bash) Warning Here we consider that you already have a local k3d instance. If not you may want to either go through the whole previous article or just run the local cluster creation.\nEnsure that you're working in the right context\n1kubectl config current-context 2k3d-crossplane Run the bootstrap command that will basically deploy all Flux's components in the namespace flux-system. Here I'll create a repository named devflux using my personal Github account.\n1export GITHUB_USER=\u0026lt;YOUR_ACCOUNT\u0026gt; 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/k3d-crossplane 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/k3d-crossplane\u0026#34; for \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux\u0026#34; 9... 10‚úî all components are healthy Check that all the pods are running properly and that the kustomization flux-system has been successfully reconciled.\n1kubectl get po -n flux-system 2NAME READY STATUS RESTARTS AGE 3helm-controller-5985c795f8-gs2pc 1/1 Running 0 86s 4notification-controller-6b7d7485fc-lzlpg 1/1 Running 0 86s 5kustomize-controller-6d4669f847-9x844 1/1 Running 0 86s 6source-controller-5fb4888d8f-wgcqv 1/1 Running 0 86s 7 8flux get kustomizations 9NAME REVISION SUSPENDED READY MESSAGE 10flux-system main/33ebef1 False True Applied revision: main/33ebef1 Running the bootstap command actually creates a github repository if it doesn't exist yet. Clone it now for our upcoming changes. You'll notice that the first commit has been made by Flux.\n1git clone https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git 2Cloning into \u0026#39;devflux\u0026#39;... 3 4cd devflux 5 6git log -1 7commit 2beb6aafea67f3386b50cbc706fb34575844040d (HEAD -\u0026gt; main, origin/main, origin/HEAD) 8Author: Flux \u0026lt;\u0026gt; 9Date: Thu Jul 14 17:13:27 2022 +0200 10 11 Add Flux sync manifests 12 13ls clusters/k3d-crossplane/flux-system/ 14gotk-components.yaml gotk-sync.yaml kustomization.yaml üìÇ Flux repository structure There are several options for organizing your resources in the Flux configuration repository. Here is a proposition for the sake of this article.\n1tree -d -L 2 2. 3‚îú‚îÄ‚îÄ apps 4‚îÇ¬†‚îú‚îÄ‚îÄ base 5‚îÇ¬†‚îî‚îÄ‚îÄ dev-cluster 6‚îú‚îÄ‚îÄ clusters 7‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 8‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 9‚îú‚îÄ‚îÄ infrastructure 10‚îÇ¬†‚îú‚îÄ‚îÄ base 11‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 12‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 13‚îú‚îÄ‚îÄ observability 14‚îÇ¬†‚îú‚îÄ‚îÄ base 15‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 16‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 17‚îî‚îÄ‚îÄ security 18 ‚îú‚îÄ‚îÄ base 19 ‚îú‚îÄ‚îÄ dev-cluster 20 ‚îî‚îÄ‚îÄ k3d-crossplane Directory Description Example /apps our applications Here we'll deploy a demo application \u0026quot;online-boutique\u0026quot; /infrastructure base infrastructure/network components Crossplane as it will be used to provision cloud resources but we can also find CSI/CNI/EBS drivers... /observability All metrics/apm/logging tools Prometheus of course, Opentelemetry ... /security Any component that enhance our security level SealedSecrets (see below) Info For the upcoming steps please refer to the demo repository here\nLet's use this structure and begin to deploy applications üöÄ.\nüîê SealedSecrets There are plenty of alternatives when it comes to secrets management in Kubernetes. In order to securely store secrets in a git repository the GitOps way we'll make use of SealedSecrets. It uses a custom resource definition named SealedSecrets in order to encrypt the Kubernetes secret at the client side then the controller is in charge of decrypting and generating the expected secret in the cluster.\nüõ†Ô∏è Deploy the controller using Helm The first thing to do is to declare the kustomization that handles all the security tools.\nclusters/k3d-crossplane/security.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: security 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 sourceRef: 10 kind: GitRepository 11 name: flux-system 12 path: ./security/k3d-crossplane 13 healthChecks: 14 - apiVersion: helm.toolkit.fluxcd.io/v1beta1 15 kind: HelmRelease 16 name: sealed-secrets 17 namespace: kube-system Info A Kustomization is a custom resource that comes with Flux. It basically points to a set of Kubernetes resources managed with kustomize The above security kustomization points to a local directory where the kustomize resources are.\n1... 2spec: 3 path: ./security/k3d-crossplane 4... Note This is worth noting that there are two types on kustomizations. That can be confusing when you start playing with Flux.\nOne managed by flux's kustomize controller. Its API is kustomization.kustomize.toolkit.fluxcd.io The other kustomization.kustomize.config.k8s.io is for the kustomize overlay The kustomization.yaml file is always used for the kustomize overlay. Flux itself doesn't need this overlay in all cases, but if you want to use features of a Kustomize overlay you will occasionally need to create it in order to access them. It provides instructions to the Kustomize CLI.\nWe will deploy SealedSecrets using the Helm chart. So we need to declare the source of this chart. Using the kustomize overlay system, we'll first create the base files that will be inherited at the cluster level.\nsecurity/base/sealed-secrets/source.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: sealed-secrets 5 namespace: flux-system 6spec: 7 interval: 30m 8 url: https://bitnami-labs.github.io/sealed-secrets Then we'll define the HelmRelease which references the above source. Put the values you want to apply to the Helm chart under spec.values\nsecurity/base/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 releaseName: sealed-secrets 8 chart: 9 spec: 10 chart: sealed-secrets 11 sourceRef: 12 kind: HelmRepository 13 name: sealed-secrets 14 namespace: flux-system 15 version: \u0026#34;2.4.0\u0026#34; 16 interval: 10m0s 17 install: 18 remediation: 19 retries: 3 20 values: 21 fullnameOverride: sealed-secrets-controller 22 resources: 23 requests: 24 cpu: 80m 25 memory: 100Mi If you're starting your repository from scratch you'll need to generate the kustomization.yaml file (kustomize overlay).\n1kustomize create --autodetect security/base/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4- helmrelease.yaml 5- source.yaml Now we declare the sealed-secret kustomization at the cluster level. Just for the example we'll overwrite a value at the cluster level using kustomize's overlay system.\nsecurity/k3d-crossplane/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 values: 8 resources: 9 requests: 10 cpu: 100m security/k3d-crossplane/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3bases: 4 - ../../base 5patches: 6 - helmrelease.yaml Pushing our changes is the only thing to do in order to get sealed-secrets deployed in the target cluster.\n1git commit -m \u0026#34;security: deploy sealed-secrets in k3d-crossplane\u0026#34; 2[security/sealed-secrets 283648e] security: deploy sealed-secrets in k3d-crossplane 3 6 files changed, 66 insertions(+) 4 create mode 100644 clusters/k3d-crossplane/security.yaml 5 create mode 100644 security/base/sealed-secrets/helmrelease.yaml 6 create mode 100644 security/base/sealed-secrets/kustomization.yaml 7 create mode 100644 security/base/sealed-secrets/source.yaml 8 create mode 100644 security/k3d-crossplane/sealed-secrets/helmrelease.yaml 9 create mode 100644 security/k3d-crossplane/sealed-secrets/kustomization.yaml After a few seconds (1 minutes by default) a new kustomization will appear.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3flux-system main/d36a33c False True Applied revision: main/d36a33c 4security main/d36a33c False True Applied revision: main/d36a33c And all the resources that we declared in the flux repository should be available and READY.\n1flux get sources helm 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee False True stored artifact for revision \u0026#39;4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee\u0026#39; 1flux get helmrelease -n kube-system 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 2.2.0 False True Release reconciliation succeeded üß™ A first test SealedSecret Let's use the CLI kubeseal to test it out. We'll create a SealedSecret that will be decrypted by the sealed-secrets controller in the cluster and create the expected secret foobar\n1kubectl create secret generic foobar -n default --dry-run=client -o yaml --from-literal=foo=bar \\ 2| kubeseal --namespace default --format yaml | kubectl apply -f - 3sealedsecret.bitnami.com/foobar created 4 5kubectl get secret -n default foobar 6NAME TYPE DATA AGE 7foobar Opaque 1 3m13s 8 9kubectl delete sealedsecrets.bitnami.com foobar 10sealedsecret.bitnami.com \u0026#34;foobar\u0026#34; deleted ‚òÅÔ∏è Deploy and configure Crossplane üîë Create the Google service account secret The first thing we need to do in order to get Crossplane working is to create the GCP serviceaccount. The steps have been covered here in the previous article. We'll create a SealedSecret gcp-creds that contains the serviceaccount file crossplane.json.\ninfrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml\n1kubectl create secret generic gcp-creds --context k3d-crossplane -n crossplane-system --from-file=creds=./crossplane.json --dry-run=client -o yaml \\ 2| kubeseal --format yaml --namespace crossplane-system - \u0026gt; infrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml üîÑ Crossplane dependencies Now we will deploy Crossplane with Flux. I won't put the manifests here you'll find all of them in this repository. However it's important to understand that, in order to deploy and configure Crossplane properly we need to do that in a specific order. Indeed several CRD's (custom resource definitions) are required:\nFirst of all we'll install the crossplane controller. Then we'll configure the provider because the custom resource is now available thanks to the crossplane controller installation. Finally a provider installation deploys several CRDs that can be used to configure the provider itself and cloud resources. The dependencies between kustomizations can be controlled using the parameters dependsOn. Looking at the file clusters/k3d-crossplane/infrastructure.yaml, we can see for example that the kustomization infrastructure-custom-resources depends on the kustomization crossplane_provider which itself depends on crossplane-configuration....\n1--- 2apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 3kind: Kustomization 4metadata: 5 name: crossplane-provider 6spec: 7... 8 dependsOn: 9 - name: crossplane-core 10--- 11apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 12kind: Kustomization 13metadata: 14 name: crossplane-configuration 15spec: 16... 17 dependsOn: 18 - name: crossplane-provider 19--- 20apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 21kind: Kustomization 22metadata: 23 name: infrastructure-custom-resources 24spec: 25... 26 dependsOn: 27 - name: crossplane-configuration Commit and push the changes for the kustomisations to appear. Note that they'll be reconciled in the defined order.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3infrastructure-custom-resources False False dependency \u0026#39;flux-system/crossplane-configuration\u0026#39; is not ready 4crossplane-configuration False False dependency \u0026#39;flux-system/crossplane-provider\u0026#39; is not ready 5security main/666f85a False True Applied revision: main/666f85a 6flux-system main/666f85a False True Applied revision: main/666f85a 7crossplane-core main/666f85a False True Applied revision: main/666f85a 8crossplane-provider main/666f85a False True Applied revision: main/666f85a Then all Crossplane components will be deployed, we can have a look to the HelmRelease status for instance.\n1kubectl describe helmrelease -n crossplane-system crossplane 2... 3Status: 4 Conditions: 5 Last Transition Time: 2022-07-15T19:12:04Z 6 Message: Release reconciliation succeeded 7 Reason: ReconciliationSucceeded 8 Status: True 9 Type: Ready 10 Last Transition Time: 2022-07-15T19:12:04Z 11 Message: Helm upgrade succeeded 12 Reason: UpgradeSucceeded 13 Status: True 14 Type: Released 15 Helm Chart: crossplane-system/crossplane-system-crossplane 16 Last Applied Revision: 1.9.0 17 Last Attempted Revision: 1.9.0 18 Last Attempted Values Checksum: 056dc1c6029b3a644adc7d6a69a93620afd25b65 19 Last Release Revision: 2 20 Observed Generation: 1 21Events: 22 Type Reason Age From Message 23 ---- ------ ---- ---- ------- 24 Normal info 20m helm-controller HelmChart \u0026#39;crossplane-system/crossplane-system-crossplane\u0026#39; is not ready 25 Normal info 20m helm-controller Helm upgrade has started 26 Normal info 19m helm-controller Helm upgrade succeeded And our GKE cluster should also be created because we defined a bunch of crossplane custom resources in infrastructure/k3d-crossplane/custom-resources/crossplane\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RUNNING 34.x.x.190 europe-west9-a 22m üöÄ Bootstrap flux in the dev cluster Our local Crossplane cluster is now ready and it created our dev cluster and we also want it to be managed with Flux. So let's configure Flux for this dev cluster using the same bootstrap command.\nAuthenticate to the newly created cluster. The following command will automatically change your current context.\n1gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project \u0026lt;your_project\u0026gt; 2Fetching cluster endpoint and auth data. 3kubeconfig entry generated for dev-cluster. 4 5kubectl config current-context 6gke_\u0026lt;your_project\u0026gt;_europe-west9-a_dev-cluster Run the bootstrap command for the dev-cluster.\n1export GITHUB_USER=Smana 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/dev-cluster 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/Smana/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/dev-cluster\u0026#34; for \u0026#34;https://github.com/Smana/devflux\u0026#34; 9... 10‚úî all components are healthy Note It's worth noting that each Kubernetes cluster generates its own sealing keys. That means that if you recreate the dev-cluster, you must regenerate all the sealedsecrets. In our example we declared a secret in order to set the Grafana credentials. Here's the command you need to run in order to create a new version of the sealedsecret and don't forget to use the proper context üòâ.\n1kubectl create secret generic kube-prometheus-stack-grafana \\ 2--from-literal=admin-user=admin --from-literal=admin-password=\u0026lt;yourpassword\u0026gt; --namespace observability --dry-run=client -o yaml \\ 3| kubeseal --namespace observability --format yaml \u0026gt; observability/dev-cluster/kube-prometheus-stack/sealedsecrets.yaml After a few seconds we'll get the following kustomizations deployed.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3apps main/1380eaa False True Applied revision: main/1380eaa 4flux-system main/1380eaa False True Applied revision: main/1380eaa 5observability main/1380eaa False True Applied revision: main/1380eaa 6security main/1380eaa False True Applied revision: main/1380eaa Here we configured the prometheus stack and deployed a demo microservices stack named \u0026quot;online-boutique\u0026quot; This demo application exposes the frontend through a service of type LoadBalancer.\n1kubectl get svc -n demo frontend-external 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3frontend-external LoadBalancer 10.140.174.201 34.155.121.2 80:31943/TCP 7m44s Use the EXTERNAL_IP\nüïµ Troubleshooting The cheatsheet in Flux's documentation contains many ways for troubleshooting when something goes wrong. Here I'll just give a sample of my favorite command lines.\nObjects that aren't ready\n1flux get all -A --status-selector ready=false Checking the logs of a given kustomization\n1flux logs --kind kustomization --name infrastructure-custom-resources 22022-07-15T19:38:52.996Z info Kustomization/infrastructure-custom-resources.flux-system - server-side apply completed 32022-07-15T19:38:53.016Z info Kustomization/infrastructure-custom-resources.flux-system - Reconciliation finished in 66.12266ms, next run in 4m0s 42022-07-15T19:11:34.697Z info Kustomization/infrastructure-custom-resources.flux-system - Discarding event, no alerts found for the involved object Show how a given pod is managed by Flux.\n1flux trace -n crossplane-system pod/crossplane-5dc8d888d7-g95qx 2 3Object: Pod/crossplane-5dc8d888d7-g95qx 4Namespace: crossplane-system 5Status: Managed by Flux 6--- 7HelmRelease: crossplane 8Namespace: crossplane-system 9Revision: 1.9.0 10Status: Last reconciled at 2022-07-15 21:12:04 +0200 CEST 11Message: Release reconciliation succeeded 12--- 13HelmChart: crossplane-system-crossplane 14Namespace: crossplane-system 15Chart: crossplane 16Version: 1.9.0 17Revision: 1.9.0 18Status: Last reconciled at 2022-07-15 21:11:36 +0200 CEST 19Message: pulled \u0026#39;crossplane\u0026#39; chart with version \u0026#39;1.9.0\u0026#39; 20--- 21HelmRepository: crossplane 22Namespace: crossplane-system 23URL: https://charts.crossplane.io/stable 24Revision: 362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d 25Status: Last reconciled at 2022-07-15 21:11:35 +0200 CEST 26Message: stored artifact for revision \u0026#39;362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d\u0026#39; If you want to check what would be the changes before pushing your commit. In thi given example I just increased the cpu requests for the sealed-secrets controller.\n1flux diff kustomization security --path security/k3d-crossplane 2‚úì Kustomization diffing... 3‚ñ∫ HelmRelease/kube-system/sealed-secrets drifted 4 5metadata.generation 6 ¬± value change 7 - 6 8 + 7 9 10spec.values.resources.requests.cpu 11 ¬± value change 12 - 100m 13 + 120m 14 15‚ö†Ô∏è identified at least one change, exiting with non-zero exit code üßπ Cleanup Don't forget to delete the Cloud resources if you don't want to have a bad suprise üíµ! Just comment the file infrastructure/k3d-crossplane/custom-resources/crossplane/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 # - cluster.yaml 5 - network.yaml üëè Achievements With our current setup everything is configured using the GitOps approach:\nWe can manage infrastructure resources using Crossplane. Our secrets are securely stored in our git repository. We have a dev-cluster that we can enable or disable just but commenting a yaml file. Our demo application can be deployed from scratch in seconds. üí≠ final thoughts Flux is probably the tool I'm using the most on a daily basis. It's really amazing!\nWhen you get familiar with its concepts and the command line it becomes really easy to use and troubleshoot. You can use either Helm when a chart is available or Kustomize.\nHowever we faced a few issues:\nIt's not straightforward to find an efficient structure depending on the company needs. Especially when you have several Kubernetes controllers that depend on other CRDs. The Helm controller doesn't maintain a state of the Kubernetes resources deployed by the Helm chart. That means that if you delete a resource which has been deployed through a Helm chart, it won't be reconciled (It will change soon. Being discussed here) Flux doesn't provide itself a web UI and switching between CLIs (kubectl, flux ...) can be annoying from a developer perspective. (I'm going to test weave-gitops ) I've been using Flux in production for more than a year and we configured it with the image automation so that the only thing a developer has to do is to merge a pull request and the new version of the application is automatically deployed in the target cluster.\nI should probably give another try to ArgoCD in order to be able to compare these precisely ü§î.\n","link":"https://blog.ogenki.io/post/devflux/","section":"post","tags":["gitops","devxp"],"title":"100% `GitOps` using Flux"},{"body":"Who am I? I'm a senior Site Reliability Engineer with a particular interest in Linux containers and cloud technologies. I worked in different companies (small startups and large scale) and I've been working in different areas in order to improve the reliability, availability of the platform as well as the developer experience. I helped several companies in their transition to the Cloud. I was leading SRE/DevOps teams (diverse profiles with developers and SREs) and I really enjoy seeing them engaged in the same direction.\nAs side activities, I am an organizer of the Cloud Native Computing meetup in Paris and the Kubernetes Community Days France.\nHobbies: Reading SF books, Kick Boxing, Surfing/Skating/Inline Roller\n","link":"https://blog.ogenki.io/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://blog.ogenki.io/tags/devxp/","section":"tags","tags":null,"title":"devxp"},{"body":"","link":"https://blog.ogenki.io/tags/gitops/","section":"tags","tags":null,"title":"gitops"},{"body":"The target of this documentation is to be able to create and manage a GKE cluster using Crossplane.\nCrossplane leverages Kubernetes base principles in order to provision cloud resources and much more: a declarative approach with drift detections and reconciliations using control loops ü§Ø. In other words, we declare what cloud resources we want and Crossplane ensures that the target state matches the one applied through the Kubernetes API.\nHere are the steps we'll follow in order to get a Kubernetes cluster for development and experimentations use cases.\nüê≥ Create the local k3d cluster for Crossplane's control plane k3d is a lightweight kubernetes cluster that leverages k3s that runs in our local laptop. There are several deployment models for Crossplane, we could for instance deploy the control plane on a management cluster on Kubernetes or a control plane per Kubernetes cluster.\nHere I chose a simple method which is fine for a personal use case: A local Kubernetes instance in which I'll deploy Crossplane.\nLet's install k3d using asdf.\n1asdf plugin-add k3d 2 3asdf install k3d $(asdf latest k3d) 4* Downloading k3d release 5.4.1... 5k3d 5.4.1 installation was successful! Create a single node Kubernetes cluster.\n1k3d cluster create crossplane 2... 3INFO[0043] You can now use it like this: 4kubectl cluster-info 5 6k3d cluster list 7crossplane 1/1 0/0 true Check that the cluster is reachable using the kubectl CLI.\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:40643 3CoreDNS is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy We only need a single node for our Crossplane use case.\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-crossplane-server-0 Ready control-plane,master 26h v1.22.7+k3s1 ‚òÅÔ∏è Generate the Google Cloud service account Warning Store the downloaded crossplane.json credentials file in a safe place.\nCreate a service account\n1GCP_PROJECT=\u0026lt;your_project\u0026gt; 2gcloud iam service-accounts create crossplane --display-name \u0026#34;Crossplane\u0026#34; --project=${GCP_PROJECT} 3Created service account [crossplane]. Assign the proper permissions to the service account.\nCompute Network Admin Kubernetes Engine Admin Service Account User 1SA_EMAIL=$(gcloud iam service-accounts list --filter=\u0026#34;email ~ ^crossplane\u0026#34; --format=\u0026#39;value(email)\u0026#39;) 2 3gcloud projects add-iam-policy-binding \u0026#34;${GCP_PROJECT}\u0026#34; --member=serviceAccount:\u0026#34;${SA_EMAIL}\u0026#34; \\ 4--role=roles/container.admin --role=roles/compute.networkAdmin --role=roles/iam.serviceAccountUser 5Updated IAM policy for project [\u0026lt;project\u0026gt;]. 6bindings: 7- members: 8 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 9 role: roles/compute.networkAdmin 10- members: 11 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 12... 13version: 1 Download the service account key (json format)\n1gcloud iam service-accounts keys create crossplane.json --iam-account ${SA_EMAIL} 2created key [ea2eb9ce2939127xxxxxxxxxx] of type [json] as [crossplane.json] for [crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com] üöß Deploy and configure Crossplane Now that we have a credentials file for Google Cloud, we can deploy the Crossplane operator and configure the provider-gcp provider.\nInfo Most of the following steps are issued from the official documentation\nWe'll first use Helm in order to install the operator\n1helm repo add crossplane-master https://charts.crossplane.io/master/ 2\u0026#34;crossplane-master\u0026#34; has been added to your repositories 3 4helm repo update 5...Successfully got an update from the \u0026#34;crossplane-master\u0026#34; chart repository 6 7helm install crossplane --namespace crossplane-system --create-namespace \\ 8--version 1.18.1 crossplane-stable/crossplane 9 10NAME: crossplane 11LAST DEPLOYED: Mon Jun 6 22:00:02 2022 12NAMESPACE: crossplane-system 13STATUS: deployed 14REVISION: 1 15TEST SUITE: None 16NOTES: 17Release: crossplane 18... Check that the operator is running properly.\n1kubectl get po -n crossplane-system 2NAME READY STATUS RESTARTS AGE 3crossplane-rbac-manager-54d96cd559-222hc 1/1 Running 0 3m37s 4crossplane-688c575476-lgklq 1/1 Running 0 3m37s Info All the files used for the upcoming steps are stored within this blog repository. So you should clone and change the current directory:\n1git clone https://github.com/Smana/smana.github.io.git 2 3cd smana.github.io/content/resources/crossplane_k3d Now we'll configure Crossplane so that it will be able to create and manage GCP resources. This is done by configuring the provider provider-gcp as follows.\nprovider.yaml\n1apiVersion: pkg.crossplane.io/v1 2kind: Provider 3metadata: 4 name: crossplane-provider-gcp 5spec: 6 package: crossplane/provider-gcp:v0.21.0 1kubectl apply -f provider.yaml 2provider.pkg.crossplane.io/crossplane-provider-gcp created 3 4kubectl get providers 5NAME INSTALLED HEALTHY PACKAGE AGE 6crossplane-provider-gcp True True crossplane/provider-gcp:v0.21.0 10s Create the Kubernetes secret that holds the GCP credentials file created above\n1kubectl create secret generic gcp-creds -n crossplane-system --from-file=creds=./crossplane.json 2secret/gcp-creds created Then we need to create a resource named ProviderConfig and reference the newly created secret.\nprovider-config.yaml\n1apiVersion: gcp.crossplane.io/v1beta1 2kind: ProviderConfig 3metadata: 4 name: default 5spec: 6 projectID: ${GCP_PROJECT} 7 credentials: 8 source: Secret 9 secretRef: 10 namespace: crossplane-system 11 name: gcp-creds 12 key: creds 1kubectl apply -f provider-config.yaml 2providerconfig.gcp.crossplane.io/default created Info If the serviceaccount has the proper permissions we can create resources in GCP. In order to learn about all the available resources and parameters we can have a look to the provider's API reference.\nThe first resource we'll create is the network that will host our Kubernetes cluster.\nnetwork.yaml\n1apiVersion: compute.gcp.crossplane.io/v1beta1 2kind: Network 3metadata: 4 name: dev-network 5 labels: 6 service: vpc 7 creation: crossplane 8spec: 9 forProvider: 10 autoCreateSubnetworks: false 11 description: \u0026#34;Network used for experimentations and POCs\u0026#34; 12 routingConfig: 13 routingMode: REGIONAL 1kubectl get network 2NAME READY SYNCED 3dev-network True True You can even get more details by describing this resource. For instance if something fails you would see the message returned by the Cloud provider in the events.\n1kubectl describe network dev-network | grep -A 20 \u0026#39;^Status:\u0026#39; 2Status: 3 At Provider: 4 Creation Timestamp: 2022-06-28T09:45:30.703-07:00 5 Id: 3005424280727359173 6 Self Link: https://www.googleapis.com/compute/v1/projects/${GCP_PROJECT}/global/networks/dev-network 7 Conditions: 8 Last Transition Time: 2022-06-28T16:45:31Z 9 Reason: Available 10 Status: True 11 Type: Ready 12 Last Transition Time: 2022-06-30T16:36:59Z 13 Reason: ReconcileSuccess 14 Status: True 15 Type: Synced üöÄ Create a GKE cluster Everything is ready so that we can create our GKE cluster. Applying the file cluster.yaml will create a cluster and attach a node group to it.\ncluster.yaml\n1--- 2apiVersion: container.gcp.crossplane.io/v1beta2 3kind: Cluster 4metadata: 5 name: dev-cluster 6spec: 7 forProvider: 8 description: \u0026#34;Kubernetes cluster for experimentations and POCs\u0026#34; 9 initialClusterVersion: \u0026#34;1.24\u0026#34; 10 releaseChannel: 11 channel: \u0026#34;RAPID\u0026#34; 12 location: europe-west9-a 13 addonsConfig: 14 gcePersistentDiskCsiDriverConfig: 15 enabled: true 16 networkPolicyConfig: 17 disabled: false 18 networkRef: 19 name: dev-network 20 ipAllocationPolicy: 21 createSubnetwork: true 22 useIpAliases: true 23 defaultMaxPodsConstraint: 24 maxPodsPerNode: 110 25 networkPolicy: 26 enabled: false 27 writeConnectionSecretToRef: 28 namespace: default 29 name: gke-conn 30--- 31apiVersion: container.gcp.crossplane.io/v1beta1 32kind: NodePool 33metadata: 34 name: main-np 35spec: 36 forProvider: 37 initialNodeCount: 1 38 autoscaling: 39 autoprovisioned: false 40 enabled: true 41 maxNodeCount: 4 42 minNodeCount: 1 43 clusterRef: 44 name: dev-cluster 45 config: 46 machineType: n2-standard-2 47 diskSizeGb: 120 48 diskType: pd-standard 49 imageType: cos_containerd 50 preemptible: true 51 labels: 52 environment: dev 53 managed-by: crossplane 54 oauthScopes: 55 - \u0026#34;https://www.googleapis.com/auth/devstorage.read_only\u0026#34; 56 - \u0026#34;https://www.googleapis.com/auth/logging.write\u0026#34; 57 - \u0026#34;https://www.googleapis.com/auth/monitoring\u0026#34; 58 - \u0026#34;https://www.googleapis.com/auth/servicecontrol\u0026#34; 59 - \u0026#34;https://www.googleapis.com/auth/service.management.readonly\u0026#34; 60 - \u0026#34;https://www.googleapis.com/auth/trace.append\u0026#34; 61 metadata: 62 disable-legacy-endpoints: \u0026#34;true\u0026#34; 63 shieldedInstanceConfig: 64 enableIntegrityMonitoring: true 65 enableSecureBoot: true 66 management: 67 autoRepair: true 68 autoUpgrade: true 69 maxPodsConstraint: 70 maxPodsPerNode: 60 71 locations: 72 - \u0026#34;europe-west9-a\u0026#34; 1kubectl apply -f cluster.yaml 2cluster.container.gcp.crossplane.io/dev-cluster created 3nodepool.container.gcp.crossplane.io/main-np created Note that it takes around 10 minutes for the Kubernetes API and the nodes to be available. The STATE will transition from PROVISIONING to RUNNING and when a change is being applied the cluster status is RECONCILING\n1watch \u0026#39;kubectl get cluster,nodepool\u0026#39; 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3cluster.container.gcp.crossplane.io/dev-cluster False True PROVISIONING 34.155.122.6 europe-west9-a 3m15s 4 5NAME READY SYNCED STATE CLUSTER-REF AGE 6nodepool.container.gcp.crossplane.io/main-np False False dev-cluster 3m15s When the column READY switches to True you can download the cluster's credentials.\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RECONCILING 34.42.42.42 europe-west9-a 6m23s 4 5gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project ${GCP_PROJECT} 6Fetching cluster endpoint and auth data. 7kubeconfig entry generated for dev-cluster. For better readability you may want to rename the context id for the newly created cluster\n1kubectl config rename-context gke_${GCP_PROJECT}_europe-west9-a_dev-cluster dev-cluster 2Context \u0026#34;gke_${GCP_PROJECT}_europe-west9-a_dev-cluster\u0026#34; renamed to \u0026#34;dev-cluster\u0026#34;. 3 4kubectl config get-contexts 5CURRENT NAME CLUSTER AUTHINFO NAMESPACE 6* dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster 7 k3d-crossplane k3d-crossplane admin@k3d-crossplane Check that you can call our brand new GKE API\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3gke-dev-cluster-main-np-d0d978f9-5fc0 Ready \u0026lt;none\u0026gt; 10m v1.24.1-gke.1400 That's great üéâ we know have a GKE cluster up and running.\nüí≠ final thoughts I've been using Crossplane for a few months now in a production environment.\nEven if I'm conviced about the declarative approach using the Kubernetes API, we decided to move with caution with it. It clearly doesn't have Terraform's community and maturity. We're still declaring our resources using the deletionPolicy: Orphan so that even if something goes wrong on the controller side the resource won't be deleted.\nFurthermore we limited to a specific list of usual AWS resources requested by our developers. Nevertheless our target has always been to empower developers and we had really positive feedback from them. That's the best indicator for us. As the project matures, we'll move more and more resources from Terraform to Crossplane.\nIMHO the key success of Crossplane depends on the providers maintenance and evolution. The Cloud providers interest and involvement is really important.\nIn our next article we'll see how to use a GitOps engine to run all the above steps.\n","link":"https://blog.ogenki.io/post/crossplane_k3d/","section":"post","tags":["kubernetes","infrastructure"],"title":"My Kubernetes cluster (GKE) with `Crossplane`"},{"body":"","link":"https://blog.ogenki.io/tags/local/","section":"tags","tags":null,"title":"local"},{"body":"In order to install binaries and to be able to switch from a version to another I like to use asdf.\nüì• Installation The recommended installation is to use Git as follows\n1git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.10.0 Then depending on your shell here are the remaining steps to follow\n1. $HOME/.asdf/asdf.sh And you may want to configure the shell completion\n1. $HOME/.asdf/completions/asdf.bash üöÄ Let's take an example List all available plugins and look for k3d\n1asdf plugin-list-all | grep k3d 2k3d https://github.com/spencergilbert/asdf-k3d.git Let's install k3d\n1asdf plugin-add k3d Check the versions available\n1asdf list-all k3d| tail -n 3 25.4.0-dev.3 35.4.0 45.4.1 We'll install the latest version\n1asdf install k3d latest 2* Downloading k3d release 5.4.1... 3k3d 5.4.1 installation was successful! Finally we can switch from a version to another. We can set a global version that would be used on all directories.\n1asdf global k3d 5.4.1 or use a local version depending on the current directory\n1cd /tmp 2asdf local k3d 5.4.1 3 4asdf current k3d 5k3d 5.4.1 /tmp/.tool-versions üßπ Cleanup Uninstall a given version\n1asdf uninstall k3d 5.4.1 Remove a plugin\n1asdf plugin remove k3d ","link":"https://blog.ogenki.io/post/asdf/asdf/","section":"post","tags":["tooling","local"],"title":"Manage tools versions with `asdf`"},{"body":"","link":"https://blog.ogenki.io/tags/tooling/","section":"tags","tags":null,"title":"tooling"},{"body":"","link":"https://blog.ogenki.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://blog.ogenki.io/categories/devxp/","section":"categories","tags":null,"title":"devxp"},{"body":"","link":"https://blog.ogenki.io/tags/helm/","section":"tags","tags":null,"title":"Helm"},{"body":"Template challenge Here you‚Äôll be able to practice in order to get familiar with some of the possibilities offered by a templating language.\n(pro tip: Don't forget the testing)\nThese examples may seem useless but the purpose of this is just playing with templates.\n1 - \u0026quot;Configuration depends on region\u0026quot; Create a secret that contains a key 'secret' and a value \u0026quot;myEuropeanSecret\u0026quot;. Set an environment variable from this secret only if the value global.region is 'eu-west1' So the first step is to add the values into the values.yaml file.\n1global: 2 region: eu-west-1 2 - \u0026quot;Create only if\u0026quot; Create a job that prints the pod's IP on stdout based on a boolean value printIP.enabled. Use the \u0026quot;busybox\u0026quot; image and the command wget -qO - http://ipinfo.io/ip. The job should be created only if the value is True, before every other resource has been created (pre-install and pre-upgrade hooks)\n3 - \u0026quot;Looping\u0026quot; Given the following values, create a loop whether in the deployment or in the helpers.tpl file in order to add the environment variables.\n1envVars: 2 key1: value1 3 key2: value2 4 key3: value3 4 - \u0026quot;Playing with strings and sprigs\u0026quot; Add to the \u0026quot;common labels\u0026quot;, a new label \u0026quot;codename\u0026quot; with a value composed with the release name, the chart name and the date in the form 20061225. The release name must be at most 3 characters long. The whole string has to be in snakecase.\n(you should get something like codename: rel_web_20210215)\n5 - We want to create a list of etcd hosts in the form of \u0026quot;etcd-0,etcd-1,etcd-2\u0026quot; based on a integer that defines the number of etcd hosts 1etcd: 2 count: 5 This list has to be defined in an environment variable ETCD_HOSTS\nProposition of solutions try_first You should try to find a solution by your own to the above exercises before checking these solutions\n1 The following command generates a secrets in the templates directory\n1kubectl create secret generic --dry-run=client eu-secret --from-literal=secret=\u0026#39;myEuropeanSecret\u0026#39; -o yaml | kubectl neat \u0026gt; templates/secret.yaml Then we'll enclose the environment variable definition with a condition depending on the region in the deployment template.\n1 {{- if eq .Values.global.region \u0026#34;eu-west-1\u0026#34; }} 2 - name: eu-secret 3 valueFrom: 4 secretKeyRef: 5 name: eu-secret 6 key: secret 7 {{- end }} 2 First of all we need to add a new value\n1printIP: 2 enabled: True Then this command will generate a job yaml\n1kubectl create job my-ip --dry-run=client --image=busybox -o yaml -- wget -qO - http://ipinfo.io/ip | kubectl neat \u0026gt; templates/job.yaml If we enclose the whole yaml, it won't be created if the boolean is False. With the hook annotation here, the job will be created before any other resource will be applied. We defined a delete policy \u0026quot;hook-failed\u0026quot; in order to keep the job, otherwise it would have been deleted.\n1{{- if .Values.printIP.enabled -}} 2apiVersion: batch/v1 3kind: Job 4metadata: 5 name: my-ip 6 annotations: 7 \u0026#34;helm.sh/hook\u0026#34;: pre-install,pre-upgrade 8 \u0026#34;helm.sh/hook-weight\u0026#34;: \u0026#34;1\u0026#34; 9 \u0026#34;helm.sh/hook-delete-policy\u0026#34;: hook-failed 10... 11{{- end -}} 3 If we want to keep the deployment easy to read, we would prefer adding the code in the _helpers.tpl\n1{{/* 2Environment variables 3*/}} 4{{- define \u0026#34;web.envVars\u0026#34; -}} 5{{- range $key, $value := .Values.envVars }} 6- name: {{ $key }} 7 value: {{ $value }} 8{{- end }} 9{{- end -}} Then this new variable could be used in the deployment as follows\n1 env: 2 {{- include \u0026#34;web.envVars\u0026#34; . | nindent 12 }} 4 The common labels can be changed in the file templates/_helpers.tpl. Here's a proposal This one is tricky, I needed to dig back into the charts available in the stable github repository.\n1codename: {{ printf \u0026#34;%s %s %s\u0026#34; (.Release.Name | trunc 3) .Chart.Name (now | date \u0026#34;20060102\u0026#34;) | snakecase }} 5 Here's an option to achieve the expected results.\n1 env: 2 - name: ETCD_HOSTS 3 value: \u0026#34;{{ range $index, $e := until (.Values.etcd.count|int) }}{{- if $index }},{{end}}etcd-{{ $index }}{{- end }}\u0026#34; ","link":"https://blog.ogenki.io/post/series/workshop_helm/templating/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Templating exercises"},{"body":"","link":"https://blog.ogenki.io/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://blog.ogenki.io/series/workshop-helm/","section":"series","tags":null,"title":"Workshop Helm"},{"body":"Create a simple webserver chart In order to get familiar with a typical chart we will create a simple webserver chart.\n1$ helm create web 2Creating web The above command will create a chart directory named web\nweb/ charts directory that contains the subcharts Chart.yaml metadatas (author, version, description), dependencies and more templates contains all the templates basically kubernetes resources in the form of templated yaml files. (go template) deployment.yaml helpers.tpl helpers, functions that can be used from the templates. hpa.yaml ingress.yaml NOTES.txt This file is used to print information after a release has been successfully installed. serviceaccount.yaml service.yaml tests contains a job that will run a command to check the application after it has been installed. test-connection.yaml values.yaml Maybe the most important file. We‚Äôll play with the values to define how the kubernetes resources will be rendered. Testing the chart Here‚Äôs a combo if you want to check properly your chart before actually deploying it:\ntemplate + lint + kubeval + test\nGolang errors When you add templating changes, you should run the command helm template --debug \u0026lt;chart_dir\u0026gt;\n1$ helm template --debug web 2install.go:173: [debug] Original chart version: \u0026#34;\u0026#34; 3install.go:190: [debug] CHART PATH: /tmp/web 4 5Error: parse error at (web/templates/_helpers.tpl:73): unexpected EOF 6helm.go:81: [debug] parse error at (web/templates/_helpers.tpl:73): unexpected EOF Read carefully if there are error messages. Always use the option --debug to see the template rendering.\nChart linting The command helm lint \u0026lt;chart_dir\u0026gt; verifies that the chart is well-formed.\n1$ helm lint web/ 2==\u0026gt; Linting web/ 3[ERROR] Chart.yaml: apiVersion \u0026#39;v3\u0026#39; is not valid. The value must be either \u0026#34;v1\u0026#34; or \u0026#34;v2\u0026#34; 4[INFO] Chart.yaml: icon is recommended 5[ERROR] Chart.yaml: chart type is not valid in apiVersion \u0026#39;v3\u0026#39;. It is valid in apiVersion \u0026#39;v2\u0026#39; 6 7Error: 1 chart(s) linted, 1 chart(s) failed Validate Kubernetes resources In order to validate that the rendered kubernetes objects are well-formed we‚Äôll make use of a tool named kubeval.\nThis is even easier by using the Helm plugin.\nInstall the plugin:\n1$ helm plugin install https://github.com/instrumenta/helm-kubeval 2Installing helm-kubeval v0.13.0 ... 3helm-kubeval 0.13.0 is installed. Then check the chart as follows\n1$ helm kubeval web 2The file web/templates/serviceaccount.yaml contains a valid ServiceAccount 3The file web/templates/secret.yaml contains a valid Secret 4... Now you can safely install the chart\n1$ helm upgrade --install web web 2Release \u0026#34;web\u0026#34; has been upgraded. Happy Helming! 3NAME: web 4LAST DEPLOYED: Mon Feb 15 18:22:23 2021 5NAMESPACE: default 6STATUS: deployed 7REVISION: 1 Check that the application works as expected This is a good practice to add tests under the directory template/tests.\nBasically, this is achieved with a job that you can call when the release is already installed (just after)\nIt returns a code 0 if the command succeeds.\nIn the chart we‚Äôve already generated there‚Äôs a job that checks the webserver availability.\nCheck that the release is already installed\n1helm list 2NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION 3web default 1 2021-02-15 15:11:09.036602795 +0100 CET deployed web-0.1.0 1.16.0 1helm test web 2NAME: web 3LAST DEPLOYED: Mon Feb 15 15:11:09 2021 4NAMESPACE: default 5STATUS: deployed 6REVISION: 1 7TEST SUITE: web-test-connection 8Last Started: Mon Feb 15 16:55:17 2021 9Last Completed: Mon Feb 15 16:55:19 2021 10Phase: Succeeded Dependencies Sometimes, the application requires another component to work (caching, database, persistence ‚Ä¶).\nThis dependency system has to be used with caution because this is generally recommended to manage the applications lifecycles independently from each other.\nLet‚Äôs say that our webserver need to store the information related to the sessions in a Redis server.\nWe‚Äôll add a redis server to our web application by declaring the dependency in the file chart.yaml.\n1dependencies: 2 - name: redis 3 version: \u0026#34;12.6.4\u0026#34; 4 repository: https://charts.bitnami.com/bitnami 5 condition: redis.enabled As you may have noticed, this dependency will be pulled only if the condition redis.enabled is True.\nSo we need to change our values.yaml accordingly:\n1redis: 2 enabled: True 3 master: 4 persistence: 5 enabled: False Check all the available values for this chart here.\nWhenever you add a dependency and you‚Äôre using a local chart (on your laptop), you must run the following command to pull it\n1helm dep update web 2Hang tight while we grab the latest from your chart repositories... 3‚Ä¶. 4...Successfully got an update from the \u0026#34;bitnami\u0026#34; chart repository 5Update Complete. ‚éàHappy Helming!‚éà 6Saving 1 charts 7Downloading redis from repo https://charts.bitnami.com/bitnami 8Deleting outdated charts The dependencies are stored in the directory charts.\nAfter testing your changes you can install the release with the command\nhelm upgrade --install \u0026lt;release_name\u0026gt; \u0026lt;chart_dir\u0026gt;\n1helm upgrade --install web web 2Release \u0026#34;web\u0026#34; has been upgraded. Happy Helming! 3NAME: web 4LAST DEPLOYED: Mon Feb 15 18:22:23 2021 5NAMESPACE: default 6STATUS: deployed 7REVISION: 2 You can notice that your webserver has been successfully installed along with a HA Redis cluster\n1kubectl get po 2NAME READY STATUS RESTARTS AGE 3web-74bf5c6c66-fjsmb 1/1 Running 0 3h14m 4web-test-connection 0/1 Completed 0 90m 5web-redis-master-0 1/1 Running 0 3m14s 6web-redis-slave-0 1/1 Running 0 3m14s 7web-redis-slave-1 1/1 Running 0 2m42s Hooks Helm comes with a hook system that allows it to run jobs at given times of the lifecycle.\nThe description is crystal clear in the documentation and you‚Äôll have the opportunity to add one later on during this workshop.\nMastering the Golang template The main challenge when you start using Helm is to learn all the tips and tricks of the Golang template\nThe official Helm documentation is very useful for that.\nYour best friends when you write Helm templates are the Sprig functions, you should definitely add this to your bookmarks.\nFurthermore, even if it has been deprecated, you should clone/fork the original stable chart repository. Indeed it has a wide range of examples.\nNote that most of the time, if you want to keep the kubernetes manifests readable, you would put most of the code in what we call helpers files. There‚Äôs often at least one named _helpers.tpl.\nNote: Even if you can do pretty advanced things with this templating language, you shouldn‚Äôt overuse it in order to keep the kubernetes resources readable and the chart maintainable.\n‚û°Ô∏è Next: Application lifecycle using Helm\n","link":"https://blog.ogenki.io/post/series/workshop_helm/build_chart/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Build your first chart"},{"body":"Apply a change, anything. For example we will add a label stage: dev. Edit the file templates/_helpers.tpl\n1{{- define \u0026#34;web.labels\u0026#34; -}} 2stage: \u0026#34;dev\u0026#34; 3... Deploy a new revision with the same command we ran previously\n1helm upgrade --install web web Now we can have a look to the changes we‚Äôve made so far to the release\n1$ helm history web 2REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 31 Mon Feb 15 15:11:09 2021 superseded web-0.1.0 1.16.0 Install complete 4... 54 Mon Feb 15 21:15:25 2021 superseded web-0.1.0 1.16.0 Upgrade complete 65 Mon Feb 15 21:21:21 2021 deployed web-0.1.0 1.16.0 Upgrade complete We can then check what would be the changes if we rollback to the previous revision\n1helm diff rollback web 4 2default, web, Deployment (apps) has changed: 3 # Source: web/templates/deployment.yaml 4 apiVersion: apps/v1 5 kind: Deployment 6 metadata: 7 name: web 8 labels: 9- stage: \u0026#34;dev\u0026#34; 10 helm.sh/chart: web-0.1.0 11... Now that we‚Äôre sure we can safely rollback to the previous revision\n1helm rollback web 4 2Rollback was a success! Happy Helming! ‚û°Ô∏è Next: Helm templating challenge\n","link":"https://blog.ogenki.io/post/series/workshop_helm/lifecycle/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Lifecycle operations"},{"body":"Helm‚Äôs configuration is stored in the environment variable $HELM_CONFIG_HOME , by default $HOME/.config/helm\nAll the environment variables are described in the documentation there.\nHere are a few tools (with a wide adoption) that will add capabilities to Helm.\nPlugins There are several plugins in order to extend Helm‚Äôs features.\nSome of them are really useful (kubeval, diff, secrets).\nHelmfile Helmfile is a really useful tool that allows you to declare the state of the releases on your cluster.\nIt helps keeping a central view of the releases deployed on a given cluster.\nIt automatically configures repositories, pulls dependencies and it is very helpful to build CI/CD workflows.\nOf course it uses Helm under the hood and a few modules/plugins such as secrets decryption, helm diff\nThese steps are very basic, you should have a look at the documentation for further details.\nInstall the helmdiff plugin (used by helmfile)\n1helm plugin install https://github.com/databus23/helm-diff We‚Äôll make use of some examples provided by CloudPosse\n1git clone git@github.com:cloudposse/helmfiles.git 2cd helmfiles Let‚Äôs say we want to install the kubernetes dashboard and the reloader tool.\n1cat \u0026gt; releases/kubernetes-dashboard/dev.yaml \u0026lt;\u0026lt;EOF 2installed: True 3banner: \u0026#34;Workshop cluster\u0026#34; 4EOF Now we‚Äôll create our main helmfile.yaml that describes all the releases we want to install\n1cat \u0026gt; helmfile.yaml \u0026lt;\u0026lt;EOF 2helmfiles: 3 - path: \u0026#34;releases/kubernetes-dashboard/helmfile.yaml\u0026#34; 4 values: 5 - releases/kubernetes-dashboard/dev.yaml 6 - path: \u0026#34;releases/reloader/helmfile.yaml\u0026#34; 7 values: 8 - installed: True 9EOF Now we can see what changes will be applied.\n1helmfile diff 2Adding repo stable https://charts.helm.sh/stable 3\u0026#34;stable\u0026#34; has been added to your repositories 4 5Comparing release=kubernetes-dashboard, chart=stable/kubernetes-dashboard 6******************** 7 8 Release was not present in Helm. Diff will show entire contents as new. 9 10‚Ä¶ The command helm sync will install the releases\n1helmfile sync 2Adding repo stable https://charts.helm.sh/stable 3\u0026#34;stable\u0026#34; has been added to your repositories 4 5Affected releases are: 6 kubernetes-dashboard (stable/kubernetes-dashboard) UPDATED 7 8Upgrading release=kubernetes-dashboard, chart=stable/kubernetes-dashboard 9Release \u0026#34;kubernetes-dashboard\u0026#34; does not exist. Installing it now. 10NAME: kubernetes-dashboard 11... You can list all the releases managed by the local helmfile.\n1helmfile list 2NAME NAMESPACE ENABLED LABELS 3kubernetes-dashboard kube-system true chart:kubernetes-dashboard,component:monitoring,namespace:kube-system,repo:stable,vendor:kubernetes 4reloader reloader true chart:stakater/reloader,component:reloader,namespace:reloader,repo:stakater,vendor:stakater Delete all the releases\n1helmfile delete 2Listing releases matching ^reloader$ 3reloader reloader 1 2021-02-16 10:10:35.378800455 +0100 CET deployed reloader-v0.0.68 v0.0.68 4 5Deleting reloader 6release \u0026#34;reloader\u0026#34; uninstalled ‚û°Ô∏è Next: Build a Helm chart\n","link":"https://blog.ogenki.io/post/series/workshop_helm/ecosystem/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Ecosystem"},{"body":"Looking for a chart Helm works with what is called a ‚Äúchart‚Äù. A chart is basically a package of yaml resources that support a templating language.\nBefore building our own chart we should always have a look of what is available in the community. There are often charts that fits our needs.\nThese charts can be installed from different sources: a Helm chart repository, a local archive or chart directory.\nFirst of all, let‚Äôs say that we want to install a Wordpress instance on an empty infrastructure.\nWe‚Äôll need to provision a database as well as the Wordpress application.\nLet‚Äôs look for a wordpress chart !\nIf you just installed Helm, your repositories list should be empty\n1helm repo list We‚Äôre going to check what are the Wordpress charts available in the artifacthub.\nYou can either browse from the web page or use the command\n1helm search hub wordpress 2URL CHART VERSION APP VERSION DESCRIPTION 3https://artifacthub.io/packages/helm/bitnami/wo... 10.6.4 5.6.1 Web publishing platform for building blogs and ... 4https://artifacthub.io/packages/helm/groundhog2... 0.2.6 5.6.0-apache A Helm chart for Wordpress on Kubernetes 5https://artifacthub.io/packages/helm/seccurecod... 2.4.0 4.0 Insecure \u0026amp; Outdated Wordpress Instance: Never e... 6https://artifacthub.io/packages/helm/presslabs/... 0.10.5 0.10.5 Presslabs WordPress Operator Helm Chart Using the Hub there are a few things that can help to choose the best option.\nFirst of all the number of stars obviously and whether the artifact comes from a verified publisher or signed by the maintainer.\nWe‚Äôll get the one provided by Bitnami. In the chart page you‚Äôll be guided with the commands to add Bitnami‚Äôs repository.\n1helm repo add bitnami https://charts.bitnami.com/bitnami 2\u0026#34;bitnami\u0026#34; has been added to your repositories 3 4helm repo update From now on we can install all the charts published by Bitnami:\n1helm search repo bitnami 2NAME CHART VERSION APP VERSION DESCRIPTION 3bitnami/bitnami-common 0.0.9 0.0.9 DEPRECATED Chart with custom templates used in ... 4bitnami/airflow 8.0.3 2.0.1 Apache Airflow is a platform to programmaticall... 5bitnami/apache 8.2.3 2.4.46 Chart for Apache HTTP Server 6bitnami/aspnet-core 1.2.3 3.1.9 ASP.NET Core is an open-source framework create... 7bitnami/cassandra 7.3.2 3.11.10 Apache Cassandra is a free and open-source dist... Inspect the chart OK let‚Äôs get back to what we want to achieve: Installing a Wordpress instance.\nNow that we identified the chart, we‚Äôre going to check what it actually does. You should always check what will be installed.\nyou can download the chart on your laptop and have a look to its content 1helm pull --untar bitnami/wordpress 2 3tree -L 2 wordpress/ 4wordpress/ 5‚îú‚îÄ‚îÄ Chart.lock 6‚îú‚îÄ‚îÄ charts 7‚îÇ ‚îú‚îÄ‚îÄ common 8‚îÇ ‚îî‚îÄ‚îÄ mariadb 9‚îú‚îÄ‚îÄ Chart.yaml 10‚îú‚îÄ‚îÄ ci 11‚îÇ ‚îú‚îÄ‚îÄ ct-values.yaml 12‚îÇ ‚îú‚îÄ‚îÄ ingress-wildcard-values.yaml 13‚îÇ ‚îú‚îÄ‚îÄ values-hpa-pdb.yaml 14‚îÇ ‚îî‚îÄ‚îÄ values-metrics-and-ingress.yaml 15‚îú‚îÄ‚îÄ README.md 16‚îú‚îÄ‚îÄ templates 17‚îÇ ‚îú‚îÄ‚îÄ configmap.yaml 18‚Ä¶ 19‚îÇ ‚îú‚îÄ‚îÄ tests 20‚îÇ ‚îî‚îÄ‚îÄ tls-secrets.yaml 21‚îú‚îÄ‚îÄ values.schema.json 22‚îî‚îÄ‚îÄ values.yaml read carefully the readme check what are the dependencies pulled by this chart 1helm show chart bitnami/wordpress 2annotations: 3 category: CMS 4apiVersion: v2 5appVersion: 5.6.1 6dependencies: 7- condition: mariadb.enabled 8 name: mariadb 9 repository: https://charts.bitnami.com/bitnami 10 version: 9.x.x 11- name: common 12 repository: https://charts.bitnami.com/bitnami 13 tags: 14 - bitnami-common 15 version: 1.x.x 16... Note: that the wordpress chart defines the mariadb chart as dependency\nLook at the available values 1helm show values bitnami/wordpress Our first release Our next step will be to set our desired values. Indeed you mentioned that Helm uses a templating language to render the manifests. This will help us to configure our instance according to our environment.\nno persistency at all, this is just a workshop 2 replicas for the wordpress instance a database named ‚Äúfoodb‚Äù an owner ‚Äúfoobar‚Äù for this database passwords All the charts have a file named values.yaml that contains the default values.\nThese values can be overridden at the command line with --set or we can put them in a yaml file that we‚Äôll use with the -f parameter.\nFor this exercise we‚Äôll create a file named ‚Äúoverride-values.yaml‚Äù and we‚Äôll use the command line for sensitive information.\n1wordpressUsername: foobar 2wordpressPassword: \u0026#34;\u0026#34; 3wordpressBlogName: Foo\u0026#39;s Blog! 4replicaCount: 2 5persistence: 6 enabled: false 7service: 8 type: ClusterIP 9mariadb: 10 auth: 11 rootPassword: \u0026#34;\u0026#34; 12 database: foodb 13 username: foobar 14 password: \u0026#34;\u0026#34; 15 primary: 16 persistence: 17 enabled: false Note: In order to define the values of a subchart you must put the chart name as the first key. here mariadb.values of the mariadb chart.\nHere we go!\nFirst of all we‚Äôll run it in dry-run mode in order to check the yaml rendering (be careful, the passwords are printed in plain text)\n1helm install foo-blog bitnami/wordpress \\ 2-f override-values.yaml \\ 3--set mariadb.auth.rootPassword=r00tP4ss \\ 4--set mariadb.auth.password=us3rP4ss \\ 5--set wordpressPassword=azerty123 \\ 6--dry-run Another word you need to know is Release.\n‚ÄúA Release is an instance of a chart running in a Kubernetes cluster‚Äù. Our release name here is foo-blog\nIf the output looks OK we can install our wordpress, just remove the --dry-run parameter\n1helm install foo-blog bitnami/wordpress -f override-values.yaml --set mariadb.auth.rootPassword=\u0026#34;r00tP4ss\u0026#34; --set mariadb.auth.password=\u0026#34;us3rP4ss\u0026#34; --set wordpressPassword=\u0026#34;azerty123\u0026#34; 2NAME: foo-blog 3LAST DEPLOYED: Fri Feb 12 16:33:21 2021 4NAMESPACE: default 5STATUS: deployed 6REVISION: 1 7NOTES: 8** Please be patient while the chart is being deployed ** 9 10Your WordPress site can be accessed through the following DNS name from within your cluster: 11 12 foo-blog-wordpress.default.svc.cluster.local (port 80) 13 14To access your WordPress site from outside the cluster follow the steps below: 15 161. Get the WordPress URL by running these commands: 17 18 NOTE: It may take a few minutes for the LoadBalancer IP to be available. 19 Watch the status with: \u0026#39;kubectl get svc --namespace default -w foo-blog-wordpress\u0026#39; 20 21 export SERVICE_IP=$(kubectl get svc --namespace default foo-blog-wordpress --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\u0026#34;) 22 echo \u0026#34;WordPress URL: http://$SERVICE_IP/\u0026#34; 23 echo \u0026#34;WordPress Admin URL: http://$SERVICE_IP/admin\u0026#34; 24 252. Open a browser and access WordPress using the obtained URL. 26 273. Login with the following credentials below to see your blog: 28 29 echo Username: foobar 30 echo Password: $(kubectl get secret --namespace default foo-blog-wordpress -o jsonpath=\u0026#34;{.data.wordpress-password}\u0026#34; | base64 --decode) When the release has been successfully installed you‚Äôll get the above ‚ÄúNOTES‚Äù that are very useful to get access to your application. You just have to copy/paste.\nBut first of all we‚Äôre going to check that the pods are actually running\n1kubectl get deploy,sts 2NAME READY UP-TO-DATE AVAILABLE AGE 3deployment.apps/foo-blog-wordpress 2/2 2 2 55m 4 5NAME READY AGEkubectl get deploy,sts 6statefulset.apps/foo-blog-mariadb 1/1 55m We didn‚Äôt define an ingress for the purpose of the workshop, therefore we‚Äôll use a port-forward\n1kubectl port-forward svc/foo-blog-wordpress 9090:80 Then open a browser using the URL http://localhost:9090/admin, you‚Äôll be prompted to fill in the credentials you defined above. (wordpressPassword)\nWe‚Äôll check the database credentials too as follows\n1MARIADB=$(kubectl get po -l app.kubernetes.io/name=mariadb -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 1kubectl exec -ti ${MARIADB} -- bash -c \u0026#39;mysql -u foobar -pus3rP4ss\u0026#39; 2Welcome to the MariaDB monitor. Commands end with ; or \\g. 3Your MariaDB connection id is 372 4Server version: 10.5.8-MariaDB Source distribution 5 6Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. 7 8Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. 9 10MariaDB [(none)]\u0026gt; SHOW GRANTS; 11+-------------------------------------------------------------------------------------------------------+ 12| Grants for foobar@% | 13+-------------------------------------------------------------------------------------------------------+ 14| GRANT USAGE ON *.* TO `foobar`@`%` IDENTIFIED BY PASSWORD \u0026#39;*CD5BE357349BDA710A444B0BD741E8EB12B8BC2C\u0026#39; | 15| GRANT ALL PRIVILEGES ON `foodb`.* TO `foobar`@`%` | 16+-------------------------------------------------------------------------------------------------------+ 172 rows in set (0.000 sec) Delete the wordpress release\n1helm uninstall foo-blog Deploy a complete monitoring stack with a single command! The purpose of this step is to show that, even if the stack is composed of dozens of manifest, Helm makes things easy.\n1helm repo add prometheus-community https://prometheus-community.github.io/helm-charts 2\u0026#34;prometheus-community\u0026#34; has been added to your repositories 3 4helm repo update 1helm install kube-prometheus prometheus-community/kube-prometheus-stack --create-namespace --namespace monitoring 2NAME: kube-prometheus 3LAST DEPLOYED: Fri Feb 12 18:03:05 2021 4NAMESPACE: monitoring 5STATUS: deployed 6REVISION: 1 7NOTES: 8kube-prometheus-stack has been installed. Check its status by running: 9 kubectl --namespace monitoring get pods -l \u0026#34;release=kube-prometheus\u0026#34; Check that all the pods are running and run a port-forward\n1kubectl port-forward -n monitoring svc/kube-prometheus-grafana 9090:80 Then open a browser using the URL http://localhost:9090/admin\ndefault credentials: admin / prom-operator\nYou should browse a few minutes over all the dashboards available. There is pretty useful info.\nYou can then have a look to the resources that have been applied with a single command line as follows\n1helm get manifest -n monitoring kube-prometheus Well for a production ready prometheus we would have played a bit with the values but you get the point.\nDelete the kube-prometheus stack\n1helm uninstall -n monitoring kube-prometheus ‚û°Ô∏è Next: Helm ecosystem\n","link":"https://blog.ogenki.io/post/series/workshop_helm/third_party/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop: Third party charts"},{"body":"Requirements docker k3d \u0026gt;5.x.x helm \u0026gt;3.x.x helmfile In order to have an easily provisioned temporary playground we‚Äôll make use of k3d which is a lightweight local Kubernetes instance.\nAfter installing the binary you should enable the completion (bash or zsh) as follows (do the same for both helm and k3d).\n1source \u0026lt;(k3d completion bash) Then create the sandbox cluster named ‚Äúhelm-workshop‚Äù\n1k3d cluster create helm-workshop 2INFO[0000] Created network \u0026#39;k3d-helm-workshop\u0026#39; 3INFO[0000] Created volume \u0026#39;k3d-helm-workshop-images\u0026#39; 4INFO[0001] Creating node \u0026#39;k3d-helm-workshop-server-0\u0026#39; 5INFO[0006] Creating LoadBalancer \u0026#39;k3d-helm-workshop-serverlb\u0026#39; 6INFO[0007] (Optional) Trying to get IP of the docker host and inject it into the cluster as \u0026#39;host.k3d.internal\u0026#39; for easy access 7INFO[0010] Successfully added host record to /etc/hosts in 2/2 nodes and to the CoreDNS ConfigMap 8INFO[0010] Cluster \u0026#39;helm-workshop\u0026#39; created successfully! 9INFO[0010] You can now use it like this: 10kubectl cluster-info Note that your current configuration should be automatically switched to the newly created cluster.\n1$ kubectl config current-context 2k3d-helm-workshop Playing with third party charts Environment and ecosystem Build your first chart Application lifecycle Templating challenge Other considerations Hosting and versioning Most of the time we would want to share the charts in order to be used on different systems or to pull the dependencies.\nThere are multiple options for that, here are the ones that are generally used.\nChartmuseum is the official solution. This is a pretty simple webserver that exposes a Rest API. Harbor. Its main purpose is to store images (containers), but it offers many other features such as vulnerability scanning, images signing and integrates chartmuseum. Artifactory can be used to stored Helm charts too An OCI store (container registry). Pushing the charts into a central location requires to manage the versions of the charts. Any changes should trigger a version bump in the Chart.yaml file.\nSecrets management One sensitive topic that we didn‚Äôt talk about is how to handle secrets.\nThis is not directly related to Helm but this is a general issue on Kubernetes.\nThere are many options, some of them work great with Helm, some others require managing secrets apart from Helm releases.\nIn the ArgoCD documentation they tried to reference all the options available.\nCleanup Pretty simple we‚Äôll drop the whole k3d cluster\n1k3d cluster delete helm-workshop ","link":"https://blog.ogenki.io/post/series/workshop_helm/intro/","section":"post","tags":["Helm","Kubernetes"],"title":"Helm workshop"},{"body":"","link":"https://blog.ogenki.io/categories/containers/","section":"categories","tags":null,"title":"containers"},{"body":"RBAC is the method used by Kubernetes to authorize access to API resources.\n‚ÑπÔ∏è When it makes sense you can use the default roles that are available in all Kubernetes installation instead of having to maintain custom ones.\nFor this lab what we want to achieve is to give permissions the following permissions to an application myapp:\nread the configmaps in the namespace foo List the pods in all the namespaces It is a good practice to configure your pod to make use of a serviceaccount. A serviceaccount are used to identify applications and give them permissions if necessary.\nCreate a service account\n1kubectl create serviceaccount myapp 2serviceaccount/myapp created When a service account is created, a token is automatically generated and stored in a secret.\n1kubectl describe sa myapp | grep -i token 2Mountable secrets: myapp-token-bz2zq 3Tokens: myapp-token-bz2zq 4 5kubectl get secret myapp-token-bz2zq --template={{.data.token}} | base64 -d 6eyJhb...EYxhjI_ckZ74A Using a tool to decode the JWT token you should see the following content\n1kubectl get secret myapp-token-bz2zq --template={{.data.token}} | base64 -d | jwt decode - 2 3Token header 4------------ 5{ 6 \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, 7 \u0026#34;kid\u0026#34;: \u0026#34;IdsXYO6E93xozgJg-LY2oETTPEHBJjydTU4vF2wy-wg\u0026#34; 8} 9 10Token claims 11------------ 12{ 13 \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, 14 \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;foo\u0026#34;, 15 \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;myapp-token-bz2zq\u0026#34;, 16 \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;myapp\u0026#34;, 17 \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;eb606bdc-b713-4b7c-8da8-c4f71075995e\u0026#34;, 18 \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:foo:myapp\u0026#34; 19} We're going to create a deployment that will be configured to used this serviceaccount. In the yaml you'll notice that we defined the serviceAccountName.\n1kubectl apply -f content/resources/kubernetes_workshop/rbac/deployment.yaml 2deployment.apps/myapp created As we didn't assigned any permissions to this serviceaccount, our application won't be able to call any of the API endpoints\n1POD_NAME=$(kubectl get po -l app=myapp -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 2 3kubectl exec ${POD_NAME} -- kubectl auth can-i -n foo --list 4Resources Non-Resource URLs Resource Names Verbs 5selfsubjectaccessreviews.authorization.k8s.io [] [] [create] 6selfsubjectrulesreviews.authorization.k8s.io [] [] [create] 7 [/.well-known/openid-configuration] [] [get] 8 [/api/*] [] [get] 9 [/api] [] [get] 10 [/apis/*] [] [get] 11 [/apis] [] [get] 12 [/healthz] [] [get] 13 [/healthz] [] [get] 14 [/livez] [] [get] 15 [/livez] [] [get] 16 [/openapi/*] [] [get] 17 [/openapi] [] [get] 18 [/openid/v1/jwks] [] [get] 19 [/readyz] [] [get] 20 [/readyz] [] [get] 21 [/version/] [] [get] 22 [/version/] [] [get] 23 [/version] [] [get] 24 [/version] [] [get] In order to allow it to read configmaps in the namespace foo, we're going to create 2 resources:\nA role which will describe the permissions and which is bounded to a namespace A rolebinding to assign this role to our application (serviceaccount) Create the role\n1kubectl apply -f content/resources/kubernetes_workshop/rbac/role.yaml 2role.rbac.authorization.k8s.io/read-configmaps created And assign it to the serviceaccount we've created previously\n1kubectl create rolebinding -n foo myapp-configmap --serviceaccount=foo:myapp --role=read-configmaps 2rolebinding.rbac.authorization.k8s.io/myapp-configmap created Note that in the above command the serviceaccount must be specified with the namespace as a prefix and separated by a semicolon.\nYou don't have to restart the pod to get the permissions enabled.\n1kubectl exec ${POD_NAME} -- kubectl auth can-i get configmaps -n foo 2yes 3 4kubectl exec ${POD_NAME} -- kubectl get cm 5NAME DATA AGE 6kube-root-ca.crt 1 3d20h 7helloworld 2 2d2h This is possible thanks to the token mounted within the container\n1kubectl exec -ti ${POD_NAME} -- bash -c \u0026#39;curl -skH \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; https://kubernetes.default/api/v1/namespaces/foo/configmaps\u0026#39; 2{ 3 \u0026#34;kind\u0026#34;: \u0026#34;ConfigMapList\u0026#34;, 4 \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, 5 \u0026#34;metadata\u0026#34;: { 6 \u0026#34;resourceVersion\u0026#34;: \u0026#34;76334\u0026#34; 7 }, 8 \u0026#34;items\u0026#34;: [ 9 { 10 \u0026#34;metadata\u0026#34;: { 11 \u0026#34;name\u0026#34;: \u0026#34;kube-root-ca.crt\u0026#34;, 12 \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, 13 \u0026#34;uid\u0026#34;: \u0026#34;c352e4cd-3b88-4400-80a0-cbba318794e4\u0026#34;, 14... Finally we want to list the pods in all the namespaces of our cluster. we need:\nA clusterrole which will describe the permissions that are cluster wide. A clusterrolebinding to assign this clusterrole to our application (serviceaccount) 1$ kubectl apply -f content/resources/kubernetes_workshop/rbac/clusterrole.yaml 2clusterrole.rbac.authorization.k8s.io/list-pods created 1kubectl create clusterrolebinding -n foo myapp-pods --serviceaccount=foo:myapp --clusterrole=list-pods 2clusterrolebinding.rbac.authorization.k8s.io/myapp-pods created Now lets have a look to the permissions our applications has in the namespace foo\n1kubectl exec ${POD_NAME} -- kubectl auth can-i -n foo --list 2Resources Non-Resource URLs Resource Names Verbs 3selfsubjectaccessreviews.authorization.k8s.io [] [] [create] 4selfsubjectrulesreviews.authorization.k8s.io [] [] [create] 5pods [] [] [get list] 6configmaps [] [] [get watch list] 7 [/.well-known/openid-configuration] [] [get] 8 [/api/*] [] [get] 9 [/api] [] [get] 10 [/apis/*] [] [get] 11... ","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/rbac/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Manage permissions in Kubernetes"},{"body":"","link":"https://blog.ogenki.io/series/workshop-kubernetes/","section":"series","tags":null,"title":"Workshop Kubernetes"},{"body":"Events The first source of information when something goes wrong is the event stream. Note that you may want to sort them by creation time\n1kubectl get events -n foo --sort-by=.metadata.creationTimestamp 2... 320m Normal Created pod/web-85575f4476-5pbqv Created container nginx 420m Normal Started pod/web-85575f4476-5pbqv Started container nginx 520m Normal SuccessfulDelete replicaset/web-987f6cf9 Deleted pod: web-987f6cf9-mzsxd 620m Normal ScalingReplicaSet deployment/web Scaled down replica set web-987f6cf9 to 0 Logs Having a look to a pod's logs is just the matter of running\n1kubectl logs -f --tail=7 -c mysql wordpress-mysql-6c597b98bd-4mbbd 22021-06-24 08:27:38 1 [Note] - \u0026#39;::\u0026#39; resolves to \u0026#39;::\u0026#39;; 32021-06-24 08:27:38 1 [Note] Server socket created on IP: \u0026#39;::\u0026#39;. 42021-06-24 08:27:38 1 [Warning] Insecure configuration for --pid-file: Location \u0026#39;/var/run/mysqld\u0026#39; in the path is accessible to all OS users. Consider choosing a different directory. 52021-06-24 08:27:38 1 [Warning] \u0026#39;proxies_priv\u0026#39; entry \u0026#39;@ root@wordpress-mysql-6c597b98bd-4mbbd\u0026#39; ignored in --skip-name-resolve mode. 62021-06-24 08:27:38 1 [Note] Event Scheduler: Loaded 0 events 72021-06-24 08:27:38 1 [Note] mysqld: ready for connections. 8Version: \u0026#39;5.6.51\u0026#39; socket: \u0026#39;/var/run/mysqld/mysqld.sock\u0026#39; port: 3306 MySQL Community Server (GPL) Alternatively you can use a tool made to display logs from multiple pods: stern. A better way to explore logs is to send them to a central location using a tool such as Loki or the well know EFK stack.\nHealth checks Kubernetes self healing system is mostly based on health checks. There are different types of health checks (please have a look to the official documentation).\nWe'll add a new plugin to kubectl which is really useful to export a resource while cleaning useless metadatas: neat\n1kubectl krew install neat 2Updated the local copy of plugin index. 3Installing plugin: neat 4Installed plugin: neat 5... Let's create a new deployment using the image nginx\n1kubectl create deploy web --image=nginx --dry-run=client -o yaml | kubectl neat \u0026gt; /tmp/web.yaml Edit its content and add an HTTP health check on port 80. The endpoint must return a code ranging between 200 and 400 and it has to be a relevant test that shows the actual availability of the service.\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 labels: 5 app: web 6 name: web 7spec: 8 replicas: 1 9 selector: 10 matchLabels: 11 app: web 12 template: 13 metadata: 14 creationTimestamp: null 15 labels: 16 app: web 17 spec: 18 containers: 19 - image: nginx 20 name: nginx 21 livenessProbe: 22 httpGet: 23 path: / 24 port: 80 25 initialDelaySeconds: 3 26 periodSeconds: 3 1kubectl apply -f /tmp/web.yaml 2deployment.apps/web created 3 4kubectl describe deploy web | grep Liveness: 5 Liveness: http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #failure=3 The pod should be up without any error\n1kubectl get po -l app=web 2NAME READY STATUS RESTARTS AGE 3web-85575f4476-6qvd5 1/1 Running 0 92s We're going to simulate a service being unavailable, just change the path being checked. Here we'll use another method to modify a resource by creating a patch and applying it.\nCreate a yaml /tmp/patch.yaml file\n1cat \u0026gt; /tmp/patch.yaml \u0026lt;\u0026lt;EOF 2spec: 3 template: 4 spec: 5 containers: 6 - name: nginx 7 livenessProbe: 8 httpGet: 9 path: /foobar 10EOF And we're going to apply our change as follows\n1kubectl patch deployment web --patch \u0026#34;$(cat /tmp/patch.yaml)\u0026#34; --record 2deployment.apps/web patched 3 4kubectl describe deployment web | grep Liveness: 5 Liveness: http-get http://:80/foobar delay=3s timeout=1s period=3s #success=1 #failure=3 Now our pod should start to fail, the number of restarts increases\n1kubectl get po -l app=web 2web-987f6cf9-n4rnb 1/1 Running 4 83s Until the pod enter in a CrashLoopBackOff, meaning that it constantly restarts.\n1kubectl get po -l app=web 2NAME READY STATUS RESTARTS AGE 3web-987f6cf9-n4rnb 0/1 CrashLoopBackOff 5 3m23s Describing the pod will give you a hint on the reason it restarts\n1kubectl describe po web-987f6cf9-n4rnb | tail -n 5 2Normal Created 4m7s (x3 over 4m30s) kubelet Created container nginx 3Normal Started 4m7s (x3 over 4m30s) kubelet Started container nginx 4Warning Unhealthy 3m56s (x9 over 4m26s) kubelet Liveness probe failed: HTTP probe failed with statuscode: 404 5Normal Killing 3m56s (x3 over 4m20s) kubelet Container nginx failed liveness probe, will be restarted 6Normal Pulling 3m56s (x4 over 4m35s) kubelet Pulling image \u0026#34;nginx\u0026#34; Rollback the latest change in order to return to a working state. Note that we used the option --record when we applied the patch. That helps saving changes history.\n1kubectl rollout history deployment web 2deployment.apps/web 3REVISION CHANGE-CAUSE 41 \u0026lt;none\u0026gt; 52 kubectl patch deployment web --patch=spec: 6 template: 7 spec: 8 containers: 9 - name: nginx 10 livenessProbe: 11 httpGet: 12 path: /foobar --record=true 13 14kubectl rollout undo deployment web 15deployment.apps/web rolled back Cleanup 1kubectl delete deploy web 2deployment.apps \u0026#34;web\u0026#34; deleted learnk8s documentation There is a great documentation that contains all the steps that help debugging a deployment: https://learnk8s.io/troubleshooting-deployments\n‚û°Ô∏è Next: RBAC\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/troubleshoot/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Troubleshooting"},{"body":"Resources allocation in Kubernetes Resources allocation in Kubernetes is made using requests and limits in the container's definition.\nrequests: What the container is guaranteed to get. These values are used when the scheduler takes a decision on where (what node) to place a given pod. limits: Are values that cannot be exceeded ‚ÑπÔ∏è You can use explain to have a look to the documentation of resources.\n1kubectl explain --recursive pod.spec.containers.resources.limits 2KIND: Pod 3VERSION: v1 4 5FIELD: limits \u0026lt;map[string]string\u0026gt; 6 7DESCRIPTION: 8 Limits describes the maximum amount of compute resources allowed. More 9... The wordpress we've created in the previous lab doesn't have resources definition. There are different ways to edit its current state (kubectl edit, apply, patch ...)\n1kubectl edit deploy wordpress replace resources: {} with this block\n1... 2 resources: 3 requests: 4 cpu: 100m 5 memory: 100Mi 6 limits: 7 cpu: 1000m 8 memory: 200Mi 9... The pods resources usage can be displayed using (this might take a few seconds)\n1kubectl top pods 2NAME CPU(cores) MEMORY(bytes) 3wordpress-694866c6b7-mqxdd 1m 171Mi 4wordpress-mysql-6c597b98bd-4mbbd 1m 531Mi Configure the autoscaling base on cpu usage. When a pod reaches 50% of its allocated cpu a new pod is created.\n1kubectl autoscale deployment wordpress --cpu-percent=50 --min=1 --max=5 2horizontalpodautoscaler.autoscaling/wordpress autoscaled It takes up to 15 seconds (default configuration) to get the first values\n1kubectl get hpa 2NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 3wordpress Deployment/wordpress \u0026lt;unknown\u0026gt;/50% 1 5 0 10s 4 5kubectl get hpa 6NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 7wordpress Deployment/wordpress 1%/50% 1 5 1 20s Now we'll run an HTTP bench using wrk. Open a new shell and run\n1kubectl run -ti --rm bench --image=jess/wrk -- /bin/sh -c \u0026#39;wrk -t12 -c100 -d180s http://wordpress\u0026#39; During the benchmark above (3 minutes duration) let's have a look to the hpa\n1watch kubectl get hpa 2Every 2.0s: kubectl get hpa 3hostname: Tue Jun 22 11:13:08 2021 4 5NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 6wordpress Deployment/wordpress 1%/50% 1 5 1 8m28s After a few seconds we'll see that the upscaling will be done automatically. Here the number of replicas will reach the maximum we defined (5 pods).\n1Every 2.0s: kubectl get hpa 2hostname: Tue Jun 22 11:14:13 2021 3 4NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE 5wordpress Deployment/wordpress 998%/50% 1 5 5 9m33s That was a pretty simple configuration, basing the autoscaling on CPU usage for a webserver makes sense. You can also base the autoscaling on any other metrics that are reported by your application.\n‚û°Ô∏è Next: Troubleshooting\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/autoscaling/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Resources allocation and autoscaling"},{"body":"‚ÑπÔ∏è This section is, for a most part, based on the official Kubernetes doc.\nBy the end of this lab we'll create the following components. You may want to come back to this schema from time to time in order to get the whole picture.\nA database with a persistent volume Check that your cluster is up and running and that your context is still configured with the namespace foo\n1kubectl config get-contexts 2CURRENT NAME CLUSTER AUTHINFO NAMESPACE 3* k3d-workshop k3d-workshop admin@k3d-workshop foo Create a persistent volume claim There are several options when it comes to persistent workloads on Kubernetes. For this workshop we'll use our local disks thanks to the local path provisionner.\nCreate a persistentVolumeClaim, it will stay pending until a pod consumes it\n1kubectl apply -f content/resources/kubernetes_workshop/mysql/pvc.yaml 2persistentvolumeclaim/local-path-mysql created 3 4kubectl get pvc 5NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 6local-path-mysql Pending local-path 16s Create the MySQL secret In Kubernetes sensitive data are stored in Secrets. Here we'll create a secret that stores the MySQL root password\n1kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD 2secret/mysql-pass created Note that a secret is stored in an base64 encoded format and can be easily decoded. (There are best practices to enforce safe access to the secrets that we're not going to cover there)\n1kubectl get secrets mysql-pass -o yaml 2apiVersion: v1 3data: 4 password: WU9VUl9QQVNTV09SRA== 5kind: Secret 6metadata: 7 creationTimestamp: \u0026#34;2021-06-20T09:11:59Z\u0026#34; 8 name: mysql-pass 9 namespace: foo 10 resourceVersion: \u0026#34;2809\u0026#34; 11 uid: c96c58d6-8472-4d68-8554-5dcfb69d834c 12type: Opaque 13 14echo -n \u0026#34;WU9VUl9QQVNTV09SRA==\u0026#34; | base64 -d 15YOUR_PASSWORD Run the MySQL deployment We will now create a MySQL deployment. It will be composed of a single replica as we're accessing to a local volume and it is configured to make use of the secret we've created previously.\n1kubectl apply -f content/resources/kubernetes_workshop/mysql/deployment.yaml 2deployment.apps/wordpress-mysql created 3 4kubectl get po -w 5NAME READY STATUS RESTARTS AGE 6wordpress-mysql-6c597b98bd-vcm62 0/1 ContainerCreating 0 9s 7wordpress-mysql-6c597b98bd-vcm62 1/1 Running 0 13s 8^C Service discovery in Kubernetes In order to be able to call our MySQL deployment we may want to expose it using a service.\n1kubectl apply -f content/resources/kubernetes_workshop/mysql/svc.yaml 2service/wordpress-mysql created 3 4kubectl get svc 5NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 6wordpress-mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 6s Kubernetes's service discovery is based on an internal DNS system. For instance a service A service is accessible using the following nomenclature: \u0026lt;service_name\u0026gt;.\u0026lt;Namespace\u0026gt;.svc.\u0026lt;Cluster_domain_name\u0026gt;\nLet's try to access to the database server using a mysql client pod and create a database named foobar\n1kubectl run -ti --rm mysql-client --restart=Never --image=mysql:5.7 -- /bin/bash 2If you don\u0026#39;t see a command prompt, try pressing enter. 3root@mysql-client:/# apt -qq update \u0026amp;\u0026amp; apt install -yq netcat 4... 5Setting up netcat (1.10-41.1) ... 6 7 8root@mysql-client:/# nc -vz wordpress-mysql.foo.svc.cluster.local 3306 9DNS fwd/rev mismatch: wordpress-mysql.foo.svc.cluster.local != 10-42-1-8.wordpress-mysql.foo.svc.cluster.local 10wordpress-mysql.foo.svc.cluster.local [10.42.1.8] 3306 (?) open 11 12root@mysql-client:/# mysql -u root -h wordpress-mysql -p 13Enter password: 14... 15 16mysql\u0026gt; show databases; 17+--------------------+ 18| Database | 19+--------------------+ 20| information_schema | 21| mysql | 22| performance_schema | 23+--------------------+ 243 rows in set (0.01 sec) 25 26mysql\u0026gt; create database foobar; 27Query OK, 1 row affected (0.00 sec) 28 29mysql\u0026gt; exit 30Bye Note: You can either use the service name wordpress-mysql, or if your source pod is in another namespace use wordpress-mysql.foo\nCheck how the data is persisted with the local-path-provisioner We may want to check how the data is stored. Now that we have a MySQL instance running and consuming the pvc, a persistent volume has been provision\n1kubectl get pvc 2NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 3local-path-mysql Bound pvc-4bb3c033-2261-4d5c-ba61-41e364769599 500Mi RWO local-path 14m Having a closer look we notice that the volume is actually a directory within a worker node.\n1kubectl describe pv pvc-4bb3c033-2261-4d5c-ba61-41e364769599 2Name: pvc-4bb3c033-2261-4d5c-ba61-41e364769599 3Labels: \u0026lt;none\u0026gt; 4Annotations: pv.kubernetes.io/provisioned-by: rancher.io/local-path 5Finalizers: [kubernetes.io/pv-protection] 6StorageClass: local-path 7Status: Bound 8Claim: foo/local-path-mysql 9Reclaim Policy: Delete 10Access Modes: RWO 11VolumeMode: Filesystem 12Capacity: 500Mi 13Node Affinity: 14 Required Terms: 15 Term 0: kubernetes.io/hostname in [k3d-workshop-agent-0] 16Message: 17Source: 18 Type: HostPath (bare host directory volume) 19 Path: /var/lib/rancher/k3s/storage/pvc-4bb3c033-2261-4d5c-ba61-41e364769599_foo_local-path-mysql 20 HostPathType: DirectoryOrCreate 21Events: \u0026lt;none\u0026gt; 1docker exec k3d-workshop-agent-0 ls /var/lib/rancher/k3s/storage/pvc-4bb3c033-2261-4d5c-ba61-41e364769599_foo_local-path-mysql 2auto.cnf 3foobar 4ib_logfile0 5ib_logfile1 6ibdata1 7mysql 8performance_schema That means that even if you restart your laptop you should retrieve the data (here the database foobar we've created previously)\n1k3d cluster stop workshop 2INFO[0000] Stopping cluster \u0026#39;workshop\u0026#39; 3 4k3d cluster list 5NAME SERVERS AGENTS LOADBALANCER 6workshop 0/1 0/1 true 7 8k3d cluster start workshop 9INFO[0000] Starting cluster \u0026#39;workshop\u0026#39; 10INFO[0000] Starting servers... 11INFO[0000] Starting Node \u0026#39;k3d-workshop-server-0\u0026#39; 12INFO[0006] Starting agents... 13INFO[0006] Starting Node \u0026#39;k3d-workshop-agent-0\u0026#39; 14INFO[0013] Starting helpers... 15INFO[0013] Starting Node \u0026#39;k3d-workshop-serverlb\u0026#39; 16 17kubectl run -ti --rm mysql-client --restart=Never --image=mysql:5.7 -- mysql -u root -h wordpress-mysql --password=\u0026#34;YOUR_PASSWORD\u0026#34; 18If you don\u0026#39;t see a command prompt, try pressing enter. 19 20mysql\u0026gt; show databases; 21+--------------------+ 22| Database | 23+--------------------+ 24| information_schema | 25| foobar | 26| mysql | 27| performance_schema | 28+--------------------+ 294 rows in set (0.00 sec) The Wordpress deployment Now we will deploy the wordpress instance with a persistent volume.\nSo first of all create a pvc as follows\n1kubectl apply -f content/resources/kubernetes_workshop/wordpress/pvc.yaml 2persistentvolumeclaim/wp-pv-claim created Then create the deployment. Note that it is configured with our mysql database as backend.\n1kubectl apply -f content/resources/kubernetes_workshop/wordpress/deployment.yaml 2deployment.apps/wordpress created 3 4$ kubectl get deploy 5NAME READY UP-TO-DATE AVAILABLE AGE 6wordpress-mysql 1/1 1 1 11h 7wordpress 1/1 1 1 4s Most of the time, when we want to expose an HTTP service to the outside world (outside of the cluster), we would create an ingress\n1kubectl apply -f content/resources/kubernetes_workshop/wordpress/svc.yaml 2service/wordpress created 3 4kubectl apply -f content/resources/kubernetes_workshop/wordpress/ingress.yaml 5ingress.networking.k8s.io/wordpress created With k3d the ingress endpoint has been defined when we've created the cluster. With the parameter -p \u0026quot;8081:80@loadbalancer\u0026quot; Our wordpress should therefore be accessible through http://localhost:8081\nConfigure your pods A ConfigMap is a kubernetes resource that stores non-sensitive data. Its content can be consumed as config files, environment variables or command args.\nLet's consider that we need a configfile to be mounted in our wordpress deployment as well as an environment variable made available.\nCreate a dumb \u0026quot;hello world\u0026quot; config file\n1echo \u0026#34;Hello World!\u0026#34; \u0026gt; /tmp/helloworld.conf Then we'll create a configmap that contains a file and environment variable we want to make use of. Note This following command doesn't actually apply the resource on our Kubernetes cluster. It just generate a local yaml file using --dry-run and -o yaml.\n1kubectl create configmap helloworld --from-file=/tmp/helloworld.conf --from-literal=HELLO=WORLD -o yaml --dry-run=client \u0026gt; /tmp/cm.yaml Check the configmap\n1apiVersion: v1 2data: 3 HELLO: WORLD 4 helloworld.conf: | 5 Hello World! 6kind: ConfigMap 7metadata: 8 creationTimestamp: null 9 name: helloworld And apply it\n1$ kubectl apply -f /tmp/cm.yaml 2configmap/helloworld created Now we're gonna make use of it by changing the wordpress deployment. For this kind of change it is recommended to use an IDE with a Kubernetes plugin that will highlight errors.\nEdit the file located here: content/resources/kubernetes_workshop/wordpress/deployment.yaml\n1... 2 env: 3 - name: WORDPRESS_DB_HOST 4 value: wordpress-mysql 5 - name: WORDPRESS_DB_PASSWORD 6 valueFrom: 7 secretKeyRef: 8 name: mysql-pass 9 key: password 10 - name: HELLO 11 valueFrom: 12 configMapKeyRef: 13 name: helloworld 14 key: HELLO 15 volumeMounts: 16 - name: wordpress-persistent-storage 17 mountPath: /var/www/html 18 - name: helloworld-config 19 mountPath: /config 20 volumes: 21 - name: wordpress-persistent-storage 22 persistentVolumeClaim: 23 claimName: wp-pv-claim 24 - name: helloworld-config 25 configMap: 26 name: helloworld 27 items: 28 - key: helloworld.conf 29 path: helloworld.conf Applying this change will trigger a rolling-update\n1$ kubectl apply -f content/resources/kubernetes_workshop/wordpress/deployment.yaml 2deployment.apps/wordpress configured 3 4$ kubectl get po 5NAME READY STATUS RESTARTS AGE 6wordpress-mysql-6c597b98bd-4mbbd 1/1 Running 2 41h 7wordpress-594f88c9c4-n9qqr 1/1 Running 0 5s And the configuration will be available in the newly created pod\n1$ kubectl exec -ti wordpress-594f88c9c4-n9qqr -- env | grep HELLO 2HELLO=WORLD 3 4$ kubectl exec -ti wordpress-594f88c9c4-n9qqr -- cat /config/helloworld.conf 5Hello World! ‚ö†Ô∏è Do not delete anything, we'll make use of these resources in the next section.\n‚û°Ô∏è Next: Resources in Kubernetes\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/application_stack/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Complete application stack"},{"body":"Prepare your local Kubernetes environment Goal: Having a running local Kubernetes environment\nEnsure that you fulfilled the requirements\nUsing k3d to create a cluster In order to have an easily provisioned temporary playground we‚Äôll make use of k3d which is a lightweight local Kubernetes instance. (Note that they are alternatives to run a local Kubernetes cluster such as: kubeadm, microk8s, minikube)\nAfter installing the binary you should enable the completion (bash or zsh) as follows (do the same for both kubectl and k3d).\n1source \u0026lt;(k3d completion bash) Then create the sandbox cluster named \u0026quot;workshop\u0026quot; with an additional worker\n1k3d cluster create workshop -p \u0026#34;8081:80@loadbalancer\u0026#34; --agents 1 2INFO[0000] Prep: Network 3INFO[0000] Created network \u0026#39;k3d-workshop\u0026#39; (ce74508d3fe09d8622f1ae83effd412d754dfdb441aa9d550723805f9b528c6b) 4INFO[0000] Created volume \u0026#39;k3d-workshop-images\u0026#39; 5INFO[0001] Creating node \u0026#39;k3d-workshop-server-0\u0026#39; 6INFO[0001] Creating node \u0026#39;k3d-workshop-agent-0\u0026#39; 7INFO[0001] Creating LoadBalancer \u0026#39;k3d-workshop-serverlb\u0026#39; 8INFO[0001] Starting cluster \u0026#39;workshop\u0026#39; 9INFO[0001] Starting servers... 10INFO[0001] Starting Node \u0026#39;k3d-workshop-server-0\u0026#39; 11INFO[0006] Starting agents... 12INFO[0006] Starting Node \u0026#39;k3d-workshop-agent-0\u0026#39; 13INFO[0018] Starting helpers... 14INFO[0018] Starting Node \u0026#39;k3d-workshop-serverlb\u0026#39; 15INFO[0019] (Optional) Trying to get IP of the docker host and inject it into the cluster as \u0026#39;host.k3d.internal\u0026#39; for easy access 16INFO[0023] Successfully added host record to /etc/hosts in 3/3 nodes and to the CoreDNS ConfigMap 17INFO[0023] Cluster \u0026#39;workshop\u0026#39; created successfully! 18INFO[0023] --kubeconfig-update-default=false --\u0026gt; sets --kubeconfig-switch-context=false 19INFO[0023] You can now use it like this: 20kubectl config use-context k3d-workshop 21kubectl cluster-info As k3d is made to be used on top of docker you can see the status of the running containers. You should have 3 containers, one for the loadbalancing, one for the control-plane and an agent (worker).\n1docker ps 2CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 34b5847b265dd rancher/k3d-proxy:v4.4.6 \u0026#34;/bin/sh -c nginx-pr‚Ä¶\u0026#34; About a minute ago Up About a minute 80/tcp, 0.0.0.0:43903-\u0026gt;6443/tcp k3d-workshop-serverlb 4523a025087b3 rancher/k3s:v1.21.1-k3s1 \u0026#34;/bin/entrypoint.sh ‚Ä¶\u0026#34; About a minute ago Up About a minute k3d-workshop-agent-0 5791b8a69bc1f rancher/k3s:v1.21.1-k3s1 \u0026#34;/bin/entrypoint.sh ‚Ä¶\u0026#34; About a minute ago Up About a minute k3d-workshop-server-0 With kubectl you'll see 2 running pods: a control-plane and a worker\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-workshop-agent-0 Ready \u0026lt;none\u0026gt; 2m34s v1.21.1+k3s1 4k3d-workshop-server-0 Ready control-plane,master 2m44s v1.21.1+k3s1 You can also have a look to the default cluster's components that are all located in the namespace kube-system\n1kubectl get pods -n kube-system 2NAME READY STATUS RESTARTS AGE 3helm-install-traefik-crd-h5j7m 0/1 Completed 0 16h 4helm-install-traefik-8mzhk 0/1 Completed 0 16h 5svclb-traefik-gh4rk 2/2 Running 2 16h 6traefik-97b44b794-lcmh4 1/1 Running 1 16h 7coredns-7448499f4d-h7xvn 1/1 Running 2 16h 8local-path-provisioner-5ff76fc89d-qvpf7 1/1 Running 1 16h 9svclb-traefik-cbvmp 2/2 Running 2 16h 10metrics-server-86cbb8457f-5v9ls 1/1 Running 1 16h The CLI configuration The main interface to the Kubernetes API is kubectl. This CLI is configured with what we call a kubeconfig you can have a look at its content wether by having a look at its default location is ~/.kube/config or running the command\n1kubectl config view 2apiVersion: v1 3clusters: 4- cluster: 5 certificate-authority-data: DATA+OMITTED 6 server: https://0.0.... and you can check if the CLI is properly configured by running\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:43903 3CoreDNS is running at https://0.0.0.0:43903/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:43903/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy Kubectl plugins It is really easy to extend the capabilities of he kubectl CLI. Here is a basic \u0026quot;hello-world\u0026quot; example:\nWrite a dumb script, just ensure its name is prefixed with kubectl- and put it in your PATH\n1cat \u0026gt; kubectl-helloworld\u0026lt;\u0026lt;EOF 2#!/bin/bash 3echo \u0026#34;Hello world!\u0026#34; 4EOF 5 6chmod u+x kubectl-helloworld \u0026amp;\u0026amp; sudo mv kubectl-helloworld /usr/local/bin Then it can be used as an argument of kubectl\n1kubectl helloworld 2Hello world! Delete our test\n1sudo rm /usr/local/bin/kubectl-helloworld You can find more information on how to create a kubectl plugin here\nIn order to benefit from the plugins written by the community there's a tool named krew\nUpdate the local index\n1kubectl krew update 2Adding \u0026#34;default\u0026#34; plugin index from https://github.com/kubernetes-sigs/krew-index.git. 3Updated the local copy of plugin index. Browse the available plugins\n1kubectl krew search 2NAME DESCRIPTION INSTALLED 3access-matrix Show an RBAC access matrix for server resources no 4advise-psp Suggests PodSecurityPolicies for cluster. no 5allctx Run commands on contexts in your kubeconfig no 6apparmor-manager Manage AppArmor profiles for cluster. no 7... For the current workshop we'll make use of ctx ns\nctx: Switch between contexts in your kubeconfig (Really helpful when you have multiple clusters to manage) ns: Switch between Kubernetes namespaces (Avoid to specify the namespace for each kubectl commands when working on a given namespace) 1kubectl krew install ctx ns 2Updated the local copy of plugin index. 3Installing plugin: ctx 4... Then you'll be able to switch between contexts (clusters) and namespaces.\n1kubectl ns 2Context \u0026#34;k3d-workshop\u0026#34; modified. 3Active namespace is \u0026#34;kube-system\u0026#34;. ‚û°Ô∏è Next: Run an application on Kubernetes\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/local/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop: Local environment"},{"body":"Namespaces Namespaces allow to logically distribute your applications, generally based on teams, projects or applications stacks. Resources names are unique within a namespace. That means that you could have a service named webserver on 2 different namespaces.\nThey can be used to isolate applications using network policies, or to define quotas.\nFor this training we'll work on a namespace named foo\n1kubectl create ns foo 2namespace/foo created And we'll make use of the plugin ns installed in the previous section to set the default namespace as follows\n1kubectl ns 2Context \u0026#34;k3d-workshop\u0026#34; modified. 3Active namespace is \u0026#34;foo\u0026#34;. Check that your kubectl is properly configured, here you can see the cluter and the namespace:\n1kubectl config get-contexts 2CURRENT NAME CLUSTER AUTHINFO NAMESPACE 3* k3d-workshop k3d-workshop admin@k3d-workshop foo Create your first pod Creating resources in Kubernetes is often done by applying a yaml/json definition through the API.\nFirst of all we need to clone this repository and change the current path to its root\n1git clone https://github.com/Smana/workshop_kubernetes_2021.git 2 3cd workshop_kubernetes_2021.git Start by creating a pretty simple pod:\n1kubectl apply -f content/resources/kubernetes_workshop/pod.yaml --namespace foo 2pod/web created 3 4kubectl get po 5NAME READY STATUS RESTARTS AGE 6web 1/1 Running 0 98s We can get detailed information about the pod as follows\n1kubectl describe po web 2Name: web 3Namespace: foo 4Priority: 0 5Node: k3d-workshop-agent-0/172.20.0.3 6Start Time: Fri, 18 Jun 2021 17:05:46 +0200 7Labels: run=web 8Annotations: \u0026lt;none\u0026gt; 9Status: Running 10IP: 10.42.1.6 11... Or even get a specific attribute, here is an example to get the pod's IP\n1kubectl get po web --template={{.status.podIP}} 210.42.1.6 This is worth noting that a pod isn't controlled by a replicaset-controller. That means that when it is deleted, it is not restarted automatically.\n1kubectl delete po web 2pod \u0026#34;web\u0026#34; deleted 3 4kubectl get po 5No resources found in foo namespace. A pod with 2 containers Now create a new pod using the manifest content/resources/kubernetes_workshop/pod2containers.yaml. Look at its content, we will be using a shared temporary directory and we'll mount its content on both containers. That way we can share data between 2 containers of a given pod.\n1kubectl apply -f content/resources/kubernetes_workshop/pod2containers.yaml 2pod/web created 3 4kubectl get pod 5NAME READY STATUS RESTARTS AGE 6web 2/2 Running 0 36s We can check that the logs are accessible on the 2 containers\n1kubectl logs web -c logger --tail=6 -f 2Mon Jun 28 21:06:20 2021 3Mon Jun 28 21:06:21 2021 4Mon Jun 28 21:06:22 2021 5Mon Jun 28 21:06:23 2021 6Mon Jun 28 21:06:24 2021 7 8kubectl exec web -c web -- tail -n 5 /log/out.log 9Mon Jun 28 21:07:19 2021 10Mon Jun 28 21:07:20 2021 11Mon Jun 28 21:07:21 2021 12Mon Jun 28 21:07:22 2021 13Mon Jun 28 21:07:23 2021 Delete the pod\n1kubectl delete po web 2pod \u0026#34;web\u0026#34; deleted Create a simple webserver deployment A deployment is a resource that describes the desired state of an application. Kubernetes will ensure that its current status is aligned with the desired one.\nCreating a simple deployment can be done using kubectl\n1kubectl create deployment podinfo --image stefanprodan/podinfo 2deployment.apps/podinfo created After a few seconds the deployment will be up to date, meaning that the a pod is up and running.\n1kubectl get deploy 2NAME READY UP-TO-DATE AVAILABLE AGE 3podinfo 1/1 1 1 14s Replicas and scaling A deployment creates a replicaset under the hood in order to ensure that the number of replicas (pods) matches the desired one.\n1kubectl get replicasets 2NAME DESIRED CURRENT READY AGE 3podinfo-7fbb45ccfc 1 1 1 36s Creating a deployment without specifying the number of replicas will create a single replica. We can scale it on demand using\n1kubectl scale deploy podinfo --replicas 6 2deployment.apps/podinfo scaled 3 4kubectl rollout status deployment podinfo 5Waiting for deployment \u0026#34;podinfo\u0026#34; rollout to finish: 4 of 6 updated replicas are available... 6Waiting for deployment \u0026#34;podinfo\u0026#34; rollout to finish: 5 of 6 updated replicas are available... 7deployment \u0026#34;podinfo\u0026#34; successfully rolled out The default Kubernetes scheduler will try to spread evenly the pods according to the available resources on worker nodes.\n1kubectl get po -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3podinfo-7fbb45ccfc-dwxtx 1/1 Running 0 114s 10.42.1.8 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4podinfo-7fbb45ccfc-p2djv 1/1 Running 0 34s 10.42.1.11 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5podinfo-7fbb45ccfc-4fk9z 1/1 Running 0 34s 10.42.1.9 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6podinfo-7fbb45ccfc-gqwz6 1/1 Running 0 34s 10.42.1.10 k3d-workshop-agent-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7podinfo-7fbb45ccfc-4qgvs 1/1 Running 0 34s 10.42.0.8 k3d-workshop-server-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8podinfo-7fbb45ccfc-r6dn5 1/1 Running 0 34s 10.42.0.9 k3d-workshop-server-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The deployment controller will ensure to start new pods if the number of replicas doesn't match its configuration.\n1kubectl delete po $(kubectl get po -l app=podinfo -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 2pod \u0026#34;podinfo-7fbb45ccfc-r6dn5\u0026#34; deleted 3 4kubectl describe rs podinfo-7fbb45ccfc 5Name: podinfo-7fbb45ccfc 6Namespace: foo 7Selector: app=podinfo,pod-template-hash=7fbb45ccfc 8Labels: app=podinfo 9 pod-template-hash=7fbb45ccfc 10Annotations: deployment.kubernetes.io/desired-replicas: 6 11 deployment.kubernetes.io/max-replicas: 8 12 deployment.kubernetes.io/revision: 5 13 deployment.kubernetes.io/revision-history: 1,3 14Controlled By: Deployment/podinfo 15Replicas: 6 current / 6 desired 16Pods Status: 6 Running / 0 Waiting / 0 Succeeded / 0 Failed 17... 18Events: 19 Type Reason Age From Message 20 ---- ------ ---- ---- ------- 21... 22 Normal SuccessfulCreate 16h (x3 over 16h) replicaset-controller (combined from similar events): Created pod: podinfo-7fbb45ccfc-pkt4r 23 Normal SuccessfulCreate 96s replicaset-controller Created pod: podinfo-7fbb45ccfc-bkm8n 24 25kubectl get deploy 26NAME READY UP-TO-DATE AVAILABLE AGE 27podinfo 6/6 6 6 18m Rolling update Using a deployment allows to manage the application lifecycle. Changing its configuration will trigger a rolling update.\nFirst of all we'll change the image tag of our deployment\n1kubectl set image deployment podinfo podinfo=stefanprodan/podinfo:5.2.1 2deployment.apps/podinfo image updated During a rolling update a new replicaset is created in order to update the application in place without any downtime. New pods (with the current deployment state) will be created in the new replicaset while they will be deleted progressively from the previous replicaset.\n1kubectl get rs -o wide 2NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR 3podinfo-7fbb45ccfc 0 0 0 21m podinfo stefanprodan/podinfo app=podinfo,pod-template-hash=7fbb45ccfc 4podinfo-564b4ddd7c 6 6 6 30s podinfo stefanprodan/podinfo:5.2.1 app=podinfo,pod-template-hash=564b4ddd7c Keeping the old replicaset makes very easy to rollback\n1kubectl rollout undo deployment podinfo 2deployment.apps/podinfo rolled back 3 4kubectl get rs -o wide 5NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR 6podinfo-7fbb45ccfc 6 6 6 22m podinfo stefanprodan/podinfo app=podinfo,pod-template-hash=7fbb45ccfc 7podinfo-564b4ddd7c 0 0 0 77s podinfo stefanprodan/podinfo:5.2.1 app=podinfo,pod-template-hash=564b4ddd7c Expose a deployment Now that we have a running web application we may want to access it. There are several ways to expose an app, here we'll use the easiest way: Create a service and run a port-forward.\nThe following command will create a service which will be in charge of forwarding calls through the tcp port 9898\n1kubectl expose deploy podinfo --port 9898 2service/podinfo exposed We can get more information on the service as follows\n1kubectl get svc -o yaml podinfo 2apiVersion: v1 3kind: Service 4metadata: 5 labels: 6 app: podinfo 7 name: podinfo 8 namespace: foo 9spec: 10 clusterIP: 10.43.47.17 11 clusterIPs: 12 - 10.43.47.17 13 ipFamilies: 14 - IPv4 15 ipFamilyPolicy: SingleStack 16 ports: 17 - port: 9898 18 selector: 19 app: podinfo A service uses the selector above to identify on which pod to forward the traffic and usually creates the endpoints accordingly.\n1kubectl get po -l app=podinfo 2NAME READY STATUS RESTARTS AGE 3podinfo-7fbb45ccfc-bkm8n 1/1 Running 1 146m 4podinfo-7fbb45ccfc-sbqht 1/1 Running 2 18h 5... 6 7kubectl get endpoints 8NAME ENDPOINTS AGE 9podinfo 10.42.0.16:9898,10.42.0.17:9898,10.42.1.18:9898 + 3 more... 92s The service we've created has an IP that's only accessible from within the cluster. Using the port-forward command we're able to forward the traffic from our local machine to the application (through the API server). Note that you can target either a deployment, a service or a single pod\n1 2kubectl port-forward svc/podinfo 9898 \u0026amp; 3Forwarding from 127.0.0.1:9898 -\u0026gt; 9898 4Forwarding from [::1]:9898 -\u0026gt; 9898 5 6curl http://localhost:9898 7Handling connection for 9898 8{ 9 \u0026#34;hostname\u0026#34;: \u0026#34;podinfo-7fbb45ccfc-sbqht\u0026#34;, 10 \u0026#34;version\u0026#34;: \u0026#34;6.0.0\u0026#34;, 11 \u0026#34;revision\u0026#34;: \u0026#34;\u0026#34;, 12 \u0026#34;color\u0026#34;: \u0026#34;#34577c\u0026#34;, 13 \u0026#34;logo\u0026#34;: \u0026#34;https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif\u0026#34;, 14 \u0026#34;message\u0026#34;: \u0026#34;greetings from podinfo v6.0.0\u0026#34;, 15 \u0026#34;goos\u0026#34;: \u0026#34;linux\u0026#34;, 16 \u0026#34;goarch\u0026#34;: \u0026#34;amd64\u0026#34;, 17 \u0026#34;runtime\u0026#34;: \u0026#34;go1.16.5\u0026#34;, 18 \u0026#34;num_goroutine\u0026#34;: \u0026#34;6\u0026#34;, 19 \u0026#34;num_cpu\u0026#34;: \u0026#34;16\u0026#34; 20} Cleanup In this section we created 2 resources: a deployment and a service.\n1fg 2kubectl port-forward svc/podinfo 9898 3^C 4 5kubectl delete svc,deploy podinfo 6service \u0026#34;podinfo\u0026#34; deleted 7deployment.apps \u0026#34;podinfo\u0026#34; deleted ‚û°Ô∏è Next: Deploy a Wordpress\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/run_app/","section":"post","tags":["Kubernetes"],"title":"Run an application on Kubernetes"},{"body":"This repository aims to quickly learn the basics of Kubernetes.\n‚ö†Ô∏è None of the examples given here are made for production.\nRequirements docker k3d \u0026gt;5.x.x kubectl krew (optional)fzf Agenda Prepare your local Kubernetes environment Run an application on Kubernetes Deploy a Wordpress Resources and autoscaling Troubleshooting RBAC Cleanup Pretty simple we‚Äôll drop the whole k3d cluster\n1k3d cluster delete workshop 2INFO[0000] Deleting cluster \u0026#39;workshop\u0026#39; 3... 4INFO[0008] Successfully deleted cluster workshop! ‚û°Ô∏è You may want to continue with the Helm workshop\n","link":"https://blog.ogenki.io/post/series/workshop_kubernetes/intro/","section":"post","tags":["Kubernetes"],"title":"Kubernetes workshop"},{"body":"","link":"https://blog.ogenki.io/tags/index/","section":"tags","tags":null,"title":"index"}]