[{"body":"","link":"https://blog.ogenki.io/fr/","section":"","tags":null,"title":""},{"body":"Terraform est probablement l'outil \u0026quot;Infrastructure As Code\u0026quot; le plus utilis√© pour construire, modifier et versionner les changements d'infrastructure Cloud. Il s'agit d'un projet Open Source d√©velopp√© par Hashicorp et qui utilise le langage HCL pour d√©clarer l'√©tat souhait√© de ressources Cloud. L'√©tat des ressources cr√©√©es est stock√© dans un fichier d'√©tat (terraform state).\nOn peut consid√©rer que Terraform est un outil \u0026quot;semi-d√©claratif\u0026quot; car il n'y a pas de fonctionnalit√© de r√©conciliation automatique int√©gr√©e. Il existe diff√©rentes approches pour r√©pondre √† cette probl√©matique, mais en r√®gle g√©n√©rale, une modification sera appliqu√©e en utilisant terraform apply. Le code est bien d√©crit dans des fichiers de configuration HCL (d√©claratif) mais l'ex√©cution est faite de mani√®re imp√©rative. De ce fait, il peut y avoir de la d√©rive entre l'√©tat d√©clar√© et le r√©el (par exemple, un coll√®gue qui serait pass√© sur la console pour changer un param√®tre üòâ).\n‚ùì‚ùì Alors, comment m'assurer que ce qui est commit dans mon repo git est vraiment appliqu√©. Comment √™tre alert√© s'il y a un changement par rapport √† l'√©tat d√©sir√© et comment appliquer automatiquement ce qui est dans mon code (GitOps) ?\nC'est la promesse de tf-controller, un operateur Kubernetes Open Source de Weaveworks, √©troitement li√© √† Flux (un moteur GitOps de la m√™me soci√©t√©). Flux est l'une des solutions que je pl√©biscite, et je vous invite donc √† lire un pr√©c√©dent article.\nInfo L'ensemble des √©tapes d√©crites ci-dessous sont faites avec ce repo Git\nüéØ Notre objectif En suivant les √©tapes de cet article nous visons les objectifs suivant:\nD√©ployer un cluster Kubernetes qui servira de \u0026quot;Control plane\u0026quot;. Pour r√©sumer il h√©bergera le controlleur Terraform qui nous permettra de d√©clarer tous les √©l√©ments d'infrastructure souhait√©s. Utiliser Flux comme moteur GitOps pour toutes les ressources Kubernetes. Concernant le controleur Terraform, nous allons voir:\nQuelle est le moyen de d√©finir des d√©pendances entre modules Cr√©ation de plusieurs ressources AWS: Zone route53, Certificat ACM, r√©seau, cluster EKS. Les diff√©rentes options de reconciliation (automatique, n√©cessitant une confirmation) Comment sauvegarder et restaurer un fichier d'√©tat (tfstate) üõ†Ô∏è Installer le controleur Terraform ‚ò∏ Le cluster \u0026quot;Control Plane\u0026quot; Afin de pouvoir utiliser le controleur Kubernetes tf-controller, il nous faut d'abord un cluster Kubernetes üòÜ. Nous allons donc cr√©er un cluster control plane en utilisant la ligne de commande terraform et les bonnes pratiques EKS.\nWarning Il est primordial que ce cluster soit r√©siliant, s√©curis√© et supervis√© car il sera responsable de la gestion de l'ensemble des ressources AWS cr√©√©es par la suite.\nSans entrer dans le d√©tail, le cluster \u0026quot;control plane\u0026quot; a √©t√© cr√©√© un utilisant ce code. Cel√†-dit, il est important de noter que toutes les op√©rations de d√©ploiement d'application se font en utilisant Flux.\nInfo En suivant les instructions du README, un cluster EKS sera cr√©√© mais pas uniquement! Il faut en effet donner les permissions au controlleur Terraform pour appliquer les changements d'infrastructure. De plus, Flux doit √™tre install√© et configur√© afin d'appliquer la configuration d√©finie ici.\nAu final on se retrouve donc avec plusieurs √©l√©ments install√©s et configur√©s:\nles addons quasi indispensables que sont aws-loadbalancer-controller et external-dns les roles IRSA pour ces m√™mes composants sont install√©s en utilisant tf-controller La stack de supervision Prometheus / Grafana. external-secrets pour pouvoir r√©cup√©rer des √©l√©ments sensibles depuis AWS secretsmanager. Afin de d√©montrer tout cela au bout de quelques minutes l'interface web pour Flux est accessible via l'URL gitops-\u0026lt;cluster_name\u0026gt;.\u0026lt;domain_name\u0026gt; V√©rifier toute de m√™me que le cluster est accessible et que Flux fonctionne correctement\n1aws eks update-kubeconfig --name controlplane-0 --alias controlplane-0 2Updated context controlplane-0 in /home/smana/.kube/config 1flux check 2... 3‚úî all checks passed 4 5flux get kustomizations 6NAME REVISION SUSPENDED READY MESSAGE 7flux-config main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 8flux-system main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 9infrastructure main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 10security main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 11tf-controller main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 12... üì¶ Le chart Helm et Flux Maintenant que notre cluster \u0026quot;controlplane\u0026quot; est op√©rationnel, l'ajout le contr√¥leur Terraform consiste √† utiliser le chart Helm.\nIl faut tout d'abord d√©clarer la source:\nsource.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: tf-controller 5spec: 6 interval: 30m 7 url: https://weaveworks.github.io/tf-controller Et d√©finir la HelmRelease:\nrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: tf-controller 5spec: 6 releaseName: tf-controller 7 chart: 8 spec: 9 chart: tf-controller 10 sourceRef: 11 kind: HelmRepository 12 name: tf-controller 13 namespace: flux-system 14 version: \u0026#34;0.12.0\u0026#34; 15 interval: 10m0s 16 install: 17 remediation: 18 retries: 3 19 values: 20 resources: 21 limits: 22 memory: 1Gi 23 requests: 24 cpu: 200m 25 memory: 500Mi 26 runner: 27 serviceAccount: 28 annotations: 29 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/tfcontroller_${cluster_name}\u0026#34; Lorsque ce changement est √©crit dans le repo Git, la HelmRelease sera d√©ploy√©e et le contr√¥lleur tf-controller d√©marera\n1kubectl get hr -n flux-system 2NAME AGE READY STATUS 3tf-controller 67m True Release reconciliation succeeded 4 5kubectl get po -n flux-system -l app.kubernetes.io/instance=tf-controller 6NAME READY STATUS RESTARTS AGE 7tf-controller-7ffdc69b54-c2brg 1/1 Running 0 2m6s Dans le repo de demo il y a d√©j√† un certain nombre de ressources AWS d√©clar√©es. Par cons√©quent, au bout de quelques minutes, le cluster se charge de la cr√©ation de celles-cis: Info Bien que la majorit√© des t√¢ches puisse √™tre r√©alis√©e de mani√®re d√©clarative ou via les utilitaires de ligne de commande tels que kubectl et flux, un autre outil existe qui offre la possibilit√© d'interagir avec les ressources terraform : tfctl\nüöÄ Appliquer un changement Parmis les bonnes pratiques avec Terraform, il y a l'usage de modules. Un module est un ensemble de ressources Terraform li√©es logigement afin d'obtenir une seule unit√© r√©utilisable. Cela permet d'abstraire la complexit√©, de prendre des entr√©es, effectuer des actions sp√©cifiques et produire des sorties.\nIl est possible de cr√©er ses propres modules et de les mettre √† disposition dans des Sources ou d'utiliser les nombreux modules partag√©s et maintenus par les communaut√©s. Il suffit alors d'indiquer quelques variables afin de l'adapter au contexte.\nAvec tf-controller, la premi√®re √©tape consiste donc √† indiquer la Source du module. Ici nous allons configurer le socle r√©seau sur AWS (vpc, subnets...) avec le module terraform-aws-vpc.\nsources/terraform-aws-vpc.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1 2kind: GitRepository 3metadata: 4 name: terraform-aws-vpc 5 namespace: flux-system 6spec: 7 interval: 30s 8 ref: 9 tag: v5.0.0 10 url: https://github.com/terraform-aws-modules/terraform-aws-vpc Nous pouvons ensuite cr√©er la ressource Terraform qui en fait usage:\nvpc/dev.yaml\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6 interval: 8m 7 path: . 8 destroyResourcesOnDeletion: true # You wouldn\u0026#39;t do that on a prod env ;) 9 storeReadablePlan: human 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-vpc 13 namespace: flux-system 14 vars: 15 - name: name 16 value: vpc-dev 17 - name: cidr 18 value: \u0026#34;10.42.0.0/16\u0026#34; 19 - name: azs 20 value: 21 - \u0026#34;eu-west-3a\u0026#34; 22 - \u0026#34;eu-west-3b\u0026#34; 23 - \u0026#34;eu-west-3c\u0026#34; 24 - name: private_subnets 25 value: 26 - \u0026#34;10.42.0.0/19\u0026#34; 27 - \u0026#34;10.42.32.0/19\u0026#34; 28 - \u0026#34;10.42.64.0/19\u0026#34; 29 - name: public_subnets 30 value: 31 - \u0026#34;10.42.96.0/24\u0026#34; 32 - \u0026#34;10.42.97.0/24\u0026#34; 33 - \u0026#34;10.42.98.0/24\u0026#34; 34 - name: enable_nat_gateway 35 value: true 36 - name: single_nat_gateway 37 value: true 38 - name: private_subnet_tags 39 value: 40 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 41 \u0026#34;karpenter.sh/discovery\u0026#34;: dev 42 - name: public_subnet_tags 43 value: 44 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 45 writeOutputsToSecret: 46 name: vpc-dev Si l'on devait r√©sumer grossi√®rement: le code terraform provenant de la source terraform-aws-vpc est utilis√© avec les variables vars.\nIl y a ensuite plusieurs param√®tres qui influent sur le fonctionnement de tf-controller. Les principaux param√®tres qui permettent de contr√¥ler la fa√ßon dont sont appliqu√©es les modifications sont .spec.approvePlan et .spec.autoApprove\nüö® D√©tection de la d√©rive D√©finir spec.approvePlan avec une valeur √† disable permet uniquement de notifier que l'√©tat actuel des ressources a d√©riv√© par rapport au code Terraform. Cela permet notamment de choisir le moment et la mani√®re dont l'application des changements sera effectu√©e.\nNote De mon point de vue il manque une section sur les notifications: La d√©rive, les plans en attentes, les probl√®mese de r√©concilation. J'essaye d'identifier les m√©thodes possibles (de pr√©f√©rence avec Prometheus) et de mettre √† jour cet article d√®s que possible.\nüîß Application manuelle L'exemple donn√© pr√©c√©demment (vpc-dev) ne contient pas le param√®tre .spec.approvePlan et h√©rite donc de la valeur par d√©faut qui est false. Par cons√©quent, l'application concr√®te des modifications (apply), n'est pas faite automatiquement.\nUn plan est ex√©cut√© et sera en attente d'une validation:\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system vpc-dev Unknown Plan generated: set approvePlan: \u0026#34;plan-v5.0.0-26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. true 2 minutes Je conseille d'ailleurs de configurer le param√®tre storeReadablePlan √† human. Cela permet de visualiser simplement les modifications en attente en utilisant tfctl:\n1tfctl show plan vpc-dev 2 3Terraform used the selected providers to generate the following execution 4plan. ressource actions are indicated with the following symbols: 5 + create 6 7Terraform will perform the following actions: 8 9 # aws_default_network_acl.this[0] will be created 10 + ressource \u0026#34;aws_default_network_acl\u0026#34; \u0026#34;this\u0026#34; { 11 + arn = (known after apply) 12 + default_network_acl_id = (known after apply) 13 + id = (known after apply) 14 + owner_id = (known after apply) 15 + tags = { 16 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 17 } 18 + tags_all = { 19 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 20 } 21 + vpc_id = (known after apply) 22 23 + egress { 24 + action = \u0026#34;allow\u0026#34; 25 + from_port = 0 26 + ipv6_cidr_block = \u0026#34;::/0\u0026#34; 27 + protocol = \u0026#34;-1\u0026#34; 28 + rule_no = 101 29 + to_port = 0 30 } 31 + egress { 32... 33Plan generated: set approvePlan: \u0026#34;plan-v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. 34To set the field, you can also run: 35 36 tfctl approve vpc-dev -f filename.yaml Apr√®s revue des modifications ci-dessus, il suffit donc d'ajouter l'identifiant du plan √† valider et de pousser le changement sur git comme suit:\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6... 7 approvePlan: plan-v5.0.0-26c38a66f1 8... En quelques instants un runner sera lanc√© qui se chargera d'appliquer les changements:\n1kubectl logs -f -n flux-system vpc-dev-tf-runner 22023/07/01 15:33:36 Starting the runner... version sha 3... 4aws_vpc.this[0]: Creating... 5aws_vpc.this[0]: Still creating... [10s elapsed] 6... 7aws_route_table_association.private[1]: Creation complete after 0s [id=rtbassoc-01b7347a7e9960a13] 8aws_nat_gateway.this[0]: Still creating... [10s elapsed] La r√©conciliation √©ffectu√©e, la ressource passe √† l'√©tat READY: True\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m ü§ñ Application automatique Nous pouvons aussi activer la r√©conciliation automatique. Pour ce faire il faut d√©clarer le param√®tre .spec.autoApprove √† true.\nToutes les ressources IRSA sont configur√©es de la sorte:\nexternal-secrets.yaml\n1piVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: irsa-external-secrets 5spec: 6 approvePlan: auto 7 destroyResourcesOnDeletion: true 8 interval: 8m 9 path: ./modules/iam-role-for-service-accounts-eks 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-iam 13 namespace: flux-system 14 vars: 15 - name: role_name 16 value: ${cluster_name}-external-secrets 17 - name: attach_external_secrets_policy 18 value: true 19 - name: oidc_providers 20 value: 21 main: 22 provider_arn: ${oidc_provider_arn} 23 namespace_service_accounts: [\u0026#34;security:external-secrets\u0026#34;] Donc si je fais le moindre changement sur la console AWS par exemple, celui-ci sera rapidement √©cras√© par celui g√©r√© par tf-controller.\nüîÑ Entr√©es et sorties: d√©pendances entre modules Lorsque qu'on utilise Terraform, on a souvent besoin de passer des donn√©es d'un module √† l'autre. G√©n√©ralement ce sont les outputs du module qui exportent ces informations. Il faut donc un moyen de les importer dans un autre module.\nReprenons encore l'exemple donn√© ci-dessus (vpc-dev). Nous notons en bas du YAML la directive suivante:\n1... 2 writeOutputsToSecret: 3 name: vpc-dev Lorsque cette ressource est appliqu√©e nous aurons un message qui confirme que les outputs sont disponibles (\u0026quot;Outputs written\u0026quot;):\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m En effet ce module exporte de nombreuses informations (126):\n1kubectl get secrets -n flux-system vpc-dev 2NAME TYPE DATA AGE 3vpc-dev Opaque 126 15s 4 5kubectl get secret -n flux-system vpc-dev --template=\u0026#39;{{.data.vpc_id}}\u0026#39; | base64 -d 6vpc-0c06a6d153b8cc4db Certains de ces √©l√©ments d'informations sont ensuite utilis√©s pour cr√©er un cluster EKS de dev:\nvpc/dev.yaml\n1... 2 varsFrom: 3 - kind: Secret 4 name: vpc-dev 5 varsKeys: 6 - vpc_id 7 - private_subnets 8... üíæ Sauvegarder et restaurer un tfstate Dans mon cas je ne souhaite pas recr√©er la zone et le certificat √† chaque destruction du controlplane. Voici un exemple des √©tapes √† mener pour que je puisse restaurer l'√©tat de ces ressources lorsque j'utilise cette demo.\nInfo La politique de suppression d'une ressource Terraform est d√©finie par le param√®tre destroyResourcesOnDeletion. Par d√©faut elles sont conserv√©es et il faut donc que ce param√®tre ait pour valeur true afin de d√©truire les √©l√©ments cr√©es lorsque l'objet Kubernetes est supprim√©.\nNote Il s'agit l√† d'une proc√©dure manuelle afin de d√©montrer le comportement de tf-controller par rapport aux fichiers d'√©tat. Par d√©faut ces tfstates sont stock√©s dans des secrets mais on pr√©ferera configurer un backend GCS ou S3\nLa cr√©ation initiale de l'environnement de d√©mo m'a permis de sauvegarder les fichiers d'√©tat (tfstate) de cette fa√ßon.\n1WORKSPACE=\u0026#34;default\u0026#34; 2STACK=\u0026#34;route53-cloud-hostedzone\u0026#34; 3BACKUPDIR=\u0026#34;${HOME}/tf-controller-backup\u0026#34; 4 5mkdir -p ${BACKUPDIR} 6 7kubectl get secrets -n flux-system tfstate-${WORKSPACE}-${STACK} -o jsonpath=\u0026#39;{.data.tfstate}\u0026#39; | \\ 8base64 -d | gzip -d \u0026gt; ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate Lorsque le cluster est cr√©√© √† nouveau, tf-controller essaye de cr√©er la zone car le fichier d'√©tat est vide.\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system route53-cloud-hostedzone Unknown Plan generated: set approvePlan: \u0026#34;plan-main@sha1:345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. true 16 minutes 5 6tfctl show plan route53-cloud-hostedzone 7 8Terraform used the selected providers to generate the following execution 9plan. resource actions are indicated with the following symbols: 10 + create 11 12Terraform will perform the following actions: 13 14 # aws_route53_zone.this will be created 15 + resource \u0026#34;aws_route53_zone\u0026#34; \u0026#34;this\u0026#34; { 16 + arn = (known after apply) 17 + comment = \u0026#34;Experimentations for blog.ogenki.io\u0026#34; 18 + force_destroy = false 19 + id = (known after apply) 20 + name = \u0026#34;cloud.ogenki.io\u0026#34; 21 + name_servers = (known after apply) 22 + primary_name_server = (known after apply) 23 + tags = { 24 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 25 } 26 + tags_all = { 27 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 28 } 29 + zone_id = (known after apply) 30 } 31 32Plan: 1 to add, 0 to change, 0 to destroy. 33 34Changes to Outputs: 35 + domain_name = \u0026#34;cloud.ogenki.io\u0026#34; 36 + nameservers = (known after apply) 37 + zone_arn = (known after apply) 38 + zone_id = (known after apply) 39 40Plan generated: set approvePlan: \u0026#34;plan-main@345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. 41To set the field, you can also run: 42 43 tfctl approve route53-cloud-hostedzone -f filename.yaml La proc√©dure de restauration consiste donc √† cr√©er le secret √† nouveau:\n1gzip ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate 2 3cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 4apiVersion: v1 5kind: Secret 6metadata: 7 name: tfstate-${WORKSPACE}-${STACK} 8 namespace: flux-system 9 annotations: 10 encoding: gzip 11type: Opaque 12data: 13 tfstate: $(cat ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate.gz | base64 -w 0) 14EOF Il faudra aussi relancer un plan de fa√ßon explicite pour mettre √† jour l'√©tat de la ressource en question\n1tfctl replan route53-cloud-hostedzone 2Ôò´ Replan requested for flux-system/route53-cloud-hostedzone 3Error: timed out waiting for the condition Nous pouvons alors v√©rifier que le fichier d'√©tat a bien √©t√© mis √† jour\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3flux-system route53-cloud-hostedzone True Outputs written: main@sha1:d0934f979d832feb870a8741ec01a927e9ee6644 false 19 minutes üîç Focus sur certaines fonctionnalit√©s de Flux Oui j'ai un peu menti sur l'agenda üòù. Il me semblait n√©cessaire de mettre en lumi√®re 2 fonctionnalit√©s que je n'avais pas exploit√© jusque l√† et qui sont fort utiles!\nSubstition de variables Lorsque Flux est initilias√© un certain nombre de Kustomization sp√©cifique √† ce cluster sont cr√©√©s. Il est possible d'y indiquer des variables de substitution qui pourront √™tre utilis√©es dans l'ensemble des ressources d√©ploy√©es par cette Kustomization. Cela permet d'√©viter un maximum la d√©duplication de code.\nJ'ai d√©couvert l'efficacit√© de cette fonctionnalit√© tr√®s r√©cemment. Je vais d√©crire ici la fa√ßon dont je l'utilise:\nLe code terraform qui cr√©e un cluster EKS, g√©n√®re aussi une ConfigMap qui contient les variables propres au cluster. On y retrouvera, bien s√ªr, le nom du cluster, mais aussi tous les param√®tres qui varient entre les clusters et qui sont utilis√©s dans les manifests Kubernetes.\nflux.tf\n1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;flux_clusters_vars\u0026#34; { 2 metadata { 3 name = \u0026#34;eks-${var.cluster_name}-vars\u0026#34; 4 namespace = \u0026#34;flux-system\u0026#34; 5 } 6 7 data = { 8 cluster_name = var.cluster_name 9 oidc_provider_arn = module.eks.oidc_provider_arn 10 aws_account_id = data.aws_caller_identity.this.account_id 11 region = var.region 12 environment = var.env 13 vpc_id = module.vpc.vpc_id 14 } 15 depends_on = [flux_bootstrap_git.this] 16} Comme sp√©cifi√© pr√©cedemment, les variables de substition sont d√©finies dans les Kustomization. Prenons un exemple concret. Ci-dessous on d√©finie la Kustomization qui d√©ploie toutes les ressources qui sont consomm√©es par tf-controller On d√©clare ici la ConfigMap eks-controlplane-0-vars qui avait √©t√© g√©n√©r√© √† la cr√©ation du cluster EKS.\ninfrastructure.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1 2kind: Kustomization 3metadata: 4 name: tf-custom-resources 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 path: ./infrastructure/controlplane-0/terraform/custom-resources 10 postBuild: 11 substitute: 12 domain_name: \u0026#34;cloud.ogenki.io\u0026#34; 13 substituteFrom: 14 - kind: ConfigMap 15 name: eks-controlplane-0-vars 16 - kind: Secret 17 name: eks-controlplane-0-vars 18 optional: true 19 sourceRef: 20 kind: GitRepository 21 name: flux-system 22 dependsOn: 23 - name: tf-controller Enfin voici un exemple de ressource Kubernetes qui en fait usage. Cet unique manifest peut √™tre utilis√© par tous les clusters!.\nexternal-dns/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-dns 5spec: 6... 7 values: 8 global: 9 imageRegistry: public.ecr.aws 10 fullnameOverride: external-dns 11 aws: 12 region: ${region} 13 zoneType: \u0026#34;public\u0026#34; 14 batchChangeSize: 1000 15 domainFilters: [\u0026#34;${domain_name}\u0026#34;] 16 logFormat: json 17 txtOwnerId: \u0026#34;${cluster_name}\u0026#34; 18 serviceAccount: 19 annotations: 20 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/${cluster_name}-external-dns\u0026#34; Cela √©limine totalement les overlays qui consistaient √† ajouter les param√®tres sp√©cifiques au cluster.\nWeb UI (Weave GitOps) Dans mon pr√©c√©dent article sur Flux, je mentionnais le fait que l'un des inconv√©nients (si l'on compare avec son principale concurrent: ArgoCD) est le manque d'une interface Web. Bien que je sois un adepte de la ligne de commande, c'est parfois bien utile d'avoir une vue synth√©tique et de pouvoir effectuer certaines op√©ration en quelques clicks üñ±\nC'est d√©sormais possible avec Weave Gitops! Bien entendu ce n'est pas comparable avec l'UI d'ArgoCD, mais l'essentiel est l√†: Mettre en pause la r√©concilation, visualiser les manifests, les d√©pendances, les √©v√©nements...\nIl existe aussi le plugin VSCode comme alternative.\nüí≠ Remarques Et voil√†, nous arrivons au bout de notre exploration de cet autre outil de gestion d'infrastructure sur Kubernetes. Malgr√© quelques petits soucis rencontr√©s en cours de route, que j'ai partag√© sur le repo Git du projet, l'exp√©rience m'a beaucoup plu. tf-controller offre une r√©ponse concr√®te √† une question fr√©quente : comment g√©rer notre infra comme on g√®re notre code ?\nJ'aime beaucoup l'approche GitOps appliqu√©e √† l'infrastructure, j'avais d'ailleurs √©crit un article sur Crossplane. tf-controller aborde la probl√©matique sous un angle diff√©rent: utiliser du Terraform directement. Cela signifie qu'on peut utiliser nos connaissances actuelles et notre code existant. Pas besoin d'apprendre une nouvelle fa√ßon de d√©clarer nos ressources. C'est un crit√®re √† prendre en compte car migrer vers un nouvel outil lorsque l'on a un existant repr√©sente un √©ffort non n√©gligeable. Cependant j'ajouterais aussi que tf-controller s'adresse aux utilisateurs de Flux uniquement et, de ce fait, restreint le publique cible.\nCeci √©tant dit, je vous encourage √† essayer tf-controller vous-m√™me, et peut-√™tre m√™me d'y apporter votre contribution üôÇ.\nNote La d√©mo que j'ai faite ici utilise pas mal de ressources, dont certaines assez cruciales (comme le r√©seau). Donc, gardez en t√™te que c'est juste pour la d√©mo ! Je sugg√®re une approche progressive si vous envisagez de le mettre en ouvre: commencez par utiliser la d√©tection de d√©rives, puis cr√©ez des ressources simples. J'ai aussi pris quelques raccourcis en terme de s√©curit√© √† √©viter absolument, notamment le fait de donner les droits admin au contr√¥leur. ","link":"https://blog.ogenki.io/fr/post/terraform-controller/","section":"post","tags":["infrastructure"],"title":"Appliquer les principes de GitOps √† l'infrastructure: Introduction √† `tf-controller`"},{"body":"","link":"https://blog.ogenki.io/fr/tags/infrastructure/","section":"tags","tags":null,"title":"infrastructure"},{"body":"","link":"https://blog.ogenki.io/fr/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://blog.ogenki.io/fr/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"Kubernetes est d√©sormais la plate-forme privil√©gi√©e pour orchestrer les applications \u0026quot;sans √©tat\u0026quot; aussi appel√© \u0026quot;stateless\u0026quot;. Les conteneurs qui ne stockent pas de donn√©es peuvent √™tre d√©truits et recr√©√©s ailleurs sans impact. En revanche, la gestion d'applications \u0026quot;stateful\u0026quot; dans un environnement dynamique tel que Kubernetes peut √™tre un v√©ritable d√©fi. Malgr√© le fait qu'il existe un nombre croissant de solutions de base de donn√©es \u0026quot;Cloud Native\u0026quot; (comme CockroachDB, TiDB, K8ssandra, Strimzi ...) et il y a de nombreux √©l√©ments √† consid√©rer lors de leur √©valuation:\nQuelle est la maturit√© de l'op√©rateur? (Dynamisme et contributeurs, gouvernance du projet) Quels sont les resources personalis√©es disponibles (\u0026quot;custom resources\u0026quot;), quelles op√©rations permettent t-elles de r√©aliser? Quels sont les type de stockage disponibles: HDD / SSD, stockage local / distant? Que se passe-t-il lorsque quelque chose se passe mal: Quelle est le niveau de r√©silience de la solution? Sauvegarde et restauration: est-il facile d'effectuer et de planifier des sauvegardes? Quelles options de r√©plication et de mise √† l'√©chelle sont disponibles? Qu'en est-il des limites de connexion et de concurrence, les pools de connexion? A propos de la supervision, quelles sont les m√©triques expos√©es et comment les exploiter? J'√©tais √† la recherche d'une solution permettant de g√©rer un serveur PostgreSQL. La base de donn√©es qui y serait h√©berg√©e est n√©cessaire pour un logiciel de r√©servation de billets nomm√© Alf.io. Nous sommes en effet en train d'organiser les Kubernetes Community Days France vous √™tes tous convi√©s! üëê.\nJe cherchais sp√©cifiquement une solution ind√©pendante d'un clouder (cloud agnostic) et l'un des principaux crit√®res √©tait la simplicit√© d'utilisation. Je connaissais d√©j√† plusieurs op√©rateurs Kubernetes, et j'ai fini par √©valuer une solution relativement r√©cente: CloudNativePG.\nCloudNativepg est l'op√©rateur de Kubernetes qui couvre le cycle de vie complet d'un cluster de base de donn√©es PostgreSQL hautement disponible avec une architecture de r√©plication native en streaming.\nCe projet √©t√© cr√©√© par l'entreprise EnterpriseDB et a √©t√© soumis √† la CNCF afin de rejoindre les projets Sandbox.\nüéØ Notre objectif Je vais donner ici une introduction aux principales fonctionnalit√©s de CloudNativePG.\nL'objectif est de:\nCr√©er une base de donn√©es PostgreSQL sur un cluster GKE, Ajouter une instance secondaire (r√©plication) Ex√©cuter quelques tests de r√©silience. Nous verrons √©galement comment tout cela se comporte en terme de performances et quels sont les outils de supervision disponibles. Enfin, nous allons jeter un ≈ìil aux m√©thodes de sauvegarde/restauration.\nInfo Dans cet article, nous allons tout cr√©er et tout mettre √† jour manuellement. Mais dans un environnement de production, il est conseill√© d'utiliser un moteur GitOps, par exemple Flux (sujet couvert dans un article pr√©c√©dent).\nSi vous souhaitez voir un exemple complet, vous pouvez consulter le d√©p√¥t git KCD France infrastructure.\nToutes les resources de cet article sont dans ce d√©p√¥t.\n‚òëÔ∏è Pr√©requis üì• Outils gcloud SDK: Nous allons d√©ployer sur Google Cloud (en particulier sur GKE) et, pour ce faire, nous devrons cr√©er quelques ressources dans notre projet GCP. Nous aurons donc besoin du SDK et de la CLI Google Cloud. Il est donc n√©cessaire de l'installer en suivant cette documentation.\nkubectl plugin: Pour faciliter la gestion des clusters, il existe un plugin kubectl qui donne des informations synth√©tiques sur l'instance PostgreSQL et permet aussi d'effectuer certaines op√©rations. Ce plugin peut √™tre install√© en utilisant krew:\n1kubectl krew install cnpg ‚òÅÔ∏è Cr√©er les resources Google Cloud Avant de cr√©er notre instance PostgreSQL, nous devons configurer certaines choses:\nNous avons besoin d'un cluster Kubernetes. (Cet article suppose que vous avez d√©j√† pris soin de provisionner un cluster GKE) Nous allons cr√©er un bucket (Google Cloud Storage) pour stocker les sauvegardes et Fichiers WAL. Nous configurerons les permissions pour nos pods afin qu'ils puissent √©crire dans ce bucket. Cr√©er le bucket √† l'aide de CLI gcloud\n1gcloud storage buckets create --location=eu --default-storage-class=coldline gs://cnpg-ogenki 2Creating gs://cnpg-ogenki/... 3 4gcloud storage buckets describe gs://cnpg-ogenki 5[...] 6name: cnpg-ogenki 7owner: 8 entity: project-owners-xxxx0008 9projectNumber: \u0026#39;xxx00008\u0026#39; 10rpo: DEFAULT 11selfLink: https://www.googleapis.com/storage/v1/b/cnpg-ogenki 12storageClass: STANDARD 13timeCreated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; 14updated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; Nous allons maintenant configurer les permissions afin que les pods (PostgreSQL Server) puissent permettant √©crire/lire √† partir du bucket gr√¢ce √† Workload Identity.\nNote Workload Identity doit √™tre activ√© au niveau du cluster GKE. Afin de v√©rifier que le cluster est bien configur√©, vous pouvez lancer la commande suivante:\n1gcloud container clusters describe \u0026lt;cluster_name\u0026gt; --format json --zone \u0026lt;zone\u0026gt; | jq .workloadIdentityConfig 2{ 3 \u0026#34;workloadPool\u0026#34;: \u0026#34;{{ gcp_project }}.svc.id.goog\u0026#34; 4} Cr√©er un compte de service Google Cloud\n1gcloud iam service-accounts create cloudnative-pg --project={{ gcp_project }} 2Created service account [cloudnative-pg]. Attribuer au compte de service la permission storage.admin\n1gcloud projects add-iam-policy-binding {{ gcp_project }} \\ 2--member \u0026#34;serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com\u0026#34; \\ 3--role \u0026#34;roles/storage.admin\u0026#34; 4[...] 5- members: 6 - serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 7 role: roles/storage.admin 8etag: BwXrGA_VRd4= 9version: 1 Autoriser le compte de service (Attention il s'agit l√† du compte de service au niveau Kubernetes) afin d'usurper le compte de service IAM. ‚ÑπÔ∏è Assurez-vous d'utiliser le format appropri√© serviceAccount:{{ gcp_project }}.svc.id.goog[{{ kubernetes_namespace }}/{{ kubernetes_serviceaccount }}]\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 7 role: roles/iam.workloadIdentityUser 8etag: BwXrGBjt5kQ= 9version: 1 Nous sommes pr√™ts √† cr√©er les ressources Kubernetes üí™\nüîë Cr√©er les secrets pour les utilisateurs PostgreSQL Nous devons cr√©er les param√®tres d'authentification des utilisateurs qui seront cr√©√©s pendant la phase de \u0026quot;bootstrap\u0026quot; (nous y reviendrons par la suite): le superutilisateur et le propri√©taire de base de donn√©es nouvellement cr√©√©.\n1kubectl create secret generic cnpg-mydb-superuser --from-literal=username=postgres --from-literal=password=foobar --namespace demo 2secret/cnpg-mydb-superuser created 1kubectl create secret generic cnpg-mydb-user --from-literal=username=smana --from-literal=password=barbaz --namespace demo 2secret/cnpg-mydb-user created üõ†Ô∏è D√©ployer l'op√©rateur CloudNativePG avec Helm Ici nous utiliserons le chart Helm pour d√©ployer CloudNativePG:\n1helm repo add cnpg https://cloudnative-pg.github.io/charts 2 3helm upgrade --install cnpg --namespace cnpg-system \\ 4--create-namespace charts/cloudnative-pg 5 6kubectl get po -n cnpg-system 7NAME READY STATUS RESTARTS AGE 8cnpg-74488f5849-8lhjr 1/1 Running 0 6h17m Cela installe aussi quelques resources personnalis√©es (Custom Resources Definitions)\n1kubectl get crds | grep cnpg.io 2backups.postgresql.cnpg.io 2022-10-08T16:15:14Z 3clusters.postgresql.cnpg.io 2022-10-08T16:15:14Z 4poolers.postgresql.cnpg.io 2022-10-08T16:15:14Z 5scheduledbackups.postgresql.cnpg.io 2022-10-08T16:15:14Z Pour une liste compl√®te des param√®tres possibles, veuillez vous r√©f√©rer √† la doc de l'API.\nüöÄ Cr√©er un serveur PostgreSQL Nous pouvons d√©sormais cr√©er notre premi√®re instance en utilisant une resource personnalis√©e Cluster. La d√©finition suivante est assez simple: Nous souhaitons d√©marrer un serveur PostgreSQL, cr√©er automatiquement une base de donn√©es nomm√©e mydb et configurer les informations d'authentification en utilisant les secrets cr√©√©s pr√©c√©demment.\n1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki 5 namespace: demo 6spec: 7 description: \u0026#34;PostgreSQL Demo Ogenki\u0026#34; 8 imageName: ghcr.io/cloudnative-pg/postgresql:14.5 9 instances: 1 10 11 bootstrap: 12 initdb: 13 database: mydb 14 owner: smana 15 secret: 16 name: cnpg-mydb-user 17 18 serviceAccountTemplate: 19 metadata: 20 annotations: 21 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 22 23 superuserSecret: 24 name: cnpg-mydb-superuser 25 26 storage: 27 storageClass: standard 28 size: 10Gi 29 30 backup: 31 barmanObjectStore: 32 destinationPath: \u0026#34;gs://cnpg-ogenki\u0026#34; 33 googleCredentials: 34 gkeEnvironment: true 35 retentionPolicy: \u0026#34;30d\u0026#34; 36 37 resources: 38 requests: 39 memory: \u0026#34;1Gi\u0026#34; 40 cpu: \u0026#34;500m\u0026#34; 41 limits: 42 memory: \u0026#34;1Gi\u0026#34; Cr√©er le namespace o√π notre instance postgresql sera d√©ploy√©e\n1kubectl create ns demo 2namespace/demo created Adapdez le fichier YAML ci-dessus vos besoins et appliquez comme suit:\n1kubectl apply -f cluster.yaml 2cluster.postgresql.cnpg.io/ogenki created Vous remarquerez que le cluster sera en phase Initializing. Nous allons utiliser le plugin CNPG pour la premi√®re fois afin de v√©rifier son √©tat. Cet outil deviendra par la suite notre meilleur ami pour afficher une vue synth√©tique de l'√©tat du cluster.\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Primary server is initializing 4Name: ogenki 5Namespace: demo 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: (switching to ogenki-1) 8Status: Setting up primary Creating primary instance ogenki-1 9Instances: 1 10Ready instances: 0 11 12Certificates Status 13Certificate Name Expiration Date Days Left Until Expiration 14---------------- --------------- -------------------------- 15ogenki-ca 2023-01-13 20:02:40 +0000 UTC 90.00 16ogenki-replication 2023-01-13 20:02:40 +0000 UTC 90.00 17ogenki-server 2023-01-13 20:02:40 +0000 UTC 90.00 18 19Continuous Backup status 20First Point of Recoverability: Not Available 21No Primary instance found 22Streaming Replication status 23Not configured 24 25Instances status 26Name Database Size Current LSN Replication role Status QoS Manager Version Node 27---- ------------- ----------- ---------------- ------ --- --------------- ---- imm√©diatement apr√®s la d√©claration de notre nouveau Cluster, une action de bootstrap est lanc√©e. Dans notre exemple, nous cr√©ons une toute nouvelle base de donn√©es nomm√©e mydb avec un propri√©taire smana dont les informations d'authentification viennent du secret cr√©√© pr√©c√©demment.\n1[...] 2 bootstrap: 3 initdb: 4 database: mydb 5 owner: smana 6 secret: 7 name: cnpg-mydb-user 8[...] 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 0/1 Running 0 55s 4ogenki-1-initdb-q75cz 0/1 Completed 0 2m32s Apr√®s quelques secondes, le cluster change de statut et devient Ready (configur√© et pr√™t √† l'usage) üëè\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7154833472216277012 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 1 10Ready instances: 1 11 12[...] 13 14Instances status 15Name Database Size Current LSN Replication role Status QoS Manager Version Node 16---- ------------- ----------- ---------------- ------ --- --------------- ---- 17ogenki-1 33 MB 0/17079F8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xczh Info Il existe de nombreuses fa√ßons de bootstrap un cluster. Par exemple, la restauration d'une sauvegarde dans une toute nouvelle instance ou en ex√©cutant du code SQL ... Plus d'infos ici.\nü©π Instance de secours et r√©silience Info Dans les architectures postgresql traditionnelles, nous trouvons g√©n√©ralement un composant suppl√©mentaire pour g√©rer la haute disponibilit√© (ex: Patroni). Un particularit√© de l'op√©rateur CloudNativePG est qu'il b√©n√©ficie des fonctionnalit√©s de base de Kubernetes et s'appuie sur un composant nomm√© Postgres instance manager.\nAjoutez une instance de secours (\u0026quot;standby\u0026quot;) en d√©finissant le nombre de r√©pliques sur 2.\n1kubectl edit cluster -n demo ogenki 2cluster.postgresql.cnpg.io/ogenki edited 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3[...] 4spec: 5 instances: 2 6[...] L'op√©rateur remarque imm√©diatement le changement, ajoute une instance de secours et d√©marre le processus de r√©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Creating a new replica Creating replica ogenki-2-join 9Instances: 2 10Ready instances: 1 11Current Write LSN: 0/1707A30 (Timeline: 1 - WAL File: 000000010000000000000001) 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 0 3m16s 4ogenki-2-join-xxrwx 0/1 Pending 0 82s Apr√®s un certain temps (qui d√©pend de la quantit√© de donn√©es √† r√©pliquer), l'instance de secours devient op√©rationnelle et nous pouvons voir les statistiques de r√©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 2 10Ready instances: 2 11Current Write LSN: 0/3000060 (Timeline: 1 - WAL File: 000000010000000000000003) 12 13[...] 14 15Streaming Replication status 16Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 17---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 18ogenki-2 0/3000060 0/3000060 0/3000060 0/3000060 00:00:00 00:00:00 00:00:00 streaming async 0 19 20Instances status 21Name Database Size Current LSN Replication role Status QoS Manager Version Node 22---- ------------- ----------- ---------------- ------ --- --------------- ---- 23ogenki-1 33 MB 0/3000060 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 24ogenki-2 33 MB 0/3000060 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc Nous allons d√©sormais √©ffectuer ce que l'on appelle un \u0026quot;Switchover\u0026quot;: Nous allons promouvoir l'instance de secours en instance primaire.\nLe plugin cnpg permet de le faire de fa√ßon imp√©rative, en utilisant la ligne de commande suivante:\n1kubectl cnpg promote ogenki ogenki-2 -n demo 2Node ogenki-2 in cluster ogenki will be promoted Dans mon cas, le basculement √©tait vraiment rapide. Nous pouvons v√©rifier que l'instance ogenki-2 est devenu primaire et que la r√©plication est effectu√©e dans l'autre sens.\n1kubectl cnpg status -n demo ogenki 2[...] 3Status: Switchover in progress Switching over to ogenki-2 4Instances: 2 5Ready instances: 1 6[...] 7Streaming Replication status 8Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 9---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 10ogenki-1 0/4004CA0 0/4004CA0 0/4004CA0 0/4004CA0 00:00:00 00:00:00 00:00:00 streaming async 0 11 12Instances status 13Name Database Size Current LSN Replication role Status QoS Manager Version Node 14---- ------------- ----------- ---------------- ------ --- --------------- ---- 15ogenki-2 33 MB 0/4004CA0 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc 16ogenki-1 33 MB 0/4004CA0 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 Maintenant, provoquons un Failover en supprimant le pod principal\n1kubectl delete po -n demo --grace-period 0 --force ogenki-2 2Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. 3pod \u0026#34;ogenki-2\u0026#34; force deleted 1Cluster Summary 2Name: ogenki 3Namespace: demo 4System ID: 7155095145869606932 5PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 6Primary instance: ogenki-1 7Status: Failing over Failing over from ogenki-2 to ogenki-1 8Instances: 2 9Ready instances: 1 10Current Write LSN: 0/4005D98 (Timeline: 3 - WAL File: 000000030000000000000004) 11 12[...] 13Instances status 14Name Database Size Current LSN Replication role Status QoS Manager Version Node 15---- ------------- ----------- ---------------- ------ --- --------------- ---- 16ogenki-1 33 MB 0/40078D8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 17ogenki-2 - - - pod not available Burstable - gke-kcdfrance-main-np-0e87115b-xszc Quelques secondes plus tard le cluster devient op√©rationnel √† nouveau.\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 13m 2 2 Cluster in healthy state ogenki-1 Jusqu'ici tout va bien, nous avons pu faire quelques tests de la haute disponibilit√© et c'√©tait assez probant üòé.\nüëÅÔ∏è Supervision Nous allons utiliser la Stack Prometheus. Nous ne couvrirons pas son installation dans cet article. Si vous voulez voir comment l'installer avec Flux, vous pouvez jeter un oeil √† cet exemple.\nPour r√©cup√©rer les m√©triques de notre instance, nous devons cr√©er un PodMonitor.\n1apiVersion: monitoring.coreos.com/v1 2kind: PodMonitor 3metadata: 4 labels: 5 prometheus-instance: main 6 name: cnpg-ogenki 7 namespace: demo 8spec: 9 namespaceSelector: 10 matchNames: 11 - demo 12 podMetricsEndpoints: 13 - port: metrics 14 selector: 15 matchLabels: 16 postgresql: ogenki Nous pouvons ensuite ajouter le tableau de bord Grafana disponible ici.\nEnfin, vous souhaiterez peut-√™tre configurer des alertes et vous pouvez cr√©er un PrometheusRule en utilisant ces r√®gles.\nüî• Performances and benchmark Info Mise √† jour: Il est d√©sormais possible de faire un test de performance avec le plugin cnpg\nAfin de connaitre les limites de votre serveur, vous devriez faire un test de performances et de conserver une base de r√©f√©rence pour de futures am√©liorations.\nNote Au sujet des performances, il existe de nombreux domaines d'am√©lioration sur lesquels nous pouvons travailler.Cela d√©pend principalement de l'objectif que nous voulons atteindre. En effet, nous ne voulons pas perdre du temps et de l'argent pour les performances dont nous n'aurons probablement jamais besoin.\nVoici les principaux √©l√©ments √† analyser:\nTuning de la configuration PostgreSQL Resources syst√®mes (cpu et m√©moire) Types de Disque : IOPS, stockage locale (local-volume-provisioner), Disques d√©di√©es pour les WAL et les donn√©es PG_DATA \u0026quot;Pooling\u0026quot; de connexions PGBouncer. CloudNativePG fourni une resource personnalis√©e Pooler qui permet de configurer cela facilement. Optimisation de la base de donn√©es, analyser les plans d'ex√©cution gr√¢ce √† explain, utiliser l'extension pg_stat_statement ... Tout d'abord, nous ajouterons des \u0026quot;labels\u0026quot; aux n≈ìuds afin d'ex√©cuter la commande pgbench sur diff√©rentes machines de celles h√©bergeant la base de donn√©es.\n1PG_NODE=$(kubectl get po -n demo -l postgresql=ogenki,role=primary -o jsonpath={.items[0].spec.nodeName}) 2kubectl label node ${PG_NODE} workload=postgresql 3node/gke-kcdfrance-main-np-0e87115b-vlzm labeled 4 5 6# Choose any other node different than the ${PG_NODE} 7kubectl label node gke-kcdfrance-main-np-0e87115b-p5d7 workload=pgbench 8node/gke-kcdfrance-main-np-0e87115b-p5d7 labeled Et nous d√©ploierons le chart Helm comme suit\n1git clone git@github.com:EnterpriseDB/cnp-bench.git 2cd cnp-bench 3 4cat \u0026gt; pgbench-benchmark/myvalues.yaml \u0026lt;\u0026lt;EOF 5cnp: 6 existingCluster: true 7 existingHost: ogenki-rw 8 existingCredentials: cnpg-mydb-superuser 9 existingDatabase: mydb 10 11pgbench: 12 # Node where to run pgbench 13 nodeSelector: 14 workload: pgbench 15 initialize: true 16 scaleFactor: 1 17 time: 600 18 clients: 10 19 jobs: 1 20 skipVacuum: false 21 reportLatencies: false 22EOF 23 24helm upgrade --install -n demo pgbench -f pgbench-benchmark/myvalues.yaml pgbench-benchmark/ Info Il existe diff√©rents services selon que vous souhaitez lire et √©crire ou de la lecture seule.\n1kubectl get ep -n demo 2NAME ENDPOINTS AGE 3ogenki-any 10.64.1.136:5432,10.64.1.3:5432 15d 4ogenki-r 10.64.1.136:5432,10.64.1.3:5432 15d 5ogenki-ro 10.64.1.136:5432 15d 6ogenki-rw 10.64.1.3:5432 15d 1kubectl logs -n demo job/pgbench-pgbench-benchmark -f 2Defaulted container \u0026#34;pgbench\u0026#34; out of: pgbench, wait-for-cnp (init), pgbench-init (init) 3pgbench (14.1, server 14.5 (Debian 14.5-2.pgdg110+2)) 4starting vacuum...end. 5transaction type: \u0026lt;builtin: TPC-B (sort of)\u0026gt; 6scaling factor: 1 7query mode: simple 8number of clients: 10 9number of threads: 1 10duration: 600 s 11number of transactions actually processed: 545187 12latency average = 11.004 ms 13initial connection time = 111.585 ms 14tps = 908.782896 (without initial connection time) üíΩ Sauvegarde and restaurations Note Le fait de pouvoir stocker des sauvegarde et fichiers WAL dans le bucket GCP est possible car nous avons attribu√© les autorisations en utilisant une annotation pr√©sente dans le ServiceAccount utilis√© par le cluster\n1serviceAccountTemplate: 2 metadata: 3 annotations: 4 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com Nous pouvons d'abord d√©clencher une sauvegarde on demand √† l'aide de la ressource personnalis√©e Backup\n1apiVersion: postgresql.cnpg.io/v1 2kind: Backup 3metadata: 4 name: ogenki-now 5 namespace: demo 6spec: 7 cluster: 8 name: ogenki 1kubectl apply -f backup.yaml 2backup.postgresql.cnpg.io/ogenki-now created 3 4kubectl get backup -n demo 5NAME AGE CLUSTER PHASE ERROR 6ogenki-now 36s ogenki completed Si vous jetez un ≈ìil au contenu du bucket GCS, vous verrez un nouveau r√©pertoire qui stocke les sauvegardes de base (\u0026quot;base backups\u0026quot;).\n1gcloud storage ls gs://cnpg-ogenki/ogenki/base 2gs://cnpg-ogenki/ogenki/base/20221023T130327/ Mais la plupart du temps, nous pr√©fererons configurer une sauvegarde planifi√©e (\u0026quot;scheduled\u0026quot;). Ci-dessous un exemple pour une sauvegarde quotidienne:\n1apiVersion: postgresql.cnpg.io/v1 2kind: ScheduledBackup 3metadata: 4 name: ogenki-daily 5 namespace: demo 6spec: 7 backupOwnerReference: self 8 cluster: 9 name: ogenki 10 schedule: 0 0 0 * * * Les restaurations ne peuvent √™tre effectu√©es que sur de nouvelles instances. Ici, nous utiliserons la sauvegarde que nous avions cr√©√©e pr√©c√©demment afin d'initialiser une toute nouvelle instance.\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore] 7 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 8 role: roles/iam.workloadIdentityUser 9etag: BwXrs755FPA= 10version: 1 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki-restore 5 namespace: demo 6spec: 7 instances: 1 8 9 serviceAccountTemplate: 10 metadata: 11 annotations: 12 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 13 14 storage: 15 storageClass: standard 16 size: 10Gi 17 18 resources: 19 requests: 20 memory: \u0026#34;1Gi\u0026#34; 21 cpu: \u0026#34;500m\u0026#34; 22 limits: 23 memory: \u0026#34;1Gi\u0026#34; 24 25 superuserSecret: 26 name: cnpg-mydb-superuser 27 28 bootstrap: 29 recovery: 30 backup: 31 name: ogenki-now Nous notons qu'un pod se charge imm√©diatement de la restauration compl√®te (\u0026quot;full recovery\u0026quot;).\n1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 1 (18h ago) 18h 4ogenki-2 1/1 Running 0 18h 5ogenki-restore-1 0/1 Init:0/1 0 0s 6ogenki-restore-1-full-recovery-5p4ct 0/1 Completed 0 51s Le nouveau cluster devient alors op√©rationnel (\u0026quot;Ready\u0026quot;).\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 18h 2 2 Cluster in healthy state ogenki-1 4ogenki-restore 80s 1 1 Cluster in healthy state ogenki-restore-1 üßπ Nettoyage Suppression du cluster\n1kubectl delete cluster -n demo ogenki ogenki-restore 2cluster.postgresql.cnpg.io \u0026#34;ogenki\u0026#34; deleted 3cluster.postgresql.cnpg.io \u0026#34;ogenki-restore\u0026#34; deleted Supprimer le service IAM\n1gcloud iam service-accounts delete cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 2You are about to delete service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 3 4Do you want to continue (Y/n)? y 5 6deleted service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com] üí≠ Conclusion Je viens tout juste de d√©couvrir CloudNativePG et je n'ai fait qu'en percevoir la surface, mais une chose est s√ªre: la gestion d'une instance PostgreSQL est vraiment facilit√©e. Cependant, le choix d'une solution de base de donn√©es est une d√©cision complexe. Il faut prendre en compte le cas d'usage, les contraintes de l'entreprise, la criticit√© de l'application et les comp√©tences des √©quipes op√©rationnelles. Il existe de nombreuses options: bases de donn√©es g√©r√©es par le fournisseur Cloud, installation traditionnelle sur serveur baremetal, solutions distribu√©es ...\nNous pouvons √©galement envisager d'utiliser Crossplane et une Composition pour fournir un niveau d'abstraction suppl√©mentaire afin de d√©clarer des bases de donn√©es des fournisseurs Cloud, mais cela n√©cessite plus de configuration.\nCloudNativePG sort du lot par sa simplicit√©: Super facile √† ex√©cuter et √† comprendre. De plus, la documentation est excellente (l'une des meilleures que j'aie jamais vues!), Surtout pour un si jeune projet open source (cela aidera peut √™tre pour √™tre accept√© en tant que projet \u0026quot;Sandbox\u0026quot; CNCF ü§û).\nSi vous voulez en savoir plus, il y avait une pr√©sentation √† ce sujet √† KubeCon NA 2022.\n","link":"https://blog.ogenki.io/fr/post/cnpg/","section":"post","tags":["data"],"title":"`CloudNativePG`: et PostgreSQL devient facile sur Kubernetes"},{"body":"","link":"https://blog.ogenki.io/fr/tags/data/","section":"tags","tags":null,"title":"data"},{"body":"In a previous article, we've seen how to use Crossplane so that we can manage cloud resources the same way as our applications. ‚ù§Ô∏è Declarative approach! There were several steps and command lines in order to get everything working and reach our target to provision a dev Kubernetes cluster.\nHere we'll achieve exactly the same thing but we'll do that in the GitOps way. According to the OpenGitOps working group there are 4 GitOps principles:\nThe desired state of our system must be expressed declaratively. This state must be stored in a versioning system. Changes are pulled and applied automatically in the target platform whenever the desired state changes. If, for any reason, the current state is modified, it will be automatically reconciled with the desired state. There are several GitOps engine options. The most famous ones are ArgoCD and Flux. We won't compare them here. I chose Flux because I like its composable architecture with different controllers, each one handling a core Flux feature (GitOps toolkit).\nLearn more about GitOps toolkit components here.\nüéØ Our target Here we want to declare our desired infrastructure components only by adding git changes. By the end of this article you'll get a GKE cluster provisioned using a local Crossplane instance. We'll discover Flux basics and how to use it in order to build a complete GitOps CD workflow.\n‚òëÔ∏è Requirements üì• Install required tools First of all we need to install a few tools using asdf\nCreate a local file .tool-versions\n1cd ~/sources/devflux/ 2 3cat \u0026gt; .tool-versions \u0026lt;\u0026lt;EOF 4flux2 0.31.3 5kubectl 1.24.3 6kubeseal 0.18.1 7kustomize 4.5.5 8EOF 1for PLUGIN in $(cat .tool-versions | awk \u0026#39;{print $1}\u0026#39;); do asdf plugin-add $PLUGIN; done 2 3asdf install 4Downloading ... 100.0% 5Copying Binary 6... Check that all the required tools are actually installed.\n1asdf current 2flux2 0.31.3 /home/smana/sources/devflux/.tool-versions 3kubectl 1.24.3 /home/smana/sources/devflux/.tool-versions 4kubeseal 0.18.1 /home/smana/sources/devflux/.tool-versions 5kustomize 4.5.5 /home/smana/sources/devflux/.tool-versions üîë Create a Github personal access token In this article the git repository is hosted in Github. In order to be able to use the flux bootstrap a personnal access token is required.\nPlease follow this procedure.\nWarning Store the Github token in a safe place for later use\nüßë‚Äçüíª Clone the devflux repository All the files used for the upcoming steps can be retrieved from this repository. You should clone it, that will be easier to copy them into your own repository.\n1git clone https://github.com/Smana/devflux.git üöÄ Bootstrap flux in the Crossplane cluster As we will often be using the flux CLI you may want to configure the bash|zsh completion\n1source \u0026lt;(flux completion bash) Warning Here we consider that you already have a local k3d instance. If not you may want to either go through the whole previous article or just run the local cluster creation.\nEnsure that you're working in the right context\n1kubectl config current-context 2k3d-crossplane Run the bootstrap command that will basically deploy all Flux's components in the namespace flux-system. Here I'll create a repository named devflux using my personal Github account.\n1export GITHUB_USER=\u0026lt;YOUR_ACCOUNT\u0026gt; 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/k3d-crossplane 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/k3d-crossplane\u0026#34; for \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux\u0026#34; 9... 10‚úî all components are healthy Check that all the pods are running properly and that the kustomization flux-system has been successfully reconciled.\n1kubectl get po -n flux-system 2NAME READY STATUS RESTARTS AGE 3helm-controller-5985c795f8-gs2pc 1/1 Running 0 86s 4notification-controller-6b7d7485fc-lzlpg 1/1 Running 0 86s 5kustomize-controller-6d4669f847-9x844 1/1 Running 0 86s 6source-controller-5fb4888d8f-wgcqv 1/1 Running 0 86s 7 8flux get kustomizations 9NAME REVISION SUSPENDED READY MESSAGE 10flux-system main/33ebef1 False True Applied revision: main/33ebef1 Running the bootstap command actually creates a github repository if it doesn't exist yet. Clone it now for our upcoming changes. You'll notice that the first commit has been made by Flux.\n1git clone https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git 2Cloning into \u0026#39;devflux\u0026#39;... 3 4cd devflux 5 6git log -1 7commit 2beb6aafea67f3386b50cbc706fb34575844040d (HEAD -\u0026gt; main, origin/main, origin/HEAD) 8Author: Flux \u0026lt;\u0026gt; 9Date: Thu Jul 14 17:13:27 2022 +0200 10 11 Add Flux sync manifests 12 13ls clusters/k3d-crossplane/flux-system/ 14gotk-components.yaml gotk-sync.yaml kustomization.yaml üìÇ Flux repository structure There are several options for organizing your resources in the Flux configuration repository. Here is a proposition for the sake of this article.\n1tree -d -L 2 2. 3‚îú‚îÄ‚îÄ apps 4‚îÇ¬†‚îú‚îÄ‚îÄ base 5‚îÇ¬†‚îî‚îÄ‚îÄ dev-cluster 6‚îú‚îÄ‚îÄ clusters 7‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 8‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 9‚îú‚îÄ‚îÄ infrastructure 10‚îÇ¬†‚îú‚îÄ‚îÄ base 11‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 12‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 13‚îú‚îÄ‚îÄ observability 14‚îÇ¬†‚îú‚îÄ‚îÄ base 15‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 16‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 17‚îî‚îÄ‚îÄ security 18 ‚îú‚îÄ‚îÄ base 19 ‚îú‚îÄ‚îÄ dev-cluster 20 ‚îî‚îÄ‚îÄ k3d-crossplane Directory Description Example /apps our applications Here we'll deploy a demo application \u0026quot;online-boutique\u0026quot; /infrastructure base infrastructure/network components Crossplane as it will be used to provision cloud resources but we can also find CSI/CNI/EBS drivers... /observability All metrics/apm/logging tools Prometheus of course, Opentelemetry ... /security Any component that enhance our security level SealedSecrets (see below) Info For the upcoming steps please refer to the demo repository here\nLet's use this structure and begin to deploy applications üöÄ.\nüîê SealedSecrets There are plenty of alternatives when it comes to secrets management in Kubernetes. In order to securely store secrets in a git repository the GitOps way we'll make use of SealedSecrets. It uses a custom resource definition named SealedSecrets in order to encrypt the Kubernetes secret at the client side then the controller is in charge of decrypting and generating the expected secret in the cluster.\nüõ†Ô∏è Deploy the controller using Helm The first thing to do is to declare the kustomization that handles all the security tools.\nclusters/k3d-crossplane/security.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: security 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 sourceRef: 10 kind: GitRepository 11 name: flux-system 12 path: ./security/k3d-crossplane 13 healthChecks: 14 - apiVersion: helm.toolkit.fluxcd.io/v1beta1 15 kind: HelmRelease 16 name: sealed-secrets 17 namespace: kube-system Info A Kustomization is a custom resource that comes with Flux. It basically points to a set of Kubernetes resources managed with kustomize The above security kustomization points to a local directory where the kustomize resources are.\n1... 2spec: 3 path: ./security/k3d-crossplane 4... Note This is worth noting that there are two types on kustomizations. That can be confusing when you start playing with Flux.\nOne managed by flux's kustomize controller. Its API is kustomization.kustomize.toolkit.fluxcd.io The other kustomization.kustomize.config.k8s.io is for the kustomize overlay The kustomization.yaml file is always used for the kustomize overlay. Flux itself doesn't need this overlay in all cases, but if you want to use features of a Kustomize overlay you will occasionally need to create it in order to access them. It provides instructions to the Kustomize CLI.\nWe will deploy SealedSecrets using the Helm chart. So we need to declare the source of this chart. Using the kustomize overlay system, we'll first create the base files that will be inherited at the cluster level.\nsecurity/base/sealed-secrets/source.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: sealed-secrets 5 namespace: flux-system 6spec: 7 interval: 30m 8 url: https://bitnami-labs.github.io/sealed-secrets Then we'll define the HelmRelease which references the above source. Put the values you want to apply to the Helm chart under spec.values\nsecurity/base/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 releaseName: sealed-secrets 8 chart: 9 spec: 10 chart: sealed-secrets 11 sourceRef: 12 kind: HelmRepository 13 name: sealed-secrets 14 namespace: flux-system 15 version: \u0026#34;2.4.0\u0026#34; 16 interval: 10m0s 17 install: 18 remediation: 19 retries: 3 20 values: 21 fullnameOverride: sealed-secrets-controller 22 resources: 23 requests: 24 cpu: 80m 25 memory: 100Mi If you're starting your repository from scratch you'll need to generate the kustomization.yaml file (kustomize overlay).\n1kustomize create --autodetect security/base/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4- helmrelease.yaml 5- source.yaml Now we declare the sealed-secret kustomization at the cluster level. Just for the example we'll overwrite a value at the cluster level using kustomize's overlay system.\nsecurity/k3d-crossplane/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 values: 8 resources: 9 requests: 10 cpu: 100m security/k3d-crossplane/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3bases: 4 - ../../base 5patches: 6 - helmrelease.yaml Pushing our changes is the only thing to do in order to get sealed-secrets deployed in the target cluster.\n1git commit -m \u0026#34;security: deploy sealed-secrets in k3d-crossplane\u0026#34; 2[security/sealed-secrets 283648e] security: deploy sealed-secrets in k3d-crossplane 3 6 files changed, 66 insertions(+) 4 create mode 100644 clusters/k3d-crossplane/security.yaml 5 create mode 100644 security/base/sealed-secrets/helmrelease.yaml 6 create mode 100644 security/base/sealed-secrets/kustomization.yaml 7 create mode 100644 security/base/sealed-secrets/source.yaml 8 create mode 100644 security/k3d-crossplane/sealed-secrets/helmrelease.yaml 9 create mode 100644 security/k3d-crossplane/sealed-secrets/kustomization.yaml After a few seconds (1 minutes by default) a new kustomization will appear.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3flux-system main/d36a33c False True Applied revision: main/d36a33c 4security main/d36a33c False True Applied revision: main/d36a33c And all the resources that we declared in the flux repository should be available and READY.\n1flux get sources helm 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee False True stored artifact for revision \u0026#39;4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee\u0026#39; 1flux get helmrelease -n kube-system 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 2.2.0 False True Release reconciliation succeeded üß™ A first test SealedSecret Let's use the CLI kubeseal to test it out. We'll create a SealedSecret that will be decrypted by the sealed-secrets controller in the cluster and create the expected secret foobar\n1kubectl create secret generic foobar -n default --dry-run=client -o yaml --from-literal=foo=bar \\ 2| kubeseal --namespace default --format yaml | kubectl apply -f - 3sealedsecret.bitnami.com/foobar created 4 5kubectl get secret -n default foobar 6NAME TYPE DATA AGE 7foobar Opaque 1 3m13s 8 9kubectl delete sealedsecrets.bitnami.com foobar 10sealedsecret.bitnami.com \u0026#34;foobar\u0026#34; deleted ‚òÅÔ∏è Deploy and configure Crossplane üîë Create the Google service account secret The first thing we need to do in order to get Crossplane working is to create the GCP serviceaccount. The steps have been covered here in the previous article. We'll create a SealedSecret gcp-creds that contains the serviceaccount file crossplane.json.\ninfrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml\n1kubectl create secret generic gcp-creds --context k3d-crossplane -n crossplane-system --from-file=creds=./crossplane.json --dry-run=client -o yaml \\ 2| kubeseal --format yaml --namespace crossplane-system - \u0026gt; infrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml üîÑ Crossplane dependencies Now we will deploy Crossplane with Flux. I won't put the manifests here you'll find all of them in this repository. However it's important to understand that, in order to deploy and configure Crossplane properly we need to do that in a specific order. Indeed several CRD's (custom resource definitions) are required:\nFirst of all we'll install the crossplane controller. Then we'll configure the provider because the custom resource is now available thanks to the crossplane controller installation. Finally a provider installation deploys several CRDs that can be used to configure the provider itself and cloud resources. The dependencies between kustomizations can be controlled using the parameters dependsOn. Looking at the file clusters/k3d-crossplane/infrastructure.yaml, we can see for example that the kustomization infrastructure-custom-resources depends on the kustomization crossplane_provider which itself depends on crossplane-configuration....\n1--- 2apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 3kind: Kustomization 4metadata: 5 name: crossplane-provider 6spec: 7... 8 dependsOn: 9 - name: crossplane-core 10--- 11apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 12kind: Kustomization 13metadata: 14 name: crossplane-configuration 15spec: 16... 17 dependsOn: 18 - name: crossplane-provider 19--- 20apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 21kind: Kustomization 22metadata: 23 name: infrastructure-custom-resources 24spec: 25... 26 dependsOn: 27 - name: crossplane-configuration Commit and push the changes for the kustomisations to appear. Note that they'll be reconciled in the defined order.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3infrastructure-custom-resources False False dependency \u0026#39;flux-system/crossplane-configuration\u0026#39; is not ready 4crossplane-configuration False False dependency \u0026#39;flux-system/crossplane-provider\u0026#39; is not ready 5security main/666f85a False True Applied revision: main/666f85a 6flux-system main/666f85a False True Applied revision: main/666f85a 7crossplane-core main/666f85a False True Applied revision: main/666f85a 8crossplane-provider main/666f85a False True Applied revision: main/666f85a Then all Crossplane components will be deployed, we can have a look to the HelmRelease status for instance.\n1kubectl describe helmrelease -n crossplane-system crossplane 2... 3Status: 4 Conditions: 5 Last Transition Time: 2022-07-15T19:12:04Z 6 Message: Release reconciliation succeeded 7 Reason: ReconciliationSucceeded 8 Status: True 9 Type: Ready 10 Last Transition Time: 2022-07-15T19:12:04Z 11 Message: Helm upgrade succeeded 12 Reason: UpgradeSucceeded 13 Status: True 14 Type: Released 15 Helm Chart: crossplane-system/crossplane-system-crossplane 16 Last Applied Revision: 1.9.0 17 Last Attempted Revision: 1.9.0 18 Last Attempted Values Checksum: 056dc1c6029b3a644adc7d6a69a93620afd25b65 19 Last Release Revision: 2 20 Observed Generation: 1 21Events: 22 Type Reason Age From Message 23 ---- ------ ---- ---- ------- 24 Normal info 20m helm-controller HelmChart \u0026#39;crossplane-system/crossplane-system-crossplane\u0026#39; is not ready 25 Normal info 20m helm-controller Helm upgrade has started 26 Normal info 19m helm-controller Helm upgrade succeeded And our GKE cluster should also be created because we defined a bunch of crossplane custom resources in infrastructure/k3d-crossplane/custom-resources/crossplane\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RUNNING 34.x.x.190 europe-west9-a 22m üöÄ Bootstrap flux in the dev cluster Our local Crossplane cluster is now ready and it created our dev cluster and we also want it to be managed with Flux. So let's configure Flux for this dev cluster using the same bootstrap command.\nAuthenticate to the newly created cluster. The following command will automatically change your current context.\n1gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project \u0026lt;your_project\u0026gt; 2Fetching cluster endpoint and auth data. 3kubeconfig entry generated for dev-cluster. 4 5kubectl config current-context 6gke_\u0026lt;your_project\u0026gt;_europe-west9-a_dev-cluster Run the bootstrap command for the dev-cluster.\n1export GITHUB_USER=Smana 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/dev-cluster 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/Smana/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/dev-cluster\u0026#34; for \u0026#34;https://github.com/Smana/devflux\u0026#34; 9... 10‚úî all components are healthy Note It's worth noting that each Kubernetes cluster generates its own sealing keys. That means that if you recreate the dev-cluster, you must regenerate all the sealedsecrets. In our example we declared a secret in order to set the Grafana credentials. Here's the command you need to run in order to create a new version of the sealedsecret and don't forget to use the proper context üòâ.\n1kubectl create secret generic kube-prometheus-stack-grafana \\ 2--from-literal=admin-user=admin --from-literal=admin-password=\u0026lt;yourpassword\u0026gt; --namespace observability --dry-run=client -o yaml \\ 3| kubeseal --namespace observability --format yaml \u0026gt; observability/dev-cluster/kube-prometheus-stack/sealedsecrets.yaml After a few seconds we'll get the following kustomizations deployed.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3apps main/1380eaa False True Applied revision: main/1380eaa 4flux-system main/1380eaa False True Applied revision: main/1380eaa 5observability main/1380eaa False True Applied revision: main/1380eaa 6security main/1380eaa False True Applied revision: main/1380eaa Here we configured the prometheus stack and deployed a demo microservices stack named \u0026quot;online-boutique\u0026quot; This demo application exposes the frontend through a service of type LoadBalancer.\n1kubectl get svc -n demo frontend-external 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3frontend-external LoadBalancer 10.140.174.201 34.155.121.2 80:31943/TCP 7m44s Use the EXTERNAL_IP\nüïµ Troubleshooting The cheatsheet in Flux's documentation contains many ways for troubleshooting when something goes wrong. Here I'll just give a sample of my favorite command lines.\nObjects that aren't ready\n1flux get all -A --status-selector ready=false Checking the logs of a given kustomization\n1flux logs --kind kustomization --name infrastructure-custom-resources 22022-07-15T19:38:52.996Z info Kustomization/infrastructure-custom-resources.flux-system - server-side apply completed 32022-07-15T19:38:53.016Z info Kustomization/infrastructure-custom-resources.flux-system - Reconciliation finished in 66.12266ms, next run in 4m0s 42022-07-15T19:11:34.697Z info Kustomization/infrastructure-custom-resources.flux-system - Discarding event, no alerts found for the involved object Show how a given pod is managed by Flux.\n1flux trace -n crossplane-system pod/crossplane-5dc8d888d7-g95qx 2 3Object: Pod/crossplane-5dc8d888d7-g95qx 4Namespace: crossplane-system 5Status: Managed by Flux 6--- 7HelmRelease: crossplane 8Namespace: crossplane-system 9Revision: 1.9.0 10Status: Last reconciled at 2022-07-15 21:12:04 +0200 CEST 11Message: Release reconciliation succeeded 12--- 13HelmChart: crossplane-system-crossplane 14Namespace: crossplane-system 15Chart: crossplane 16Version: 1.9.0 17Revision: 1.9.0 18Status: Last reconciled at 2022-07-15 21:11:36 +0200 CEST 19Message: pulled \u0026#39;crossplane\u0026#39; chart with version \u0026#39;1.9.0\u0026#39; 20--- 21HelmRepository: crossplane 22Namespace: crossplane-system 23URL: https://charts.crossplane.io/stable 24Revision: 362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d 25Status: Last reconciled at 2022-07-15 21:11:35 +0200 CEST 26Message: stored artifact for revision \u0026#39;362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d\u0026#39; If you want to check what would be the changes before pushing your commit. In thi given example I just increased the cpu requests for the sealed-secrets controller.\n1flux diff kustomization security --path security/k3d-crossplane 2‚úì Kustomization diffing... 3‚ñ∫ HelmRelease/kube-system/sealed-secrets drifted 4 5metadata.generation 6 ¬± value change 7 - 6 8 + 7 9 10spec.values.resources.requests.cpu 11 ¬± value change 12 - 100m 13 + 120m 14 15‚ö†Ô∏è identified at least one change, exiting with non-zero exit code üßπ Cleanup Don't forget to delete the Cloud resources if you don't want to have a bad suprise üíµ! Just comment the file infrastructure/k3d-crossplane/custom-resources/crossplane/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 # - cluster.yaml 5 - network.yaml üëè Achievements With our current setup everything is configured using the GitOps approach:\nWe can manage infrastructure resources using Crossplane. Our secrets are securely stored in our git repository. We have a dev-cluster that we can enable or disable just but commenting a yaml file. Our demo application can be deployed from scratch in seconds. üí≠ final thoughts Flux is probably the tool I'm using the most on a daily basis. It's really amazing!\nWhen you get familiar with its concepts and the command line it becomes really easy to use and troubleshoot. You can use either Helm when a chart is available or Kustomize.\nHowever we faced a few issues:\nIt's not straightforward to find an efficient structure depending on the company needs. Especially when you have several Kubernetes controllers that depend on other CRDs. The Helm controller doesn't maintain a state of the Kubernetes resources deployed by the Helm chart. That means that if you delete a resource which has been deployed through a Helm chart, it won't be reconciled (It will change soon. Being discussed here) Flux doesn't provide itself a web UI and switching between CLIs (kubectl, flux ...) can be annoying from a developer perspective. (I'm going to test weave-gitops ) I've been using Flux in production for more than a year and we configured it with the image automation so that the only thing a developer has to do is to merge a pull request and the new version of the application is automatically deployed in the target cluster.\nI should probably give another try to ArgoCD in order to be able to compare these precisely ü§î.\n","link":"https://blog.ogenki.io/fr/post/devflux/","section":"post","tags":["gitops","devxp"],"title":"100% `GitOps` using Flux"},{"body":"Qui suis-je? I'm a senior Site Reliability Engineer with a particular interest in Linux containers and cloud technologies. I worked in different companies (small startups and large scale) and I've been working in different areas in order to improve the reliability, availability of the platform as well as the developer experience. I helped several companies in their transition to the Cloud. I was leading SRE/DevOps teams (diverse profiles with developers and SREs) and I really enjoy seeing them engaged in the same direction.\nAs side activities, I am an organizer of the Cloud Native Computing meetup in Paris and the Kubernetes Community Days France.\nHobbies: Reading SF books, Kick Boxing, Surfing/Skating/Inline Roller\n","link":"https://blog.ogenki.io/fr/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://blog.ogenki.io/fr/tags/devxp/","section":"tags","tags":null,"title":"devxp"},{"body":"","link":"https://blog.ogenki.io/fr/tags/gitops/","section":"tags","tags":null,"title":"gitops"},{"body":"","link":"https://blog.ogenki.io/fr/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"La cible de cette documentation est de pouvoir cr√©er et g√©rer un cluster GKE en utilisant Crossplane.\nCrossplane exploite les principes de base de Kubernetes afin de fournir des ressources cloud et bien plus encore: une ** approche d√©clarative ** avec ** Drift Detections ** et ** r√©conciliations ** Utilisation de boucles de contr√¥le: Exploseding_head:.En d'autres termes, nous d√©clarons les ressources cloud que nous voulons et Crossplane garantit que l'√©tat cible correspond √† celui appliqu√© via l'API Kubernetes.\nVoici les √©tapes que nous suivrons afin d'obtenir un cluster Kubernetes pour le d√©veloppement et les cas d'utilisation des exp√©rimentations.\nüê≥ Create the local k3d cluster for Crossplane's control plane k3d est un cluster Kubernetes l√©ger qui exploite K3S qui s'ex√©cute dans notre ordinateur portable local. Il existe plusieurs mod√®les de d√©ploiement pour Crossplane, nous pourrions par exemple d√©ployer le plan de contr√¥le sur un cluster de gestion sur Kubernetes ou un plan de contr√¥le par cluster Kubernetes. Ici, j'ai choisi une m√©thode simple qui est bien pour un cas d'utilisation personnelle: une instance Kubernetes locale ** dans laquelle je vais d√©ployer Crossplane.\nLet's install k3d using asdf.\n1asdf plugin-add k3d 2 3asdf install k3d $(asdf latest k3d) 4* Downloading k3d release 5.4.1... 5k3d 5.4.1 installation was successful! Create a single node Kubernetes cluster.\n1k3d cluster create crossplane 2... 3INFO[0043] You can now use it like this: 4kubectl cluster-info 5 6k3d cluster list 7crossplane 1/1 0/0 true Check that the cluster is reachable using the kubectl CLI.\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:40643 3CoreDNS is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy We only need a single node for our Crossplane use case.\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-crossplane-server-0 Ready control-plane,master 26h v1.22.7+k3s1 ‚òÅÔ∏è Generate the Google Cloud service account Warning Store the downloaded crossplane.json credentials file in a safe place.\nCreate a service account\n1GCP_PROJECT=\u0026lt;your_project\u0026gt; 2gcloud iam service-accounts create crossplane --display-name \u0026#34;Crossplane\u0026#34; --project=${GCP_PROJECT} 3Created service account [crossplane]. Assign the proper permissions to the service account.\nCompute Network Admin Kubernetes Engine Admin Service Account User 1SA_EMAIL=$(gcloud iam service-accounts list --filter=\u0026#34;email ~ ^crossplane\u0026#34; --format=\u0026#39;value(email)\u0026#39;) 2 3gcloud projects add-iam-policy-binding \u0026#34;${GCP_PROJECT}\u0026#34; --member=serviceAccount:\u0026#34;${SA_EMAIL}\u0026#34; \\ 4--role=roles/container.admin --role=roles/compute.networkAdmin --role=roles/iam.serviceAccountUser 5Updated IAM policy for project [\u0026lt;project\u0026gt;]. 6bindings: 7- members: 8 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 9 role: roles/compute.networkAdmin 10- members: 11 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 12... 13version: 1 Download the service account key (json format)\n1gcloud iam service-accounts keys create crossplane.json --iam-account ${SA_EMAIL} 2created key [ea2eb9ce2939127xxxxxxxxxx] of type [json] as [crossplane.json] for [crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com] üöß Deploy and configure Crossplane Now that we have a credentials file for Google Cloud, we can deploy the Crossplane operator and configure the provider-gcp provider.\nInfo Most of the following steps are issued from the official documentation\nWe'll first use Helm in order to install the operator\n1helm repo add crossplane-master https://charts.crossplane.io/master/ 2\u0026#34;crossplane-master\u0026#34; has been added to your repositories 3 4helm repo update 5...Successfully got an update from the \u0026#34;crossplane-master\u0026#34; chart repository 6 7helm install crossplane --namespace crossplane-system --create-namespace \\ 8--version 1.18.1 crossplane-stable/crossplane 9 10NAME: crossplane 11LAST DEPLOYED: Mon Jun 6 22:00:02 2022 12NAMESPACE: crossplane-system 13STATUS: deployed 14REVISION: 1 15TEST SUITE: None 16NOTES: 17Release: crossplane 18... Check that the operator is running properly.\n1kubectl get po -n crossplane-system 2NAME READY STATUS RESTARTS AGE 3crossplane-rbac-manager-54d96cd559-222hc 1/1 Running 0 3m37s 4crossplane-688c575476-lgklq 1/1 Running 0 3m37s Info All the files used for the upcoming steps are stored within this blog repository. So you should clone and change the current directory:\n1git clone https://github.com/Smana/smana.github.io.git 2 3cd smana.github.io/content/resources/crossplane_k3d Now we'll configure Crossplane so that it will be able to create and manage GCP resources. This is done by configuring the provider provider-gcp as follows.\nprovider.yaml\n1apiVersion: pkg.crossplane.io/v1 2kind: Provider 3metadata: 4 name: crossplane-provider-gcp 5spec: 6 package: crossplane/provider-gcp:v0.21.0 1kubectl apply -f provider.yaml 2provider.pkg.crossplane.io/crossplane-provider-gcp created 3 4kubectl get providers 5NAME INSTALLED HEALTHY PACKAGE AGE 6crossplane-provider-gcp True True crossplane/provider-gcp:v0.21.0 10s Create the Kubernetes secret that holds the GCP credentials file created above\n1kubectl create secret generic gcp-creds -n crossplane-system --from-file=creds=./crossplane.json 2secret/gcp-creds created Then we need to create a resource named ProviderConfig and reference the newly created secret.\nprovider-config.yaml\n1apiVersion: gcp.crossplane.io/v1beta1 2kind: ProviderConfig 3metadata: 4 name: default 5spec: 6 projectID: ${GCP_PROJECT} 7 credentials: 8 source: Secret 9 secretRef: 10 namespace: crossplane-system 11 name: gcp-creds 12 key: creds 1kubectl apply -f provider-config.yaml 2providerconfig.gcp.crossplane.io/default created Info If the serviceaccount has the proper permissions we can create resources in GCP. In order to learn about all the available resources and parameters we can have a look to the provider's API reference.\nThe first resource we'll create is the network that will host our Kubernetes cluster.\nnetwork.yaml\n1apiVersion: compute.gcp.crossplane.io/v1beta1 2kind: Network 3metadata: 4 name: dev-network 5 labels: 6 service: vpc 7 creation: crossplane 8spec: 9 forProvider: 10 autoCreateSubnetworks: false 11 description: \u0026#34;Network used for experimentations and POCs\u0026#34; 12 routingConfig: 13 routingMode: REGIONAL 1kubectl get network 2NAME READY SYNCED 3dev-network True True You can even get more details by describing this resource. For instance if something fails you would see the message returned by the Cloud provider in the events.\n1kubectl describe network dev-network | grep -A 20 \u0026#39;^Status:\u0026#39; 2Status: 3 At Provider: 4 Creation Timestamp: 2022-06-28T09:45:30.703-07:00 5 Id: 3005424280727359173 6 Self Link: https://www.googleapis.com/compute/v1/projects/${GCP_PROJECT}/global/networks/dev-network 7 Conditions: 8 Last Transition Time: 2022-06-28T16:45:31Z 9 Reason: Available 10 Status: True 11 Type: Ready 12 Last Transition Time: 2022-06-30T16:36:59Z 13 Reason: ReconcileSuccess 14 Status: True 15 Type: Synced üöÄ Create a GKE cluster Everything is ready so that we can create our GKE cluster. Applying the file cluster.yaml will create a cluster and attach a node group to it.\ncluster.yaml\n1--- 2apiVersion: container.gcp.crossplane.io/v1beta2 3kind: Cluster 4metadata: 5 name: dev-cluster 6spec: 7 forProvider: 8 description: \u0026#34;Kubernetes cluster for experimentations and POCs\u0026#34; 9 initialClusterVersion: \u0026#34;1.24\u0026#34; 10 releaseChannel: 11 channel: \u0026#34;RAPID\u0026#34; 12 location: europe-west9-a 13 addonsConfig: 14 gcePersistentDiskCsiDriverConfig: 15 enabled: true 16 networkPolicyConfig: 17 disabled: false 18 networkRef: 19 name: dev-network 20 ipAllocationPolicy: 21 createSubnetwork: true 22 useIpAliases: true 23 defaultMaxPodsConstraint: 24 maxPodsPerNode: 110 25 networkPolicy: 26 enabled: false 27 writeConnectionSecretToRef: 28 namespace: default 29 name: gke-conn 30--- 31apiVersion: container.gcp.crossplane.io/v1beta1 32kind: NodePool 33metadata: 34 name: main-np 35spec: 36 forProvider: 37 initialNodeCount: 1 38 autoscaling: 39 autoprovisioned: false 40 enabled: true 41 maxNodeCount: 4 42 minNodeCount: 1 43 clusterRef: 44 name: dev-cluster 45 config: 46 machineType: n2-standard-2 47 diskSizeGb: 120 48 diskType: pd-standard 49 imageType: cos_containerd 50 preemptible: true 51 labels: 52 environment: dev 53 managed-by: crossplane 54 oauthScopes: 55 - \u0026#34;https://www.googleapis.com/auth/devstorage.read_only\u0026#34; 56 - \u0026#34;https://www.googleapis.com/auth/logging.write\u0026#34; 57 - \u0026#34;https://www.googleapis.com/auth/monitoring\u0026#34; 58 - \u0026#34;https://www.googleapis.com/auth/servicecontrol\u0026#34; 59 - \u0026#34;https://www.googleapis.com/auth/service.management.readonly\u0026#34; 60 - \u0026#34;https://www.googleapis.com/auth/trace.append\u0026#34; 61 metadata: 62 disable-legacy-endpoints: \u0026#34;true\u0026#34; 63 shieldedInstanceConfig: 64 enableIntegrityMonitoring: true 65 enableSecureBoot: true 66 management: 67 autoRepair: true 68 autoUpgrade: true 69 maxPodsConstraint: 70 maxPodsPerNode: 60 71 locations: 72 - \u0026#34;europe-west9-a\u0026#34; 1kubectl apply -f cluster.yaml 2cluster.container.gcp.crossplane.io/dev-cluster created 3nodepool.container.gcp.crossplane.io/main-np created Note that it takes around 10 minutes for the Kubernetes API and the nodes to be available. The STATE will transition from PROVISIONING to RUNNING and when a change is being applied the cluster status is RECONCILING\n1watch \u0026#39;kubectl get cluster,nodepool\u0026#39; 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3cluster.container.gcp.crossplane.io/dev-cluster False True PROVISIONING 34.155.122.6 europe-west9-a 3m15s 4 5NAME READY SYNCED STATE CLUSTER-REF AGE 6nodepool.container.gcp.crossplane.io/main-np False False dev-cluster 3m15s When the column READY switches to True you can download the cluster's credentials.\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RECONCILING 34.42.42.42 europe-west9-a 6m23s 4 5gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project ${GCP_PROJECT} 6Fetching cluster endpoint and auth data. 7kubeconfig entry generated for dev-cluster. For better readability you may want to rename the context id for the newly created cluster\n1kubectl config rename-context gke_${GCP_PROJECT}_europe-west9-a_dev-cluster dev-cluster 2Context \u0026#34;gke_${GCP_PROJECT}_europe-west9-a_dev-cluster\u0026#34; renamed to \u0026#34;dev-cluster\u0026#34;. 3 4kubectl config get-contexts 5CURRENT NAME CLUSTER AUTHINFO NAMESPACE 6* dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster 7 k3d-crossplane k3d-crossplane admin@k3d-crossplane Check that you can call our brand new GKE API\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3gke-dev-cluster-main-np-d0d978f9-5fc0 Ready \u0026lt;none\u0026gt; 10m v1.24.1-gke.1400 That's great üéâ we know have a GKE cluster up and running.\nüí≠ final thoughts I've been using Crossplane for a few months now in a production environment.\nEven if I'm conviced about the declarative approach using the Kubernetes API, we decided to move with caution with it. It clearly doesn't have Terraform's community and maturity. We're still declaring our resources using the deletionPolicy: Orphan so that even if something goes wrong on the controller side the resource won't be deleted.\nFurthermore we limited to a specific list of usual AWS resources requested by our developers. Nevertheless our target has always been to empower developers and we had really positive feedback from them. That's the best indicator for us. As the project matures, we'll move more and more resources from Terraform to Crossplane.\nIMHO the key success of Crossplane depends on the providers maintenance and evolution. The Cloud providers interest and involvement is really important.\nIn our next article we'll see how to use a GitOps engine to run all the above steps.\n","link":"https://blog.ogenki.io/fr/post/crossplane_k3d/","section":"post","tags":["kubernetes","infrastructure"],"title":"Mon cluster Kubernetes (GKE) avec `Crossplane`"},{"body":"Afin d'installer des binaires et de pouvoir passer d'une version √† une autre, j'aime utiliser asdf.\nüì• Installation L'installation recommand√©e consiste √† utiliser Git comme suit\n1git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.10.0 Il y a quelques √©tapes suppl√©mentaires qui d√©pendent de votre shell. Voici celles que j'utilise pour bash:\n1. $HOME/.asdf/asdf.sh Vous voudrez probablement configurer la completion du shell comme suit\n1. $HOME/.asdf/completions/asdf.bash üöÄ Prenons un exemple Listons tous les plugins disponibles pour trouver k3d\n1asdf plugin-list-all | grep k3d 2k3d https://github.com/spencergilbert/asdf-k3d.git Installons k3d\n1asdf plugin-add k3d V√©rifier les versions disponibles\n1asdf list-all k3d| tail -n 3 25.4.0-dev.3 35.4.0 45.4.1 Nous installerons la derni√®re version\n1asdf install k3d latest 2* Downloading k3d release 5.4.1... 3k3d 5.4.1 installation was successful! Enfin, nous pouvons passer d'une version √† une autre. Nous pouvons d√©finir une version \u0026quot;globale\u0026quot; qui serait utilis√©e sur tous les r√©pertoires.\n1asdf global k3d 5.4.1 ou utilisez une version locale en fonction du r√©pertoire actuel\n1cd /tmp 2asdf local k3d 5.4.1 3 4asdf current k3d 5k3d 5.4.1 /tmp/.tool-versions üßπ Faire le m√©nage D√©sinstaller une version donn√©e\n1asdf uninstall k3d 5.4.1 Retirer un plugin\n1asdf plugin remove k3d ","link":"https://blog.ogenki.io/fr/post/asdf/asdf/","section":"post","tags":["tooling","local"],"title":"G√©rer les versions d'outils avec `asdf`"},{"body":"","link":"https://blog.ogenki.io/fr/tags/local/","section":"tags","tags":null,"title":"local"},{"body":"","link":"https://blog.ogenki.io/fr/tags/tooling/","section":"tags","tags":null,"title":"tooling"},{"body":"","link":"https://blog.ogenki.io/fr/archives/","section":"","tags":null,"title":""},{"body":"","link":"https://blog.ogenki.io/fr/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://blog.ogenki.io/fr/tags/index/","section":"tags","tags":null,"title":"index"},{"body":"","link":"https://blog.ogenki.io/fr/series/","section":"series","tags":null,"title":"Series"}]